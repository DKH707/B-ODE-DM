{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DKH707/B-ODE-DM/blob/main/covid19_Cook_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009472\n",
        "- https://arxiv.org/abs/2302.09125\n",
        "- https://github.com/stefanradev93/BayesFlow\n",
        "- https://github.com/bayesflow-org/JANA-Paper\n",
        "- https://github.com/stefanradev93/AIAgainstCorona/tree/main\n",
        "- https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series"
      ],
      "metadata": {
        "id": "HPH8Lw7RTGYf"
      },
      "id": "HPH8Lw7RTGYf"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/stefanradev93/BayesFlow\n",
        "! pip install -U ipython-autotime numpy pandas tensorflow wbgapi\n",
        "get_ipython().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "eOu7rYGRcV01"
      },
      "id": "eOu7rYGRcV01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autotime\n",
        "import os, warnings, datetime, pathlib, shutil, google.colab, dataclasses, pickle, wbgapi\n",
        "import numpy as np, pandas as pd, tensorflow as tf\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from functools import partial\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, LSTM\n",
        "from bayesflow.networks import InvertibleNetwork, SequentialNetwork\n",
        "from bayesflow.coupling_networks import CouplingLayer\n",
        "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
        "from bayesflow.amortizers import AmortizedLikelihood, AmortizedPosterior, AmortizedPosteriorLikelihood\n",
        "from bayesflow.trainers import Trainer\n",
        "from bayesflow import default_settings\n",
        "from bayesflow.helper_functions import build_meta_dict\n",
        "import bayesflow.diagnostics as diag\n",
        "from bayesflow.computational_utilities import maximum_mean_discrepancy\n",
        "warnings.filterwarnings(\"ignore\", message=\"Could not infer format, so each element will be parsed individually\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "plt.rcParams.update({\"text.usetex\": False, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{{amsmath}}\"})\n",
        "mnt = '/content/drive'\n",
        "google.colab.drive.mount(mnt)\n",
        "RNG = np.random.default_rng(42)\n",
        "EPS = 1e-6\n",
        "\n",
        "class MultiConvLayer(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_filters=32, strides=1):\n",
        "        super(MultiConvLayer, self).__init__()\n",
        "\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(\n",
        "                n_filters // 2,\n",
        "                kernel_size=f,\n",
        "                strides=strides,\n",
        "                padding=\"causal\",\n",
        "                activation=\"relu\",\n",
        "                kernel_initializer=\"glorot_uniform\",\n",
        "            )\n",
        "            for f in range(2, 8)\n",
        "        ]\n",
        "        self.dim_red = tf.keras.layers.Conv1D(\n",
        "            n_filters, 1, 1, activation=\"relu\", kernel_initializer=\"glorot_uniform\"\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = tf.concat([conv(x) for conv in self.convs], axis=-1)\n",
        "        out = self.dim_red(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiConvNet(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_layers=3, n_filters=64, strides=1):\n",
        "        super(MultiConvNet, self).__init__()\n",
        "\n",
        "        self.net = tf.keras.Sequential(\n",
        "            [MultiConvLayer(n_filters, strides) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        self.lstm = LSTM(n_filters)\n",
        "\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = self.net(x)\n",
        "        out = self.lstm(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SummaryNet(tf.keras.Model):\n",
        "    def __init__(self, n_summary):\n",
        "        super(SummaryNet, self).__init__()\n",
        "        self.net_I = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_R = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_D = MultiConvNet(n_filters=n_summary // 3)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        x = tf.split(x, 3, axis=-1)\n",
        "        x_i = self.net_I(x[0])\n",
        "        x_r = self.net_R(x[1])\n",
        "        x_d = self.net_D(x[2])\n",
        "        return tf.concat([x_i, x_r, x_d], axis=-1)\n",
        "\n",
        "class MemoryNetwork(tf.keras.Model):\n",
        "    def __init__(self, meta):\n",
        "        super(MemoryNetwork, self).__init__()\n",
        "\n",
        "        self.gru = GRU(meta[\"n_hidden\"], return_sequences=True, return_state=True)\n",
        "        self.h = meta[\"n_hidden\"]\n",
        "        self.n_params = meta[\"n_params\"]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, target, condition):\n",
        "        \"\"\"Performs a forward pass through the network.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        target    : tf.Tesnor of shape (batch_size, time_stes, dim)\n",
        "            The time-dependent signal to process.\n",
        "        condition : tf.Tensor of shape (batch_size, cond_dim)\n",
        "            The conditional (static) variables, e.g., parameters.\n",
        "        \"\"\"\n",
        "        shift_target = target[:, :-1, :]\n",
        "        init = tf.zeros((target.shape[0], 1, target.shape[2]))\n",
        "        inp_teacher = tf.concat([init, shift_target], axis=1)\n",
        "        inp_teacher_c = tf.concat([inp_teacher, condition], axis=-1)\n",
        "        out, _ = self.gru(inp_teacher_c)\n",
        "        return out\n",
        "\n",
        "    def step_loop(self, target, condition, state):\n",
        "        out, new_state = self.gru(\n",
        "            tf.concat([target, condition], axis=-1), initial_state=state\n",
        "        )\n",
        "        return out, new_state\n",
        "\n",
        "class InvertibleNetworkWithMemory(tf.keras.Model):\n",
        "    \"\"\"Implements a chain of conditional invertible blocks for Bayesian parameter inference.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_params,\n",
        "        num_coupling_layers=4,\n",
        "        coupling_settings=None,\n",
        "        coupling_design=\"affine\",\n",
        "        permutation=\"fixed\",\n",
        "        use_act_norm=True,\n",
        "        act_norm_init=None,\n",
        "        use_soft_flow=False,\n",
        "        soft_flow_bounds=(1e-3, 5e-2),\n",
        "    ):\n",
        "        \"\"\"Initializes a custom invertible network with recurrent memory.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Create settings dict for coupling layer\n",
        "        settings = dict(\n",
        "            latent_dim=num_params,\n",
        "            coupling_settings=coupling_settings,\n",
        "            coupling_design=coupling_design,\n",
        "            permutation=permutation,\n",
        "            use_act_norm=use_act_norm,\n",
        "            act_norm_init=act_norm_init,\n",
        "        )\n",
        "\n",
        "        # Create sequence of coupling layers and store reference to dimensionality\n",
        "        self.coupling_layers = [\n",
        "            CouplingLayer(**settings) for _ in range(num_coupling_layers)\n",
        "        ]\n",
        "\n",
        "        # Store attributes\n",
        "        self.soft_flow = use_soft_flow\n",
        "        self.soft_low = soft_flow_bounds[0]\n",
        "        self.soft_high = soft_flow_bounds[1]\n",
        "        self.use_act_norm = use_act_norm\n",
        "        self.latent_dim = num_params\n",
        "        self.dynamic_summary_net = MemoryNetwork({\"n_hidden\": 256, \"n_params\": 3})\n",
        "        self.latent_dim = num_params\n",
        "\n",
        "    def call(self, targets, condition, inverse=False):\n",
        "        \"\"\"Performs one pass through an invertible chain (either inverse or forward).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        targets   : tf.Tensor\n",
        "            The estimation quantities of interest, shape (batch_size, ...)\n",
        "        condition : tf.Tensor\n",
        "            The conditional data x, shape (batch_size, summary_dim)\n",
        "        inverse   : bool, default: False\n",
        "            Flag indicating whether to run the chain forward or backwards\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (z, log_det_J)  :  tuple(tf.Tensor, tf.Tensor)\n",
        "            If inverse=False: The transformed input and the corresponding Jacobian of the transformation,\n",
        "            v shape: (batch_size, ...), log_det_J shape: (batch_size, ...)\n",
        "\n",
        "        target          :  tf.Tensor\n",
        "            If inverse=True: The transformed out, shape (batch_size, ...)\n",
        "\n",
        "        Important\n",
        "        ---------\n",
        "        If ``inverse=False``, the return is ``(z, log_det_J)``.\\n\n",
        "        If ``inverse=True``, the return is ``target``.\n",
        "        \"\"\"\n",
        "\n",
        "        if inverse:\n",
        "            return self.inverse(targets, condition)\n",
        "        return self.forward(targets, condition)\n",
        "\n",
        "    @tf.function\n",
        "    def forward(self, targets, condition, **kwargs):\n",
        "        \"\"\"Performs a forward pass though the chain.\"\"\"\n",
        "\n",
        "        # Add memory condition\n",
        "        memory = self.dynamic_summary_net(targets, condition)\n",
        "        condition = tf.concat([memory, condition], axis=-1)\n",
        "\n",
        "        z = targets\n",
        "        log_det_Js = []\n",
        "        for layer in self.coupling_layers:\n",
        "            z, log_det_J = layer(z, condition, **kwargs)\n",
        "            log_det_Js.append(log_det_J)\n",
        "        # Sum Jacobian determinants for all layers (coupling blocks) to obtain total Jacobian.\n",
        "        log_det_J = tf.add_n(log_det_Js)\n",
        "        return z, log_det_J\n",
        "\n",
        "    @tf.function\n",
        "    def inverse(self, z, condition, **kwargs):\n",
        "        \"\"\"Performs a reverse pass through the chain.\"\"\"\n",
        "\n",
        "        target = z\n",
        "        T = z.shape[1]\n",
        "        gru_inp = tf.zeros((z.shape[0], 1, z.shape[-1]))\n",
        "        state = tf.zeros((z.shape[0], self.dynamic_summary_net.h))\n",
        "        outs = []\n",
        "        for t in range(T):\n",
        "            # One step condition\n",
        "            memory, state = self.dynamic_summary_net.step_loop(\n",
        "                gru_inp, condition[:, t : t + 1, :], state\n",
        "            )\n",
        "            condition_t = tf.concat([memory, condition[:, t : t + 1, :]], axis=-1)\n",
        "            target_t = target[:, t : t + 1, :]\n",
        "            for layer in reversed(self.coupling_layers):\n",
        "                target_t = layer(target_t, condition_t, inverse=True, **kwargs)\n",
        "            outs.append(target_t)\n",
        "            gru_inp = target_t\n",
        "        return tf.concat(outs, axis=1)\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class COVID():\n",
        "    # country: str = 'Germany'\n",
        "    name: str = 'covid_000'\n",
        "    n_steps: int = 100\n",
        "    n_calibrate: int = 5000\n",
        "    refresh: bool = False\n",
        "\n",
        "    def read_or_create(self, file, fun=None, refresh=False):\n",
        "        \"\"\"Read or create pickle for simulation results\"\"\"\n",
        "        try:\n",
        "            if refresh:\n",
        "                file.unlink(missing_ok=True)\n",
        "            with open(file, \"rb\") as f:\n",
        "                sims = pickle.load(f)\n",
        "            print(f'{file} successfully read')\n",
        "        except Exception as e:\n",
        "            print(f'Running sims: {e}')\n",
        "            with open(file, \"wb\") as f:\n",
        "                sims = fun()\n",
        "                pickle.dump(sims, f)\n",
        "        return sims\n",
        "\n",
        "\n",
        "    def check_params(self, p):\n",
        "        for key in ['E_0','sim_diff','t_1','t_2','t_3','t_4','t_5','delta_1','delta_2','delta_3','delta_4','lag_I','lag_R','lag_D']:\n",
        "            p[key] = int(round(p[key]))\n",
        "        p['E_0'] = max(p['E_0'], 1)\n",
        "        if all([\n",
        "            all(val > EPS for key, val in p.items() if key[:3] != 'phi'),\n",
        "            p['alpha'] < 1 - EPS,\n",
        "            p['delta'] < 1 - EPS,\n",
        "            p['sim_diff'] > max(p['lag_I'],p['lag_R'],p['lag_D']),\n",
        "            *[p[f't_{i}'] + p[f'delta_{i}'] <= p[f't_{i+1}'] for i in range(1,5)],\n",
        "        ]):\n",
        "            return p\n",
        "\n",
        "    def prior_fun(self):\n",
        "        alpha_f = (0.7**2) * ((1 - 0.7) / (0.17**2) - (1 - 0.7))\n",
        "        beta_f = alpha_f * (1 / 0.7 - 1)\n",
        "        while True:\n",
        "            p = self.check_params({\n",
        "                'N'       :86e6,\n",
        "                'E_0'     :RNG.gamma(shape=2, scale=30),\n",
        "                'alpha'   :RNG.uniform(low=0.005, high=0.99),\n",
        "                'beta'    :RNG.lognormal(mean=np.log(0.25), sigma=0.3),\n",
        "                'gamma'   :RNG.lognormal(mean=np.log(1/6.5), sigma=0.5),\n",
        "                'delta'   :RNG.uniform(low=0.01, high=0.3),\n",
        "                'epsilon' :RNG.uniform(low=1/14, high=1/3),\n",
        "                'eta'     :RNG.lognormal(mean=np.log(1/3.2), sigma=0.5),\n",
        "                'lambda'  :RNG.lognormal(mean=np.log(1.2), sigma=0.5),\n",
        "                'mu'      :RNG.lognormal(mean=np.log(1/8), sigma=0.2),\n",
        "                'theta'   :RNG.uniform(low=1/14, high=1/3),\n",
        "                'sim_diff':16,\n",
        "                't_1'     :RNG.normal(loc=8, scale=3),\n",
        "                't_2'     :RNG.normal(loc=15, scale=3),\n",
        "                't_3'     :RNG.normal(loc=22, scale=3),\n",
        "                't_4'     :RNG.normal(loc=66, scale=3),\n",
        "                't_5'     :self.n_steps,\n",
        "                'delta_1' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_2' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_3' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_4' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'lambda_0':RNG.lognormal(mean=np.log(1.20), sigma=0.5),\n",
        "                'lambda_1':RNG.lognormal(mean=np.log(0.60), sigma=0.5),\n",
        "                'lambda_2':RNG.lognormal(mean=np.log(0.30), sigma=0.5),\n",
        "                'lambda_3':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "                # 'lambda_4':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "                'lambda_4':RNG.lognormal(mean=np.log(0.15), sigma=0.5),\n",
        "                'A_I'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'A_R'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'A_D'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'phi_I'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'phi_R'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'phi_D'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'lag_I'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'lag_R'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'lag_D'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'sigma_I' :RNG.gamma(shape=1, scale=5),\n",
        "                'sigma_R' :RNG.gamma(shape=1, scale=5),\n",
        "                'sigma_D' :RNG.gamma(shape=1, scale=5),\n",
        "            })\n",
        "            if p:\n",
        "                return p\n",
        "\n",
        "    def calc_lambda_array(self, p):\n",
        "        \"\"\"Computes the array of time-varying contact rates/transimission probabilities.\"\"\"\n",
        "        # Array of initial lambdas\n",
        "        lambd0_arr = np.array([p['lambda_0']] * (p['t_1'] + p['sim_diff'] - 1))\n",
        "\n",
        "        # Compute lambd1 array\n",
        "        if p['delta_1'] == 1:\n",
        "            lambd1_arr = np.array([p['lambda_1']] * (p['t_2'] - p['t_1']))\n",
        "        else:\n",
        "            lambd1_arr = np.linspace(p['lambda_0'], p['lambda_1'], p['delta_1'])\n",
        "            lambd1_arr = np.append(lambd1_arr, [p['lambda_1']] * (p['t_2'] - p['t_1'] - p['delta_1']))\n",
        "\n",
        "        # Compute lambd2 array\n",
        "        if p['delta_2'] == 1:\n",
        "            lambd2_arr = np.array([p['lambda_2']] * (p['t_3'] - p['t_2']))\n",
        "        else:\n",
        "            lambd2_arr = np.linspace(p['lambda_1'], p['lambda_2'], p['delta_2'])\n",
        "            lambd2_arr = np.append(lambd2_arr, [p['lambda_2']] * (p['t_3'] - p['t_2'] - p['delta_2']))\n",
        "\n",
        "        # Compute lambd3 array\n",
        "        if p['delta_3'] == 1:\n",
        "            lambd3_arr = np.array([p['lambda_3']] * (p['t_4'] - p['t_3']))\n",
        "        else:\n",
        "            lambd3_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_3'])\n",
        "            lambd3_arr = np.append(lambd3_arr, [p['lambda_3']] * (p['t_4'] - p['t_3'] - p['delta_3']))\n",
        "\n",
        "        # Compute lambd4 array\n",
        "        if p['delta_4'] == 1:\n",
        "            lambd4_arr = np.array([p['lambda_4']] * (p['t_5'] - p['t_4']))\n",
        "        else:\n",
        "            lambd4_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_4'])\n",
        "            lambd4_arr = np.append(lambd4_arr, [p['lambda_4']] * (p['t_5'] - p['t_4'] - p['delta_4']))\n",
        "\n",
        "        return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
        "\n",
        "\n",
        "    def prep_params(self, params):\n",
        "        try:\n",
        "            p = self.const_params | params  # if params was passed as dict\n",
        "        except:\n",
        "            p = self.const_params | dict(zip(self.param_names, params))  # if prior_values was passed as list\n",
        "        return self.check_params(p)\n",
        "\n",
        "\n",
        "    def sir(self, prior_draw):\n",
        "        p = self.prep_params(prior_draw)\n",
        "        assert p\n",
        "        sim_lag = p['sim_diff'] - 1\n",
        "        lambd_arr = self.calc_lambda_array(p)\n",
        "\n",
        "        # Initial conditions\n",
        "        S, E, C, I, R, D = [p['N'] - p['E_0']], [p['E_0']], [0], [0], [0], [0]\n",
        "\n",
        "        # Containers\n",
        "        I_news = []\n",
        "        R_news = []\n",
        "        D_news = []\n",
        "\n",
        "        # Reported new cases\n",
        "        I_data = np.zeros(p['t_5'])\n",
        "        R_data = np.zeros(p['t_5'])\n",
        "        D_data = np.zeros(p['t_5'])\n",
        "        fs_I = np.zeros(p['t_5'])\n",
        "        fs_R = np.zeros(p['t_5'])\n",
        "        fs_D = np.zeros(p['t_5'])\n",
        "\n",
        "        # Simulate T-1 tiemsteps\n",
        "        for t in range(p['t_5'] + sim_lag):\n",
        "            # Calculate new exposed cases\n",
        "            E_new = lambd_arr[t] * ((C[t] + p['beta'] * I[t]) / p['N']) * S[t]\n",
        "\n",
        "            # Remove exposed from susceptible\n",
        "            S_t = S[t] - E_new\n",
        "\n",
        "            # Calculate current exposed by adding new exposed and\n",
        "            # subtracting the exposed becoming carriers.\n",
        "            E_t = E[t] + E_new - p['gamma'] * E[t]\n",
        "\n",
        "            # Calculate current carriers by adding the new exposed and subtracting\n",
        "            # those who will develop symptoms and become detected and those who\n",
        "            # will go through the disease asymptomatically.\n",
        "            C_t = C[t] + p['gamma'] * E[t] - (1 - p['alpha']) * p['eta'] * C[t] - p['alpha'] * p['theta'] * C[t]\n",
        "\n",
        "            # Calculate current infected by adding the symptomatic carriers and\n",
        "            # subtracting the dead and recovered. The newly infected are just the\n",
        "            # carriers who get detected.\n",
        "            I_t = I[t] + (1 - p['alpha']) * p['eta'] * C[t] - (1 - p['delta']) * p['mu'] * I[t] - p['delta'] * p['epsilon'] * I[t]\n",
        "            I_new = (1 - p['alpha']) * p['eta'] * C[t]\n",
        "\n",
        "            # Calculate current recovered by adding the symptomatic and asymptomatic\n",
        "            # recovered. The newly recovered are only the detected recovered\n",
        "            R_t = R[t] + p['alpha'] * p['theta'] * C[t] + (1 - p['delta']) * p['mu'] * I[t]\n",
        "            R_new = (1 - p['delta']) * p['mu'] * I[t]\n",
        "\n",
        "            # Calculate the current dead\n",
        "            D_t = D[t] + p['delta'] * p['epsilon'] * I[t]\n",
        "            D_new = p['delta'] * p['epsilon'] * I[t]\n",
        "\n",
        "            # Ensure some numerical onstraints\n",
        "            S_t = np.clip(S_t, 0, p['N'])\n",
        "            E_t = np.clip(E_t, 0, p['N'])\n",
        "            C_t = np.clip(C_t, 0, p['N'])\n",
        "            I_t = np.clip(I_t, 0, p['N'])\n",
        "            R_t = np.clip(R_t, 0, p['N'])\n",
        "            D_t = np.clip(D_t, 0, p['N'])\n",
        "\n",
        "            # Keep track of process over time\n",
        "            S.append(S_t)\n",
        "            E.append(E_t)\n",
        "            C.append(C_t)\n",
        "            I.append(I_t)\n",
        "            R.append(R_t)\n",
        "            D.append(D_t)\n",
        "            I_news.append(I_new)\n",
        "            R_news.append(R_new)\n",
        "            D_news.append(D_new)\n",
        "\n",
        "            # From here, start adding new cases with delay D\n",
        "            # Note, we assume the same delay\n",
        "            if t >= sim_lag:\n",
        "                # Compute lags and add to data arrays\n",
        "                fs_I[t - sim_lag] = (1 - p['A_I']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * p['phi_I']))\n",
        "                )\n",
        "                fs_R[t - sim_lag] = (1 - p['A_R']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * p['phi_R']))\n",
        "                )\n",
        "                fs_D[t - sim_lag] = (1 - p['A_D']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * p['phi_D']))\n",
        "                )\n",
        "                I_data[t - sim_lag] = I_news[t - p['lag_I']]\n",
        "                R_data[t - sim_lag] = R_news[t - p['lag_R']]\n",
        "                D_data[t - sim_lag] = D_news[t - p['lag_D']]\n",
        "\n",
        "        # Compute weekly modulation\n",
        "        I_data = (1 - fs_I) * I_data\n",
        "        R_data = (1 - fs_R) * R_data\n",
        "        D_data = (1 - fs_D) * D_data\n",
        "\n",
        "        # Add noise\n",
        "        I_data = I_data + RNG.standard_t(4) * np.sqrt(I_data) * p['sigma_I']\n",
        "        R_data = R_data + RNG.standard_t(4) * np.sqrt(R_data) * p['sigma_R']\n",
        "        D_data = D_data + RNG.standard_t(4) * np.sqrt(D_data) * p['sigma_D']\n",
        "        n = I_data.shape[0]\n",
        "        return (\n",
        "            pd.DataFrame({'S':S[-n:],'E':E[-n:],'C':C[-n:],'I':I[-n:],'R':R[-n:],'D':D[-n:],'dI_obs':I_data,'dR_obs':R_data,'dD_obs':D_data})\n",
        "            .clip(0, p['N']).rename_axis('t'))\n",
        "\n",
        "    def calibrate(self):\n",
        "        c = self.generator(self.n_calibrate)\n",
        "        d = {'prior':c['prior_draws'], 'data':c['sim_data']}\n",
        "        funs = [np.mean, np.std, np.min, np.max]\n",
        "        c |= {f'{key}_{f.__name__}': f(val, axis=0) for f in funs for key, val in d.items()}\n",
        "        c |= {f'obs_{f.__name__}': c[f'data_{f.__name__}'][...,-self.n_obs:] for f in funs}\n",
        "        return c\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # self.iso = wbgapi.economy.coder(self.country)\n",
        "        # assert self.iso, f'Unrecognized country {self.country}'\n",
        "        # self.tot_pop = next(wbgapi.data.fetch('SP.POP.TOTL', economy=self.iso, time=2020))['value']\n",
        "        # self.load_data()\n",
        "\n",
        "        self.root_path = pathlib.Path(mnt + f'/MyDrive/bayesian disease modeling/{self.name}')\n",
        "        if self.refresh:\n",
        "            # delete root_path and everything in it\n",
        "            shutil.rmtree(self.root_path, ignore_errors=True)\n",
        "        # self.cntry_path = self.root_path / self.country\n",
        "        self.model_path = self.root_path / f'model/'\n",
        "        # self.cntry_path.mkdir(exist_ok=True, parents=True)\n",
        "        self.model_path.mkdir(exist_ok=True, parents=True)\n",
        "        self.calibration_file = self.model_path / 'calibration.pkl'\n",
        "        self.diagnostic_file  = self.model_path / 'diagnostic.pkl'\n",
        "        # self.predictive_file  = self.cntry_path / 'predictive.pkl'\n",
        "\n",
        "\n",
        "        # self.file = {key: self.model_path / f'{key}.pkl' for key in ['calibration','diagnostic','predictive','error']}\n",
        "        # self.file = {key: self.model_path / f'{key}.pkl' for key in ['calibration','diagnostic','predictive','error']}\n",
        "        # self.iso = wbgapi.economy.coder(self.country)\n",
        "        # assert self.iso, f'Unrecognized country {self.country}'\n",
        "        # self.tot_pop = next(wbgapi.data.fetch('SP.POP.TOTL', economy=self.iso, time=2020))['value']\n",
        "        # self.path = self.root_path / self.iso\n",
        "        # self.path.mkdir(exist_ok=True)\n",
        "        # self.load_data()\n",
        "\n",
        "        prior_samples = [self.prior_fun() for _ in range(100)]\n",
        "        self.const_params = pd.DataFrame(prior_samples).agg(['mean','std']).T.query('std < 1e-5')['mean'].to_dict()\n",
        "        self.param_names = [key for key in prior_samples[0].keys() if key not in self.const_params]\n",
        "        self.param_latex = [f'${key}$' if key in ['N','E_0','sim_diff','t_1','t_2','t_3','t_4','A_I','A_R','A_D','lag_I','lag_R','lag_D'] else f'$\\{key}$' for key in self.param_names]\n",
        "        self.n_params = len(self.param_names)\n",
        "        self.obs_classes = self.sir(prior_samples[0]).filter(like='obs').columns.tolist()\n",
        "        self.n_obs = len(self.obs_classes)\n",
        "\n",
        "        self.prior = Prior(prior_fun=lambda: [val for key, val in self.prior_fun().items() if key in self.param_names], param_names=self.param_names)\n",
        "        self.simulator = Simulator(simulator_fun=self.sir)\n",
        "        self.generator = GenerativeModel(self.prior, self.simulator)\n",
        "        self.calibration = self.read_or_create(file=self.calibration_file, fun=self.calibrate)\n",
        "\n",
        "        # summary_net = SequentialNetwork()\n",
        "        # inference_net = InvertibleNetwork(num_params=len(self.param_names), num_coupling_layers=3)\n",
        "        coupling_settings = {\n",
        "            \"dense_args\": dict(units=128, activation=\"swish\", kernel_regularizer=None),\n",
        "            \"num_dense\": 2,\n",
        "            \"dropout\": False,\n",
        "        }\n",
        "        summary_net = SummaryNet(n_summary=192)\n",
        "        inference_net = InvertibleNetwork(\n",
        "            num_params=len(self.param_names),\n",
        "            num_coupling_layers=6,\n",
        "            coupling_settings=coupling_settings,\n",
        "        )\n",
        "        self.model = Trainer(\n",
        "            generative_model = self.generator,\n",
        "            configurator = self.pre_amortizer,\n",
        "            amortizer = AmortizedPosterior(summary_net=summary_net, inference_net=inference_net),# summary_loss_fun=\"MMD\"),\n",
        "            checkpoint_path = self.model_path, max_to_keep = 3, #memory = True, memory is broken for now\n",
        "            skip_checks = True,\n",
        "        )\n",
        "\n",
        "    def pre_amortizer(self, dct):\n",
        "        if 'sim_data' in dct:\n",
        "            data  = np.array(dct['sim_data'])[...,-self.n_obs:]\n",
        "            prior = np.array(dct['prior_draws'])\n",
        "        elif 'real_data' in dct:\n",
        "            data  = np.array(dct['real_data'])\n",
        "            prior = np.full([1,self.n_params], np.nan)\n",
        "        else:\n",
        "            raise Exception\n",
        "        dct['summary_conditions'] = np.float32((data - self.calibration['obs_mean']) / self.calibration['obs_std'])\n",
        "        dct['parameters'] = np.float32((prior - self.calibration['prior_mean']) / self.calibration['prior_std'])\n",
        "        return dct\n",
        "\n",
        "    def post_amortizer(self, dct):\n",
        "        for _ in range(3-dct['parameters_out'].ndim):\n",
        "            dct['parameters_out'] = dct['parameters_out'][np.newaxis]\n",
        "        dct['posterior_draws'] = dct['parameters_out'] * self.calibration['prior_std'] + self.calibration['prior_mean']\n",
        "        return dct\n",
        "\n",
        "    def draw_samples(self, n_posteriors, n_priors=None, real_data=None, ensemble=True):\n",
        "        if real_data is not None:\n",
        "            samples = {'real_data':[real_data]}\n",
        "        elif n_priors is not None:\n",
        "            samples = self.model.generative_model(n_priors)\n",
        "        else:\n",
        "            raise Exception('Must specify n_priors or real_data')\n",
        "        samples = self.model.configurator(samples)\n",
        "        samples['parameters_out'] = self.model.amortizer.sample(samples, n_posteriors)\n",
        "        samples = self.post_amortizer(samples)\n",
        "        if ensemble:\n",
        "            samples['valid_draws'] = [[p for p in d if self.prep_params(p) is not None] for d in samples['posterior_draws']]\n",
        "            samples['posterior'] = pd.concat([pd.concat([pd.DataFrame([p], columns=self.param_names).assign(prior_idx=i, posterior_idx=j).set_index(['prior_idx','posterior_idx']) for j, p in enumerate(D)]) for i, D in enumerate(samples['valid_draws'])])\n",
        "            samples['ensemble'] = pd.concat([self.sir(p).reset_index().assign(prior_idx=i, posterior_idx=j).set_index(['prior_idx','posterior_idx','t']) for (i,j),p in samples['posterior'].iterrows()])\n",
        "            if real_data is not None:\n",
        "                R = real_data.reset_index().rename_axis('t')\n",
        "                samples['ensemble'] = samples['ensemble'].join(R).set_index('date',append=True)\n",
        "        return samples\n",
        "\n",
        "    def get_diagnostics(self, n_posteriors=50, refresh=False):\n",
        "        self.diagnostic = self.read_or_create(\n",
        "            file=self.diagnostic_file, refresh=refresh,\n",
        "            fun=lambda: self.draw_samples(n_posteriors=n_posteriors, n_priors=21*n_posteriors, ensemble=False))\n",
        "        self.plot('loss')\n",
        "        self.plot('ecdf', self.diagnostic)\n",
        "        self.plot('hist', self.diagnostic)\n",
        "        self.plot('recovery', self.diagnostic)\n",
        "\n",
        "\n",
        "    def plot(self, kind='loss', samples=None):\n",
        "        if kind == 'loss':\n",
        "            fig = diag.plot_losses(**self.model.loss_history.get_plottable())\n",
        "        else:\n",
        "            opts = {'param_names':self.param_latex, 'post_samples':samples['parameters_out'], 'prior_samples':samples['parameters']}\n",
        "            if kind == 'ecdf':\n",
        "                fig = diag.plot_sbc_ecdf(**opts)\n",
        "            elif kind == 'hist':\n",
        "                fig = diag.plot_sbc_histograms(**opts)\n",
        "            elif kind == 'recovery':\n",
        "                fig = diag.plot_recovery(**opts)\n",
        "            else:\n",
        "                raise Exception(f'Unrecognized kind \"{kind}\"')\n",
        "        fig.savefig(self.model_path / f'{kind}.png')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # def load_data(self, country, start=39):\n",
        "    #     \"\"\"Download and prepare data from Johns Hopkins\"\"\"\n",
        "    #     url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_'\n",
        "    #     fetch = lambda cl: pd.read_csv(url+cl+'_global.csv', sep=\",\").drop(columns=['Province/State','Lat','Long']).groupby('Country/Region').sum().loc[country]\n",
        "    #     real_data = (pd.DataFrame({'dI_real':fetch('confirmed'), 'dR_real':fetch('recovered'), 'dD_real':fetch('deaths')})\n",
        "    #         .assign(date = lambda x: pd.to_datetime(x.index)).set_index('date')\n",
        "    #         .diff().dropna().clip(0, self.tot_pop).astype(int).iloc[start:start+self.n_steps])\n",
        "    #     return real_data\n",
        "\n",
        "    def fetch_data(self, cntry, start=39):\n",
        "        iso = wbgapi.economy.coder(cntry)\n",
        "        assert iso, f'Unrecognized country {cntry}'\n",
        "        tot_pop = next(wbgapi.data.fetch('SP.POP.TOTL', economy=iso, time=2020))['value']\n",
        "\n",
        "        url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_'\n",
        "        fetch = lambda cl: pd.read_csv(url+cl+'_global.csv', sep=\",\").drop(columns=['Province/State','Lat','Long']).groupby('Country/Region').sum().loc[cntry]\n",
        "        real_data = (\n",
        "            pd.DataFrame({'dI_real':fetch('confirmed'), 'dR_real':fetch('recovered'), 'dD_real':fetch('deaths')})\n",
        "            .assign(date = lambda x: pd.to_datetime(x.index)).set_index('date')\n",
        "            .diff().dropna().clip(0, tot_pop).astype(int).iloc[start:start+self.n_steps])\n",
        "        return dict(cntry=cntry, iso=iso, tot_pop=tot_pop, real_data=real_data)\n",
        "\n",
        "\n",
        "    def get_predictive(self, cntry, n_posteriors=500, refresh=False):\n",
        "        cntry_path = self.root_path / cntry\n",
        "        cntry_path.mkdir(exist_ok=True, parents=True)\n",
        "        predictive_file = cntry_path / 'predictive.pkl'\n",
        "        dct = self.fetch_data(cntry)\n",
        "        predictive = self.read_or_create(\n",
        "            file=predictive_file, refresh=refresh,\n",
        "            fun=lambda: self.draw_samples(n_posteriors=n_posteriors, real_data=dct['real_data'], ensemble=True))\n",
        "\n",
        "        fig, ax = plt.subplots(self.n_obs, 1, sharex=True, figsize=(20,10))\n",
        "        fig.suptitle(cntry)\n",
        "        E = predictive['ensemble'].groupby('date')\n",
        "        for i, obs in enumerate(self.obs_classes):\n",
        "            real = obs.replace('obs','real')\n",
        "            ax[i].plot(E[real].median(), 'k.', label='Real')\n",
        "            for ci, clr in {90:'green', 50:'red', 10:'blue'}.items():\n",
        "                x = (100 - ci) / 200\n",
        "                lb = E[obs].quantile(x)\n",
        "                ub = E[obs].quantile(1-x)\n",
        "                ax[i].fill_between(x=lb.index, y1=lb, y2=ub, color=clr, alpha=0.3, label=f'{ci}% Prediction Interval')\n",
        "            ax[i].legend()\n",
        "            ax[i].set_title(obs[:2])\n",
        "        fig.savefig(cntry_path / f'predictive.png')\n",
        "        plt.show()\n",
        "\n",
        "        ode_params = [\n",
        "            'alpha','beta','gamma','delta','epsilon','eta','mu','theta',\n",
        "            'lambda_1','lambda_2','lambda_3','lambda_4','t_1','t_2','t_3','t_4',]\n",
        "        tx = [f'${key}$' if key in ['N','E_0','sim_diff','t_1','t_2','t_3','t_4','A_I','A_R','A_D','lag_I','lag_R','lag_D'] else f'$\\{key}$' for key in ode_params]\n",
        "        pst = predictive['posterior'].assign(kind='posterior')\n",
        "        pir = pd.DataFrame(self.prior(pst.shape[0])['prior_draws'], columns=self.param_names).assign(kind='prior')\n",
        "        Q = pd.concat([pir,pst]).set_index('kind')[ode_params]\n",
        "        Q.columns = tx\n",
        "        M = Q.melt(ignore_index=False).reset_index()\n",
        "        sns.set_palette(\"Set2\")\n",
        "        fig = sns.FacetGrid(M, hue='kind', col='variable', col_wrap=4, sharex=False, sharey=False)\n",
        "        fig.map(sns.histplot, 'value', kde=True, element='step', alpha=0.7)\n",
        "        fig.add_legend()\n",
        "        fig.savefig(cntry_path / f'distributions.png')\n",
        "        plt.show()\n",
        "        return predictive\n",
        "\n",
        "\n",
        "self = COVID(\n",
        "    name='radev_model_03',\n",
        "    # refresh=True,\n",
        "    # n_calibrate = 50,\n",
        ")\n",
        "# h = self.model.train_online(epochs=1000, iterations_per_epoch=32*10, batch_size=32, validation_sims=500)"
      ],
      "metadata": {
        "id": "qoBCndZaQ_UO"
      },
      "id": "qoBCndZaQ_UO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cntry = 'Germany'\n",
        "pred = self.get_predictive(cntry=cntry)#, refresh=True)\n"
      ],
      "metadata": {
        "id": "UW9FGdGvslln"
      },
      "id": "UW9FGdGvslln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = Prior(prior_fun=lambda: [val for key, val in self.prior_fun().items() if key in ode_params])\n",
        "p(10)\n",
        "pred['posterior'][ode_params]"
      ],
      "metadata": {
        "id": "toLF8L63wkey"
      },
      "id": "toLF8L63wkey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ode_params = [\n",
        "    'alpha','beta','gamma','delta','epsilon','eta','mu','theta',\n",
        "    'lambda_0','lambda_1','lambda_2','lambda_3','lambda_4','t_1','t_2','t_3','t_4',]\n",
        "f = diag.plot_posterior_2d(\n",
        "    posterior_draws=pred['posterior'][ode_params],\n",
        "    prior=Prior(prior_fun=lambda: [val for key, val in self.prior_fun().items() if key in ode_params]),\n",
        "    param_names=ode_params,\n",
        "    )"
      ],
      "metadata": {
        "id": "mnaF2piVuBEZ"
      },
      "id": "mnaF2piVuBEZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(pred['posterior'])"
      ],
      "metadata": {
        "id": "Le4dcVO4xu0b"
      },
      "id": "Le4dcVO4xu0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ode_params = [\n",
        "    'alpha','beta','gamma','delta','epsilon','eta','mu','theta',\n",
        "    'lambda_1','lambda_2','lambda_3','lambda_4','t_1','t_2','t_3','t_4',\n",
        "\n",
        "    ]\n",
        "tx = [f'${key}$' if key in ['N','E_0','sim_diff','t_1','t_2','t_3','t_4','A_I','A_R','A_D','lag_I','lag_R','lag_D'] else f'$\\{key}$' for key in ode_params]\n",
        "pst = pred['posterior'].assign(kind='posterior')\n",
        "pir = pd.DataFrame(self.prior(post.shape[0])['prior_draws'], columns=self.param_names).assign(kind='prior')\n",
        "Q = pd.concat([pir,pst]).set_index('kind')[ode_params]\n",
        "Q.columns = tx\n",
        "M = Q.melt(ignore_index=False).reset_index()\n",
        "sns.set_palette(\"Set2\")\n",
        "g = sns.FacetGrid(M, hue='kind', col='variable', col_wrap=4, sharex=False, sharey=False)\n",
        "g.map(sns.histplot, 'value', kde=True, element='step', alpha=0.7)\n",
        "g.add_legend()\n"
      ],
      "metadata": {
        "id": "hvZCf3LQuBLs"
      },
      "id": "hvZCf3LQuBLs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred['posterior'].shape"
      ],
      "metadata": {
        "id": "af358pW1tlQU"
      },
      "id": "af358pW1tlQU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diag.plot_posterior_2d?"
      ],
      "metadata": {
        "id": "4ZeXEV50teXn"
      },
      "id": "4ZeXEV50teXn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDEPJJcRs3e4"
      },
      "id": "BDEPJJcRs3e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv', sep=\",\")\n",
        "for cntry in df['Country/Region'].sample(frac=1).unique():\n",
        "# for cntry in ['Germany','US','Israel','Italy','Sweden']:\n",
        "    try:\n",
        "        self.get_predictive(cntry=cntry)\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "BjIA7PscUH-Q"
      },
      "id": "BjIA7PscUH-Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.get_diagnostics(refresh=True)"
      ],
      "metadata": {
        "id": "Dcs6Q_JPUA81"
      },
      "id": "Dcs6Q_JPUA81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.read_csv(f\n",
        "# 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_{cl}_global.csv', sep=\",\")\n",
        "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_'\n",
        "cl = 'deaths'\n",
        "df = pd.read_csv(url+cl+'_global.csv', sep=\",\").drop(columns=['Province/State','Lat','Long']).groupby('Country/Region').sum().loc[self.country]\n",
        "df#.query('`Country/Region` == @self.country')\n"
      ],
      "metadata": {
        "id": "J5pA-AQZL1xv"
      },
      "id": "J5pA-AQZL1xv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "country = 'US'\n",
        "iso = wbgapi.economy.coder(country)\n",
        "print(iso)\n",
        "tot_pop = next(wbgapi.data.fetch('SP.POP.TOTL', economy=self.iso, time=2020))['value']\n",
        "print(tot_pop)"
      ],
      "metadata": {
        "id": "BCmTaDXsJmoq"
      },
      "id": "BCmTaDXsJmoq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self.get_diagnostics(refresh=True)"
      ],
      "metadata": {
        "id": "a5L1ZS54Q2Fw"
      },
      "id": "a5L1ZS54Q2Fw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.get_predictive(refresh=True)"
      ],
      "metadata": {
        "id": "dhdEeFy4Cu5A"
      },
      "id": "dhdEeFy4Cu5A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wu8CkxW9Q4U"
      },
      "id": "2wu8CkxW9Q4U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E = self.predictive['ensemble']\n",
        "# E['dD_obs'].describe()\n",
        "# E.join(self.real_data.reset_index())\n",
        "# self.real_data.reset_index().rename_axis('t')\n",
        "# E.join(self.real_data.reset_index().rename_axis('t')).set_index('date',append=True)"
      ],
      "metadata": {
        "id": "w9eQLi6I7-BM"
      },
      "id": "w9eQLi6I7-BM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure?"
      ],
      "metadata": {
        "id": "lOc5_2y3B98F"
      },
      "id": "lOc5_2y3B98F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(self.n_obs, 1, sharex=True, figsize=(20,10))\n",
        "fig.suptitle(self.country)\n",
        "E = self.predictive['ensemble'].groupby('date')\n",
        "for i, obs in enumerate(self.obs_classes):\n",
        "    real = obs.replace('obs','real')\n",
        "    ax[i].plot(E[real].median(), 'k.', label='Real')\n",
        "    for ci, clr in {90:'green', 50:'red', 10:'blue'}.items():\n",
        "        x = (100 - ci) / 200\n",
        "        lb = E[obs].quantile(x)\n",
        "        ub = E[obs].quantile(1-x)\n",
        "        ax[i].fill_between(x=lb.index, y1=lb, y2=ub, color=clr, alpha=0.3, label=f'{ci}% Prediction Interval')\n",
        "    ax[i].legend()\n",
        "    ax[i].set_title(obs[:2])\n",
        "fig.savefig(self.cntry_path / f'predictive.png')\n",
        "        # rl = obs.replace('obs','real')\n",
        "        # pr\n"
      ],
      "metadata": {
        "id": "5AkdEyKO6I-t"
      },
      "id": "5AkdEyKO6I-t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot?"
      ],
      "metadata": {
        "id": "NwXbXIgPBb-l"
      },
      "id": "NwXbXIgPBb-l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax[0].fill_between?"
      ],
      "metadata": {
        "id": "uUg48wEIA9xG"
      },
      "id": "uUg48wEIA9xG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for obs in self.obs_classes:\n",
        "#     e = E.groupby('t')[obs]\n",
        "#     for ci in [75, 95]:\n",
        "#         x = (100-ci) / 200\n",
        "#         lb = e.quantile(x)\n",
        "#         ub = e.quantile(1-x)\n",
        "#         plt.plot(lb)\n",
        "\n",
        "\n",
        "\n",
        "    # .agg(median='median', mean='mean', a=quantile)\n",
        "#     e = E.groupby('t')[obs].quantile([0.25,0.75])\n",
        "#     e = E.groupby('t')[obs].describe(percentiles=[0.5,0.9])\n",
        "\n",
        "    # q50 = e.median()\n",
        "    # e\n",
        "\n",
        "    # plt.plot(Q[obs],label=obs)\n",
        "    # rl = obs.replace('obs','real')\n",
        "    # plt.plot(Q[rl],label=rl)\n",
        "    # plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Q.plot()"
      ],
      "metadata": {
        "id": "sNOl2BneM2w_"
      },
      "id": "sNOl2BneM2w_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples['posterior_draws'].shape\n",
        "samples['parameters_out'].shape\n"
      ],
      "metadata": {
        "id": "IGXwhU1rC0fq"
      },
      "id": "IGXwhU1rC0fq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = pd.concat([pd.concat([pd.DataFrame([p], columns=self.param_names).assign(prior_idx=prior_idx, posterior_idx=posterior_idx).set_index(['prior_idx','posterior_idx']) for posterior_idx, p in enumerate(P) if self.prep_params(p) is not None]) for prior_idx, P in enumerate(samples['posterior_draws'])])\n",
        "P"
      ],
      "metadata": {
        "id": "oVOynVFEJ7xH"
      },
      "id": "oVOynVFEJ7xH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples['valid_draws'] = [[p for p in d if self.prep_params(p) is not None] for d in samples['posterior_draws']]\n",
        "samples['posterior'] = pd.concat([pd.concat([pd.DataFrame([p], columns=self.param_names).assign(prior_idx=i, posterior_idx=j).set_index(['prior_idx','posterior_idx']) for j, p in enumerate(d)]) for i, d in enumerate(samples['valid_draws'])])\n",
        "samples['ensemble'] = pd.concat([self.sir(p).reset_index().assign(prior_idx=i, posterior_idx=j).set_index(['prior_idx','posterior_idx','t']) for (i,j),p in samples['posterior'].iterrows()])\n",
        "E"
      ],
      "metadata": {
        "id": "NrQKV4I8KStc"
      },
      "id": "NrQKV4I8KStc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = pd.concat([pd.concat([pd.DataFrame([p], columns=self.param_names).assign(prior_idx=prior_idx, posterior_idx=posterior_idx).set_index(['prior_idx','posterior_idx']) for posterior_idx, p in enumerate(P)]) for prior_idx, P in enumerate(samples['posterior_draws'])])\n",
        "# self.check_params(P.iloc[0].values)\n",
        "# dict(zip(self.param_names,P.iloc[0].values))\n",
        "# self.prep_params(P.iloc[0])\n",
        "# P.to_dict('records')[0]\n",
        "\n",
        "g = lambda p: self.prep_params(p) is not None\n",
        "# P['valid'] = P.apply(g, axis=1)\n",
        "# P.query('valid')\n",
        "P[P.apply(g, axis=1)]"
      ],
      "metadata": {
        "id": "DEvY_X6gEOBd"
      },
      "id": "DEvY_X6gEOBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.prior(1)['prior_draws'].shape"
      ],
      "metadata": {
        "id": "zzC4W7ALFnOO"
      },
      "id": "zzC4W7ALFnOO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_posteriors = 10\n",
        "samples = self.pre_amortizer({'real_data':self.load_data()})\n",
        "samples['parameters_out'] = self.model.amortizer.sample(samples, n_posteriors)"
      ],
      "metadata": {
        "id": "XJT_L09xB3jW"
      },
      "id": "XJT_L09xB3jW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = self.draw_samples(n_priors=5, n_posteriors=7)"
      ],
      "metadata": {
        "id": "cLXxdPyE-xMq"
      },
      "id": "cLXxdPyE-xMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.plot('recovery',samples)"
      ],
      "metadata": {
        "id": "Sh_dg8UB-4gn"
      },
      "id": "Sh_dg8UB-4gn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.plot('loss')"
      ],
      "metadata": {
        "id": "pB6MEY2v1V0p"
      },
      "id": "pB6MEY2v1V0p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self."
      ],
      "metadata": {
        "id": "-RJUB2MaBuvQ"
      },
      "id": "-RJUB2MaBuvQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = self.model.configurator(self.model.generative_model(300))\n",
        "samples['parameters_out'] = self.model.amortizer.sample(samples, 500)\n",
        "samples['posterior_draws'] = samples['parameters_out'] * self.calibration['prior_std'] + self.calibration['prior_mean']\n",
        "# samples = self.post_amortizer(samples)\n",
        "samples.keys()"
      ],
      "metadata": {
        "id": "ARm-kX3p23Eo"
      },
      "id": "ARm-kX3p23Eo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self.calibration['prior_draws'].shape, self.calibration['sim_data'].shape\n",
        "# f = diag.plot_sbc_ecdf(samples['posterior_draws'], samples[\"parameters\"], param_names=self.param_names)\n",
        "f = diag.plot_recovery(post_samples=samples['parameters_out'], prior_samples=samples['parameters'], param_names=self.param_names)\n",
        "\n",
        "#   (samples['posterior_draws'], samples[\"parameters\"]"
      ],
      "metadata": {
        "id": "vBTy5Y_3tQhT"
      },
      "id": "vBTy5Y_3tQhT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(self, n_priors=2, n_sims=None, n_samples=1):\n",
        "    n_sims = n_sims if n_sims else 20*n_priors\n",
        "    samples = self.generate_data(n_priors=n_priors, n_sims=n_sims)\n",
        "    samples = self.pre_amortizer(samples)\n",
        "    samples['parameters_out'] = self.model.amortizer.sample(samples, n_samples=n_samples)\n",
        "    samples = self.post_amortizer(samples)\n",
        "    return samples\n",
        "    # return self.post_amortizer(dct)"
      ],
      "metadata": {
        "id": "2d_yicE5giSs"
      },
      "id": "2d_yicE5giSs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = self.generator(5)\n",
        "'sim_data' in w\n",
        "# hasattr(w,'sim_data')\n",
        "# w.keys()\n"
      ],
      "metadata": {
        "id": "gyHWd78xfj5W"
      },
      "id": "gyHWd78xfj5W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = np.mean\n",
        "dir(f)\n",
        "f.__name__"
      ],
      "metadata": {
        "id": "ZTwqdft6c_NZ"
      },
      "id": "ZTwqdft6c_NZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = self.calibration\n",
        "c.keys()\n",
        "c['obs_mean'].shape"
      ],
      "metadata": {
        "id": "ljvJP4mecedM"
      },
      "id": "ljvJP4mecedM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = RNG.uniform(size=[2,3])\n",
        "fun = np.mean\n",
        "fun(a,axis=1)\n",
        "np.apply_over_axes('sum', a, 0)"
      ],
      "metadata": {
        "id": "4Av3YQWiazLv"
      },
      "id": "4Av3YQWiazLv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.generator(10)['prior_draws'].shape, self.generator(10)['sim_data'].shape"
      ],
      "metadata": {
        "id": "xSjS-RZRaLVo"
      },
      "id": "xSjS-RZRaLVo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate():\n",
        "    c = self.generator(n_priors=self.n_calibrate, n_sims=1)\n",
        "    d = {'prior':c['prior_samples'], 'data':pd.concat(c['data']).groupby('t')}\n",
        "    funs = ['mean','std','min','max']\n",
        "    c |= {f'{key}_{fun}': np.float32(val.agg(fun)) for fun in funs for key, val in d.items()}\n",
        "    c |= {f'obs_{fun}'  : c[f'data_{fun}'][...,-self.n_obs:] for fun in funs}\n",
        "    # c |= {f'prior_{fun}': c[f'prior_{fun}'].values for fun in funs}\n",
        "\n",
        "    # c |= {f'obs_{fun}_tf'  : self.to_tf(c[f'data_{fun}' ], 3, True ) for fun in funs}\n",
        "    # c |= {f'prior_{fun}_tf': self.to_tf(c[f'prior_{fun}'], 2, False) for fun in funs}\n",
        "    return c\n"
      ],
      "metadata": {
        "id": "0NbGKsQRaGip"
      },
      "id": "0NbGKsQRaGip",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.obs_classes"
      ],
      "metadata": {
        "id": "D9EBMnklZsuK"
      },
      "id": "D9EBMnklZsuK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior_samples = [self.prior_fun() for _ in range(100)]\n",
        "self.sir(prior_samples[0])"
      ],
      "metadata": {
        "id": "8_g32OxcXjNy"
      },
      "id": "8_g32OxcXjNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(self.generator(n_priors=2, n_sims=3)['data'][0])\n",
        "# np.float32(self.generator(n_priors=2, n_sims=3)['data'])\n",
        "samples = self.pre_amortizer(self.generator(n_priors=2, n_sims=3))\n",
        "samples = self.pre_amortizer(self.generator(n_priors=2, n_sims=3))\n",
        "{key:type(val) for key,val in samples.items()}\n",
        "{key:val.shape for key,val in samples.items() if key != 'data'}"
      ],
      "metadata": {
        "id": "r3qZplbIUYGH"
      },
      "id": "r3qZplbIUYGH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.prior(10000)"
      ],
      "metadata": {
        "id": "syY2A7e7TlU7"
      },
      "id": "syY2A7e7TlU7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.prior(10000)"
      ],
      "metadata": {
        "id": "V-dHSipkRow5"
      },
      "id": "V-dHSipkRow5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autotime\n",
        "import os, datetime, pathlib, shutil, google.colab, dataclasses, pickle, wbgapi\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "import numpy as np, pandas as pd, tensorflow as tf\n",
        "from functools import partial\n",
        "from scipy import stats\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, LSTM\n",
        "from bayesflow.networks import InvertibleNetwork, SequentialNetwork\n",
        "from bayesflow.coupling_networks import CouplingLayer\n",
        "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
        "from bayesflow.amortizers import AmortizedLikelihood, AmortizedPosterior, AmortizedPosteriorLikelihood\n",
        "from bayesflow.trainers import Trainer\n",
        "from bayesflow import default_settings\n",
        "from bayesflow.helper_functions import build_meta_dict\n",
        "import bayesflow.diagnostics as diag\n",
        "from bayesflow.computational_utilities import maximum_mean_discrepancy\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "plt.rcParams.update({\"text.usetex\": False, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{{amsmath}}\"})\n",
        "mnt = '/content/drive'\n",
        "google.colab.drive.mount(mnt)\n",
        "RNG = np.random.default_rng(42)\n",
        "# SCALE = 1000\n",
        "EPS = 1e-6\n",
        "\n",
        "class MultiConvLayer(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_filters=32, strides=1):\n",
        "        super(MultiConvLayer, self).__init__()\n",
        "\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(\n",
        "                n_filters // 2,\n",
        "                kernel_size=f,\n",
        "                strides=strides,\n",
        "                padding=\"causal\",\n",
        "                activation=\"relu\",\n",
        "                kernel_initializer=\"glorot_uniform\",\n",
        "            )\n",
        "            for f in range(2, 8)\n",
        "        ]\n",
        "        self.dim_red = tf.keras.layers.Conv1D(\n",
        "            n_filters, 1, 1, activation=\"relu\", kernel_initializer=\"glorot_uniform\"\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = tf.concat([conv(x) for conv in self.convs], axis=-1)\n",
        "        out = self.dim_red(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiConvNet(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_layers=3, n_filters=64, strides=1):\n",
        "        super(MultiConvNet, self).__init__()\n",
        "\n",
        "        self.net = tf.keras.Sequential(\n",
        "            [MultiConvLayer(n_filters, strides) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        self.lstm = LSTM(n_filters)\n",
        "\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = self.net(x)\n",
        "        out = self.lstm(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SummaryNet(tf.keras.Model):\n",
        "    def __init__(self, n_summary):\n",
        "        super(SummaryNet, self).__init__()\n",
        "        self.net_I = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_R = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_D = MultiConvNet(n_filters=n_summary // 3)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        x = tf.split(x, 3, axis=-1)\n",
        "        x_i = self.net_I(x[0])\n",
        "        x_r = self.net_R(x[1])\n",
        "        x_d = self.net_D(x[2])\n",
        "        return tf.concat([x_i, x_r, x_d], axis=-1)\n",
        "\n",
        "class MemoryNetwork(tf.keras.Model):\n",
        "    def __init__(self, meta):\n",
        "        super(MemoryNetwork, self).__init__()\n",
        "\n",
        "        self.gru = GRU(meta[\"n_hidden\"], return_sequences=True, return_state=True)\n",
        "        self.h = meta[\"n_hidden\"]\n",
        "        self.n_params = meta[\"n_params\"]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, target, condition):\n",
        "        \"\"\"Performs a forward pass through the network.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        target    : tf.Tesnor of shape (batch_size, time_stes, dim)\n",
        "            The time-dependent signal to process.\n",
        "        condition : tf.Tensor of shape (batch_size, cond_dim)\n",
        "            The conditional (static) variables, e.g., parameters.\n",
        "        \"\"\"\n",
        "        shift_target = target[:, :-1, :]\n",
        "        init = tf.zeros((target.shape[0], 1, target.shape[2]))\n",
        "        inp_teacher = tf.concat([init, shift_target], axis=1)\n",
        "        inp_teacher_c = tf.concat([inp_teacher, condition], axis=-1)\n",
        "        out, _ = self.gru(inp_teacher_c)\n",
        "        return out\n",
        "\n",
        "    def step_loop(self, target, condition, state):\n",
        "        out, new_state = self.gru(\n",
        "            tf.concat([target, condition], axis=-1), initial_state=state\n",
        "        )\n",
        "        return out, new_state\n",
        "\n",
        "class InvertibleNetworkWithMemory(tf.keras.Model):\n",
        "    \"\"\"Implements a chain of conditional invertible blocks for Bayesian parameter inference.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_params,\n",
        "        num_coupling_layers=4,\n",
        "        coupling_settings=None,\n",
        "        coupling_design=\"affine\",\n",
        "        permutation=\"fixed\",\n",
        "        use_act_norm=True,\n",
        "        act_norm_init=None,\n",
        "        use_soft_flow=False,\n",
        "        soft_flow_bounds=(1e-3, 5e-2),\n",
        "    ):\n",
        "        \"\"\"Initializes a custom invertible network with recurrent memory.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Create settings dict for coupling layer\n",
        "        settings = dict(\n",
        "            latent_dim=num_params,\n",
        "            coupling_settings=coupling_settings,\n",
        "            coupling_design=coupling_design,\n",
        "            permutation=permutation,\n",
        "            use_act_norm=use_act_norm,\n",
        "            act_norm_init=act_norm_init,\n",
        "        )\n",
        "\n",
        "        # Create sequence of coupling layers and store reference to dimensionality\n",
        "        self.coupling_layers = [\n",
        "            CouplingLayer(**settings) for _ in range(num_coupling_layers)\n",
        "        ]\n",
        "\n",
        "        # Store attributes\n",
        "        self.soft_flow = use_soft_flow\n",
        "        self.soft_low = soft_flow_bounds[0]\n",
        "        self.soft_high = soft_flow_bounds[1]\n",
        "        self.use_act_norm = use_act_norm\n",
        "        self.latent_dim = num_params\n",
        "        self.dynamic_summary_net = MemoryNetwork({\"n_hidden\": 256, \"n_params\": 3})\n",
        "        self.latent_dim = num_params\n",
        "\n",
        "    def call(self, targets, condition, inverse=False):\n",
        "        \"\"\"Performs one pass through an invertible chain (either inverse or forward).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        targets   : tf.Tensor\n",
        "            The estimation quantities of interest, shape (batch_size, ...)\n",
        "        condition : tf.Tensor\n",
        "            The conditional data x, shape (batch_size, summary_dim)\n",
        "        inverse   : bool, default: False\n",
        "            Flag indicating whether to run the chain forward or backwards\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (z, log_det_J)  :  tuple(tf.Tensor, tf.Tensor)\n",
        "            If inverse=False: The transformed input and the corresponding Jacobian of the transformation,\n",
        "            v shape: (batch_size, ...), log_det_J shape: (batch_size, ...)\n",
        "\n",
        "        target          :  tf.Tensor\n",
        "            If inverse=True: The transformed out, shape (batch_size, ...)\n",
        "\n",
        "        Important\n",
        "        ---------\n",
        "        If ``inverse=False``, the return is ``(z, log_det_J)``.\\n\n",
        "        If ``inverse=True``, the return is ``target``.\n",
        "        \"\"\"\n",
        "\n",
        "        if inverse:\n",
        "            return self.inverse(targets, condition)\n",
        "        return self.forward(targets, condition)\n",
        "\n",
        "    @tf.function\n",
        "    def forward(self, targets, condition, **kwargs):\n",
        "        \"\"\"Performs a forward pass though the chain.\"\"\"\n",
        "\n",
        "        # Add memory condition\n",
        "        memory = self.dynamic_summary_net(targets, condition)\n",
        "        condition = tf.concat([memory, condition], axis=-1)\n",
        "\n",
        "        z = targets\n",
        "        log_det_Js = []\n",
        "        for layer in self.coupling_layers:\n",
        "            z, log_det_J = layer(z, condition, **kwargs)\n",
        "            log_det_Js.append(log_det_J)\n",
        "        # Sum Jacobian determinants for all layers (coupling blocks) to obtain total Jacobian.\n",
        "        log_det_J = tf.add_n(log_det_Js)\n",
        "        return z, log_det_J\n",
        "\n",
        "    @tf.function\n",
        "    def inverse(self, z, condition, **kwargs):\n",
        "        \"\"\"Performs a reverse pass through the chain.\"\"\"\n",
        "\n",
        "        target = z\n",
        "        T = z.shape[1]\n",
        "        gru_inp = tf.zeros((z.shape[0], 1, z.shape[-1]))\n",
        "        state = tf.zeros((z.shape[0], self.dynamic_summary_net.h))\n",
        "        outs = []\n",
        "        for t in range(T):\n",
        "            # One step condition\n",
        "            memory, state = self.dynamic_summary_net.step_loop(\n",
        "                gru_inp, condition[:, t : t + 1, :], state\n",
        "            )\n",
        "            condition_t = tf.concat([memory, condition[:, t : t + 1, :]], axis=-1)\n",
        "            target_t = target[:, t : t + 1, :]\n",
        "            for layer in reversed(self.coupling_layers):\n",
        "                target_t = layer(target_t, condition_t, inverse=True, **kwargs)\n",
        "            outs.append(target_t)\n",
        "            gru_inp = target_t\n",
        "        return tf.concat(outs, axis=1)\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class COVID():\n",
        "    country: str = 'Germany'\n",
        "    name: str = 'covid_000'\n",
        "    n_steps: int = 100\n",
        "    n_calibrate: int = 5000\n",
        "    refresh: bool = False\n",
        "\n",
        "    def load_data(self, start=39):\n",
        "        \"\"\"Download and prepare data from Johns Hopkins\"\"\"\n",
        "        def fetch(cl):\n",
        "            return (\n",
        "                pd.read_csv(f'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_{cl}_global.csv', sep=\",\")\n",
        "                .drop(columns=['Province/State','Lat','Long'])\n",
        "                .groupby('Country/Region').sum()\n",
        "                .loc[self.country]\n",
        "            )\n",
        "        data = (\n",
        "            pd.DataFrame(\n",
        "            {'dI_real':fetch('confirmed'), 'dR_real':fetch('recovered'), 'dD_real':fetch('deaths')})\n",
        "            .assign(t = lambda x: pd.to_datetime(x.index)).set_index('t')\n",
        "            .diff().dropna().clip(0, self.tot_pop).astype(int).iloc[start:start+self.n_steps]\n",
        "            )\n",
        "        return data\n",
        "\n",
        "    def read_or_create(self, file, fun=None, refresh=False):\n",
        "        \"\"\"Read or create pickle for simulation results\"\"\"\n",
        "        try:\n",
        "            assert not refresh\n",
        "            with open(file, \"rb\") as f:\n",
        "                sims = pickle.load(f)\n",
        "            print(f'{file} successfully read')\n",
        "        except Exception as e:\n",
        "            print(f'Running sims due to error: {e}')\n",
        "            with open(file, \"wb\") as f:\n",
        "                sims = fun()\n",
        "                pickle.dump(sims, f)\n",
        "        return sims\n",
        "\n",
        "\n",
        "    def check_params(self, p):\n",
        "        for key in ['E_0','sim_diff','t_1','t_2','t_3','t_4','t_5','delta_1','delta_2','delta_3','delta_4','lag_I','lag_R','lag_D']:\n",
        "            p[key] = int(round(p[key]))\n",
        "        p['E_0'] = max(p['E_0'], 1)\n",
        "        if all([\n",
        "            all(val > EPS for key, val in p.items() if key[:3] != 'phi'),\n",
        "            p['alpha'] < 1 - EPS,\n",
        "            p['delta'] < 1 - EPS,\n",
        "            p['sim_diff'] > max(p['lag_I'],p['lag_R'],p['lag_D']),\n",
        "            *[p[f't_{i}'] + p[f'delta_{i}'] <= p[f't_{i+1}'] for i in range(1,5)],\n",
        "        ]):\n",
        "            return p\n",
        "\n",
        "    def prior_fun(self, batch_size=1):\n",
        "        alpha_f = (0.7**2) * ((1 - 0.7) / (0.17**2) - (1 - 0.7))\n",
        "        beta_f = alpha_f * (1 / 0.7 - 1)\n",
        "        L = []\n",
        "        while len(L) < batch_size:\n",
        "            p = self.check_params({\n",
        "                'N'       :self.tot_pop,\n",
        "                'E_0'     :RNG.gamma(shape=2, scale=30),\n",
        "                'alpha'   :RNG.uniform(low=0.005, high=0.99),\n",
        "                'beta'    :RNG.lognormal(mean=np.log(0.25), sigma=0.3),\n",
        "                'gamma'   :RNG.lognormal(mean=np.log(1/6.5), sigma=0.5),\n",
        "                'delta'   :RNG.uniform(low=0.01, high=0.3),\n",
        "                'epsilon' :RNG.uniform(low=1/14, high=1/3),\n",
        "                'eta'     :RNG.lognormal(mean=np.log(1/3.2), sigma=0.5),\n",
        "                'lambda'  :RNG.lognormal(mean=np.log(1.2), sigma=0.5),\n",
        "                'mu'      :RNG.lognormal(mean=np.log(1/8), sigma=0.2),\n",
        "                'theta'   :RNG.uniform(low=1/14, high=1/3),\n",
        "                'sim_diff':16,\n",
        "                't_1'     :RNG.normal(loc=8, scale=3),\n",
        "                't_2'     :RNG.normal(loc=15, scale=3),\n",
        "                't_3'     :RNG.normal(loc=22, scale=3),\n",
        "                't_4'     :RNG.normal(loc=66, scale=3),\n",
        "                't_5'     :self.n_steps,\n",
        "                'delta_1' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_2' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_3' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_4' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'lambda_0':RNG.lognormal(mean=np.log(1.20), sigma=0.5),\n",
        "                'lambda_1':RNG.lognormal(mean=np.log(0.60), sigma=0.5),\n",
        "                'lambda_2':RNG.lognormal(mean=np.log(0.30), sigma=0.5),\n",
        "                'lambda_3':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "                # 'lambda_4':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "                'lambda_4':RNG.lognormal(mean=np.log(0.15), sigma=0.5),\n",
        "                'f_I'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'f_R'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'f_D'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'phi_I'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'phi_R'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'phi_D'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'lag_I'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'lag_R'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'lag_D'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'sigma_I' :RNG.gamma(shape=1, scale=5),\n",
        "                'sigma_R' :RNG.gamma(shape=1, scale=5),\n",
        "                'sigma_D' :RNG.gamma(shape=1, scale=5),\n",
        "            })\n",
        "            if p:\n",
        "                L.append(p)\n",
        "        return pd.DataFrame(L).rename_axis('prior_idx')\n",
        "        # return L)\n",
        "\n",
        "    def calc_lambda_array(self, p):\n",
        "        \"\"\"Computes the array of time-varying contact rates/transimission probabilities.\"\"\"\n",
        "        # Array of initial lambdas\n",
        "        lambd0_arr = np.array([p['lambda_0']] * (p['t_1'] + p['sim_diff'] - 1))\n",
        "\n",
        "        # Compute lambd1 array\n",
        "        if p['delta_1'] == 1:\n",
        "            lambd1_arr = np.array([p['lambda_1']] * (p['t_2'] - p['t_1']))\n",
        "        else:\n",
        "            lambd1_arr = np.linspace(p['lambda_0'], p['lambda_1'], p['delta_1'])\n",
        "            lambd1_arr = np.append(lambd1_arr, [p['lambda_1']] * (p['t_2'] - p['t_1'] - p['delta_1']))\n",
        "\n",
        "        # Compute lambd2 array\n",
        "        if p['delta_2'] == 1:\n",
        "            lambd2_arr = np.array([p['lambda_2']] * (p['t_3'] - p['t_2']))\n",
        "        else:\n",
        "            lambd2_arr = np.linspace(p['lambda_1'], p['lambda_2'], p['delta_2'])\n",
        "            lambd2_arr = np.append(lambd2_arr, [p['lambda_2']] * (p['t_3'] - p['t_2'] - p['delta_2']))\n",
        "\n",
        "        # Compute lambd3 array\n",
        "        if p['delta_3'] == 1:\n",
        "            lambd3_arr = np.array([p['lambda_3']] * (p['t_4'] - p['t_3']))\n",
        "        else:\n",
        "            lambd3_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_3'])\n",
        "            lambd3_arr = np.append(lambd3_arr, [p['lambda_3']] * (p['t_4'] - p['t_3'] - p['delta_3']))\n",
        "\n",
        "        # Compute lambd4 array\n",
        "        if p['delta_4'] == 1:\n",
        "            lambd4_arr = np.array([p['lambda_4']] * (p['t_5'] - p['t_4']))\n",
        "        else:\n",
        "            lambd4_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_4'])\n",
        "            lambd4_arr = np.append(lambd4_arr, [p['lambda_4']] * (p['t_5'] - p['t_4'] - p['delta_4']))\n",
        "\n",
        "        return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
        "\n",
        "\n",
        "    def sir(self, prior_draw):\n",
        "        try:\n",
        "            p = self.const_params | prior_draw  # if prior_values was passed as dict\n",
        "        except:\n",
        "            p = self.const_params | dict(zip(self.param_names, prior_draw))  # if prior_values was passed as list\n",
        "        assert self.check_params(p)\n",
        "        sim_lag = p['sim_diff'] - 1\n",
        "        lambd_arr = self.calc_lambda_array(p)\n",
        "\n",
        "        # Initial conditions\n",
        "        S, E, C, I, R, D = [self.tot_pop - p['E_0']], [p['E_0']], [0], [0], [0], [0]\n",
        "\n",
        "        # Containers\n",
        "        I_news = []\n",
        "        R_news = []\n",
        "        D_news = []\n",
        "\n",
        "        # Reported new cases\n",
        "        I_data = np.zeros(p['t_5'])\n",
        "        R_data = np.zeros(p['t_5'])\n",
        "        D_data = np.zeros(p['t_5'])\n",
        "        fs_I = np.zeros(p['t_5'])\n",
        "        fs_R = np.zeros(p['t_5'])\n",
        "        fs_D = np.zeros(p['t_5'])\n",
        "\n",
        "        # Simulate T-1 tiemsteps\n",
        "        for t in range(p['t_5'] + sim_lag):\n",
        "            # Calculate new exposed cases\n",
        "            E_new = lambd_arr[t] * ((C[t] + p['beta'] * I[t]) / self.tot_pop) * S[t]\n",
        "\n",
        "            # Remove exposed from susceptible\n",
        "            S_t = S[t] - E_new\n",
        "\n",
        "            # Calculate current exposed by adding new exposed and\n",
        "            # subtracting the exposed becoming carriers.\n",
        "            E_t = E[t] + E_new - p['gamma'] * E[t]\n",
        "\n",
        "            # Calculate current carriers by adding the new exposed and subtracting\n",
        "            # those who will develop symptoms and become detected and those who\n",
        "            # will go through the disease asymptomatically.\n",
        "            C_t = C[t] + p['gamma'] * E[t] - (1 - p['alpha']) * p['eta'] * C[t] - p['alpha'] * p['theta'] * C[t]\n",
        "\n",
        "            # Calculate current infected by adding the symptomatic carriers and\n",
        "            # subtracting the dead and recovered. The newly infected are just the\n",
        "            # carriers who get detected.\n",
        "            I_t = I[t] + (1 - p['alpha']) * p['eta'] * C[t] - (1 - p['delta']) * p['mu'] * I[t] - p['delta'] * p['epsilon'] * I[t]\n",
        "            I_new = (1 - p['alpha']) * p['eta'] * C[t]\n",
        "\n",
        "            # Calculate current recovered by adding the symptomatic and asymptomatic\n",
        "            # recovered. The newly recovered are only the detected recovered\n",
        "            R_t = R[t] + p['alpha'] * p['theta'] * C[t] + (1 - p['delta']) * p['mu'] * I[t]\n",
        "            R_new = (1 - p['delta']) * p['mu'] * I[t]\n",
        "\n",
        "            # Calculate the current dead\n",
        "            D_t = D[t] + p['delta'] * p['epsilon'] * I[t]\n",
        "            D_new = p['delta'] * p['epsilon'] * I[t]\n",
        "\n",
        "            # Ensure some numerical onstraints\n",
        "            S_t = np.clip(S_t, 0, self.tot_pop)\n",
        "            E_t = np.clip(E_t, 0, self.tot_pop)\n",
        "            C_t = np.clip(C_t, 0, self.tot_pop)\n",
        "            I_t = np.clip(I_t, 0, self.tot_pop)\n",
        "            R_t = np.clip(R_t, 0, self.tot_pop)\n",
        "            D_t = np.clip(D_t, 0, self.tot_pop)\n",
        "\n",
        "            # Keep track of process over time\n",
        "            S.append(S_t)\n",
        "            E.append(E_t)\n",
        "            C.append(C_t)\n",
        "            I.append(I_t)\n",
        "            R.append(R_t)\n",
        "            D.append(D_t)\n",
        "            I_news.append(I_new)\n",
        "            R_news.append(R_new)\n",
        "            D_news.append(D_new)\n",
        "\n",
        "            # From here, start adding new cases with delay D\n",
        "            # Note, we assume the same delay\n",
        "            if t >= sim_lag:\n",
        "                # Compute lags and add to data arrays\n",
        "                fs_I[t - sim_lag] = (1 - p['f_I']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * p['phi_I']))\n",
        "                )\n",
        "                fs_R[t - sim_lag] = (1 - p['f_R']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * p['phi_R']))\n",
        "                )\n",
        "                fs_D[t - sim_lag] = (1 - p['f_D']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * p['phi_D']))\n",
        "                )\n",
        "                I_data[t - sim_lag] = I_news[t - p['lag_I']]\n",
        "                R_data[t - sim_lag] = R_news[t - p['lag_R']]\n",
        "                D_data[t - sim_lag] = D_news[t - p['lag_D']]\n",
        "\n",
        "        # Compute weekly modulation\n",
        "        I_data = (1 - fs_I) * I_data\n",
        "        R_data = (1 - fs_R) * R_data\n",
        "        D_data = (1 - fs_D) * D_data\n",
        "\n",
        "        # Add noise\n",
        "        I_data = I_data + RNG.standard_t(4) * np.sqrt(I_data) * p['sigma_I']\n",
        "        R_data = R_data + RNG.standard_t(4) * np.sqrt(R_data) * p['sigma_R']\n",
        "        D_data = D_data + RNG.standard_t(4) * np.sqrt(D_data) * p['sigma_D']\n",
        "        n = I_data.shape[0]\n",
        "        return (\n",
        "            pd.DataFrame({'S':S[-n:],'E':E[-n:],'C':C[-n:],'I':I[-n:],'R':R[-n:],'D':D[-n:],'dI_obs':I_data,'dR_obs':R_data,'dD_obs':D_data})\n",
        "            .clip(0, self.tot_pop).rename_axis('t'))\n",
        "\n",
        "\n",
        "    # def generate_data(self, n_priors=1, n_sims=1):\n",
        "    #     prior = self.prior(n_priors).reset_index()\n",
        "    #     prior = pd.concat([prior.assign(sim_idx=j).set_index(['prior_idx','sim_idx']) for j in range(n_sims)])\n",
        "    #     data = [self.sir(p).reset_index().assign(prior_idx=i, sim_idx=j).set_index(['prior_idx','sim_idx','t']) for (i,j), p in prior.iterrows()]\n",
        "    #     return dict(prior=prior, data=data)\n",
        "    #     # return self.prep_data(prior, data)\n",
        "\n",
        "    # def get_real_data(self):\n",
        "    #     prior = pd.DataFrame([np.full(len(self.param_names), np.nan)], columns=self.param_names)\n",
        "    #     data = [self.load_data().reset_index().assign(sim_idx=0, prior_idx=0).set_index(['sim_idx','prior_idx','t'])]\n",
        "    #     return dict(prior=prior, data=data)\n",
        "    #     # return self.prep_data(prior, data)\n",
        "\n",
        "    # # def prep_data(self, prior, data):\n",
        "    # #     return dict(prior=prior, prior_draws=self.to_tf(prior, 2, False), data=pd.concat(data), obs=self.to_tf(data, 3, True))\n",
        "\n",
        "    # def pre_amortizer(self, dct):\n",
        "    #     # dct['prior_samples'] = np.float32(dct['prior'])\n",
        "    #     # dct['ensemble'] = pd.concat(dct['data'])\n",
        "    #     # dct['data'] = np.float32(dct['data'])\n",
        "\n",
        "    #     dct['summary_conditions'] = (np.float32(dct['data'])[...,-self.n_obs:] - self.calibration['obs_mean']) / self.calibration['obs_std']\n",
        "    #     dct['parameters'] = (np.float32(dct['prior']) - self.calibration['prior_mean']) / self.calibration['prior_std']\n",
        "\n",
        "    #     # dct['summary_conditions'] = (self.to_tf(dct['data'], 3, True) - self.calibration['obs_mean_tf']) / self.calibration['obs_std_tf']\n",
        "    #     # dct['parameters'] = (self.to_tf(dct['prior'], 2, False) - self.calibration['prior_mean_tf']) / self.calibration['prior_std_tf']\n",
        "\n",
        "\n",
        "    #     return dct\n",
        "\n",
        "    # def post_amortizer(self, dct):\n",
        "    #     dct['posterior_samples'] = np.float32(dct['parameters_out']) * self.calibration['prior_std'] + self.calibration['prior_mean']\n",
        "    #     return dct\n",
        "\n",
        "    # def generate_samples(self, n_priors=2, n_sims=None, n_samples=1):\n",
        "    #     n_sims = n_sims if n_sims else 20*n_priors\n",
        "    #     samples = self.generate_data(n_priors=n_priors, n_sims=n_sims)\n",
        "    #     samples = self.pre_amortizer(samples)\n",
        "    #     samples['parameters_out'] = self.model.amortizer.sample(samples, n_samples=n_samples)\n",
        "    #     samples = self.post_amortizer(samples)\n",
        "    #     return samples\n",
        "    #     # return self.post_amortizer(dct)\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.root_path = pathlib.Path(mnt + f'/MyDrive/bayesian disease modeling/{self.name}')\n",
        "        if self.refresh:\n",
        "            # delete root_path and everything in it\n",
        "            shutil.rmtree(self.root_path, ignore_errors=True)\n",
        "        self.model_path = self.root_path / f'model/'\n",
        "        self.model_path.mkdir(exist_ok=True, parents=True)\n",
        "        self.file = {key: self.model_path / f'{key}.pkl' for key in ['calibration','diagnostic','predictive','error']}\n",
        "\n",
        "        self.iso = wbgapi.economy.coder(self.country)\n",
        "        assert self.iso, f'Unrecognized country {self.country}'\n",
        "        self.tot_pop = next(wbgapi.data.fetch('SP.POP.TOTL', economy=self.iso, time=2020))['value']\n",
        "        self.path = self.root_path / self.iso\n",
        "        self.path.mkdir(exist_ok=True)\n",
        "\n",
        "        prior_samples = self.prior_fun(10)\n",
        "        self.const_params = prior_samples.agg(['mean','std']).T.query('std < 1e-5')['mean'].to_dict()\n",
        "        self.param_names = [key for key in prior_samples.columns if key not in self.const_params]\n",
        "        self.param_latex = [f'${key}$' if key in ['E_0','lag_I','lag_R','lag_D'] else f'$\\{key}$' for key in self.const_params]\n",
        "        self.n_params = len(self.param_names)\n",
        "        self.obs_classes = self.sir(self.prior().iloc[0]).filter(like='obs').columns.tolist()\n",
        "\n",
        "        # self.prior = lambda batch_size=1: self.prior_fun(batch_size)[self.param_names]\n",
        "        self.prior = Prior(batch_prior_fun=lambda batch_size=1: self.prior_fun(batch_size)[self.param_names], param_names=self.param_names)\n",
        "        self.simulator = Simulator(simulator_fun=self.sir)\n",
        "        self.generator = GenerativeModel(self.prior, self.simulator)\n",
        "\n",
        "    #     self.n_obs = len(self.obs_classes)\n",
        "    #     self.calibration = self.read_or_create(file=self.file['calibration'], fun=self.calibrate)\n",
        "\n",
        "    #     coupling_settings = {\n",
        "    #         \"dense_args\": dict(units=128, activation=\"swish\", kernel_regularizer=None),\n",
        "    #         \"num_dense\": 2,\n",
        "    #         \"dropout\": False,\n",
        "    #     }\n",
        "    #     summary_net = SummaryNet(n_summary=192)\n",
        "    #     # summary_net = SequentialNetwork()\n",
        "    #     inference_net = InvertibleNetwork(\n",
        "    #         num_params=len(self.param_names),\n",
        "    #         num_coupling_layers=6,\n",
        "    #         coupling_settings=coupling_settings,\n",
        "    #     )\n",
        "    #     self.model = Trainer(\n",
        "    #         # generative_model = lambda n: self.generate_data(n_prior=n),\n",
        "    #         generative_model = partial(self.generate_data, n_sims=1),\n",
        "    #         # configurator = self.configurator,\n",
        "    #         configurator = self.pre_amortizer,\n",
        "    #         amortizer = AmortizedPosterior(summary_net=summary_net, inference_net=inference_net),# summary_loss_fun=\"MMD\"),\n",
        "    #         checkpoint_path = self.model_path, max_to_keep = 3, #memory = True, memory is broken for now\n",
        "    #         # skip_checks = True,\n",
        "    #     )\n",
        "\n",
        "    # def plot_diagnostics(self, n_samples=100, refresh=False):\n",
        "    #     self.diagnostic = self.read_or_create(\n",
        "    #         file=self.file['diagnostic'], refresh=refresh,\n",
        "    #         fun=lambda: self.sample(n_samples=n_samples, n_sims=20*n_samples, ensemble=False))\n",
        "    #     self.plot('loss')\n",
        "    #     self.plot('ecdf', self.diagnostic)\n",
        "    #     self.plot('hist', self.diagnostic)\n",
        "    #     self.plot('recovery', self.diagnostic)\n",
        "\n",
        "\n",
        "    # def plot(self, kind='loss', samples=None):\n",
        "    #     if kind == 'loss':\n",
        "    #         fig = diag.plot_losses(**self.model.loss_history.get_plottable())\n",
        "    #     else:\n",
        "    #         opts = {'param_names':self.param_names, 'post_samples':samples['posterior_samples'], 'prior_samples':samples['prior_samples']}\n",
        "    #         if kind == 'ecdf':\n",
        "    #             fig = diag.plot_sbc_ecdf(**opts)\n",
        "    #         elif kind == 'hist':\n",
        "    #             fig = diag.plot_sbc_histograms(**opts)\n",
        "    #         elif kind == 'recovery':\n",
        "    #             fig = diag.plot_recovery(**opts)\n",
        "    #         else:\n",
        "    #             raise Exception(f'Unrecognized kind \"{kind}\"')\n",
        "    #     fig.savefig(self.path / f'{kind}.png')\n",
        "    #     plt.show()\n",
        "\n",
        "    # def calibrate(self):\n",
        "    #     c = self.generate_data(n_priors=self.n_calibrate, n_sims=1)\n",
        "    #     d = {'prior':c['prior'], 'data':pd.concat(c['data']).groupby('t')}\n",
        "    #     funs = ['mean','std','min','max']\n",
        "    #     c |= {f'{key}_{fun}': np.float32(val.agg(fun)) for fun in funs for key, val in d.items()}\n",
        "    #     c |= {f'obs_{fun}'  : c[f'data_{fun}'][...,-self.n_obs:] for fun in funs}\n",
        "    #     # c |= {f'prior_{fun}': c[f'prior_{fun}'].values for fun in funs}\n",
        "\n",
        "    #     # c |= {f'obs_{fun}_tf'  : self.to_tf(c[f'data_{fun}' ], 3, True ) for fun in funs}\n",
        "    #     # c |= {f'prior_{fun}_tf': self.to_tf(c[f'prior_{fun}'], 2, False) for fun in funs}\n",
        "    #     return c\n",
        "\n",
        "self = COVID(\n",
        "    name='radev_model_03',\n",
        "    refresh=True,\n",
        "    n_calibrate = 50,\n",
        ")\n",
        "# h = self.model.train_online(epochs=1000, iterations_per_epoch=32*1, batch_size=32, validation_sims=500)"
      ],
      "metadata": {
        "id": "5UnBF5ODLlx5"
      },
      "id": "5UnBF5ODLlx5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior_samples = self.prior_fun(100)\n",
        "\n",
        "prior_samples.iloc[0]\n",
        "self.sir(prior_samples.iloc[0])\n",
        "# self.sir(self.prior(1))"
      ],
      "metadata": {
        "id": "GBJshEhFQUIe"
      },
      "id": "GBJshEhFQUIe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam = self.generator(10)\n",
        "sam.keys()"
      ],
      "metadata": {
        "id": "zlHH2GwdPI98"
      },
      "id": "zlHH2GwdPI98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = self.model.generative_model(2)\n",
        "#  = partial(self.generate_data, n_sims=1),\n",
        "#             # configurator = self.configurator,\n",
        "#             configurator = self.pre_amortizer,"
      ],
      "metadata": {
        "id": "C5K_-th2MeWl"
      },
      "id": "C5K_-th2MeWl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = self.model.configurator(samples)"
      ],
      "metadata": {
        "id": "2bHVjJvTMusG"
      },
      "id": "2bHVjJvTMusG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# samples.keys()\n",
        "self.model.amortizer.sample(input_dict=samples, n_samples=3)"
      ],
      "metadata": {
        "id": "8IDtblG1MosU"
      },
      "id": "8IDtblG1MosU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{key: val.shape for key, val in samples.items()}"
      ],
      "metadata": {
        "id": "9yPidHkVMYHe"
      },
      "id": "9yPidHkVMYHe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = self.model.configurator(self.generate_data(n_priors=300, n_sims=1))\n",
        "{key: val.shape for key, val in samples.items()}"
      ],
      "metadata": {
        "id": "Vaks_YtUJZ__"
      },
      "id": "Vaks_YtUJZ__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples['posterior_samples'] = self.model.amortizer.sample(samples, n_samples=100)"
      ],
      "metadata": {
        "id": "GWkiO81KKVEx"
      },
      "id": "GWkiO81KKVEx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{key: val.shape for key, val in samples.items()}"
      ],
      "metadata": {
        "id": "1yoheVVcJjRN"
      },
      "id": "1yoheVVcJjRN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = self.generate_samples(n_priors=300, n_sims=2, n_samples=500)"
      ],
      "metadata": {
        "id": "oxHNjAVz965D"
      },
      "id": "oxHNjAVz965D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples['prior_samples'].shape, samples['posterior_samples'].shape"
      ],
      "metadata": {
        "id": "sV2m7FtQ_cI0"
      },
      "id": "sV2m7FtQ_cI0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self.plot('loss')\n",
        "self.plot('recovery', samples)"
      ],
      "metadata": {
        "id": "RB2sdCI_7e03"
      },
      "id": "RB2sdCI_7e03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opts = {'param_names':self.param_names, 'post_samples':samples['posterior_draws'], 'prior_samples':samples['prior_samples'].to_numpy()}\n",
        "fig = diag.plot_recovery(**opts)"
      ],
      "metadata": {
        "id": "c8gdI7qP0d_x"
      },
      "id": "c8gdI7qP0d_x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{key:type(val) for key, val in samples.items()}\n",
        "\n",
        "samples['posterior_draws'].shape, samples['prior'].shape"
      ],
      "metadata": {
        "id": "E2OLZbSaxmn7"
      },
      "id": "E2OLZbSaxmn7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = self.generate_sample(n_prior=2)"
      ],
      "metadata": {
        "id": "My3ENEZ7VPCa"
      },
      "id": "My3ENEZ7VPCa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ntW8ZFmr0AlY"
      },
      "id": "ntW8ZFmr0AlY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self.plot('loss')\n",
        "self.plot('recovery', samples)\n",
        "# # samples.keys()\n",
        "# opts = {'param_names':self.param_names, 'post_samples':samples['posterior_draws'], 'prior_samples':samples['prior_draws']}\n",
        "# fig = diag.plot_sbc_ecdf(**opts)"
      ],
      "metadata": {
        "id": "rEh4EIZSvJbF"
      },
      "id": "rEh4EIZSvJbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_prior = 2\n",
        "n_sims = 3\n",
        "prior = self.prior(n_prior).reset_index()\n",
        "prior = pd.concat([prior.assign(sim_idx=j).set_index(['prior_idx','sim_idx']) for j in range(n_sims)])\n",
        "data = [self.sir(p).reset_index().assign(prior_idx=i, sim_idx=j).set_index(['prior_idx','sim_idx','t']) for (i,j), p in prior.iterrows()]\n",
        "# # # data = [self.sir(p).assign(sim_idx=i, prior_idx=j).set_index(['prior_idx','sim_idx'], append=True) for (i,j), p in prior.iterrows()]\n",
        "# # # data = [self.sir(p) for j, p in prior.iterrows()]\n",
        "# len(data)\n",
        "# data[1]"
      ],
      "metadata": {
        "id": "fFB_SIJhkV_Q"
      },
      "id": "fFB_SIJhkV_Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.convert_to_tensor()\n",
        "# tf.convert_to_tensor(self.calibration['prior_mean'])\n",
        "c = self.generate_data(n_prior=50)\n",
        "c['prior'].agg('mean')"
      ],
      "metadata": {
        "id": "ME-PHJuOaIhe"
      },
      "id": "ME-PHJuOaIhe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['prior_draws'].shape"
      ],
      "metadata": {
        "id": "eCEvElXXV1xK"
      },
      "id": "eCEvElXXV1xK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.plot(kind='loss')"
      ],
      "metadata": {
        "id": "k1FWll9vVQvC"
      },
      "id": "k1FWll9vVQvC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# np.full((\n",
        "pd.DataFrame(np.full((1,len(self.param_names)), np.nan), columns=self.param_names)\n",
        "pd.DataFrame([np.full(len(self.param_names), np.nan)], columns=self.param_names)"
      ],
      "metadata": {
        "id": "pQdgs9itMYV0"
      },
      "id": "pQdgs9itMYV0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dct['parameters_out'].shape\n",
        "dct['obs'].shape\n",
        "# # dct['summary_conditions'].shape\n",
        "dct['parameters_out'].shape\n",
        "\n",
        "# dct['prior'].shape\n",
        "dct['posterior_draws'] = dct['parameters_out'] * self.calibration['prior_std'] + self.calibration['prior_mean']\n",
        "dct['posterior_draws']\n",
        "dct['prior']"
      ],
      "metadata": {
        "id": "A4k-Kq5TJwSX"
      },
      "id": "A4k-Kq5TJwSX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dct['prior_draws'].shape\n",
        "# dct['parameters'].shape\n",
        "# d=2\n",
        "dct = self.generate_data(n_prior_draws=d, n_sims=20*d)\n",
        "dct = self.model.configurator(dct)\n",
        "self.model.amortizer.sample(dct, 3)\n",
        "# self.model(dct, 3)"
      ],
      "metadata": {
        "id": "M-VefgOAGeIy"
      },
      "id": "M-VefgOAGeIy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.sample(dct=dct, n_samples=3)"
      ],
      "metadata": {
        "id": "KOaqs__tFnpK"
      },
      "id": "KOaqs__tFnpK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self.calibration.keys()\n",
        "# self.n_obs\n",
        "np.repeat(dct['prior_draws'], 2, axis=0).shape"
      ],
      "metadata": {
        "id": "ctKXKjDgyV7L"
      },
      "id": "ctKXKjDgyV7L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.generate_obs(n_sims=2, n_prior_draws=5)['obs'].shape\n",
        "# dct = self.get_real_data()\n",
        "# dct['parameters'] = (\n",
        "# dct['obs'].shape\n",
        "# self.n_obs\n",
        "# dct['prior_draws'] - pad(self.calibration['prior_mean'],2)\n",
        "# ) / self.calibration['prior_std'\n",
        "# dct = self.pre_amortizer(dct)\n",
        "# dct['parameters'].shape, dct['obs'].shape\n",
        "# P['data_df']\n",
        "# P['data']#.shape\n",
        "# P['prior_draws'].shape\n"
      ],
      "metadata": {
        "id": "t5GNM5-6pWDa"
      },
      "id": "t5GNM5-6pWDa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[1]*2"
      ],
      "metadata": {
        "id": "nLCbKPGS6aFD"
      },
      "id": "nLCbKPGS6aFD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = self.prior(3)\n",
        "w = [p for i,p in P.iterrows() for j in range(2)]\n",
        "w = [p for i,p in P.iterrows() for j in range(2)]\n",
        "len(w)"
      ],
      "metadata": {
        "id": "IrTF7uaamus0"
      },
      "id": "IrTF7uaamus0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = self.generator(n_sims=2, n_prior_draws=3)['prior_draws']\n",
        "len(P)\n",
        "def g(X):\n",
        "    # Y = pd.DataFrame()\n",
        "    # Y['a'] = X['alpha']# + X['beta']\n",
        "    # Y['b'] = X['alpha']# + X['beta']\n",
        "    # display(Y)\n",
        "    # return Y\n",
        "    return pd.Series({'a':[17,17],'n':[19,21]})#X.assign(new=17)\n",
        "# ?P[['new','newner']] =\n",
        "P.apply(g, axis=1)\n",
        "# P"
      ],
      "metadata": {
        "id": "97FGStNImMC8"
      },
      "id": "97FGStNImMC8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[k for k, p in self.enumerate(generator(n_sims=2, n_prior_draws=3)['prior_draws'])]"
      ],
      "metadata": {
        "id": "THIA2Em50AJx"
      },
      "id": "THIA2Em50AJx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = pad(self.prior(4),3)\n",
        "# a = [p for _ in range(3)]\n",
        "# a = [self.prior(4).values]*3\n",
        "# display(a[0])\n",
        "# display(a[1])\n",
        "np.repeat(p,3, axis=0).shape"
      ],
      "metadata": {
        "id": "llneAWqZgBQC"
      },
      "id": "llneAWqZgBQC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = self.model.configurator(self.model.generative_model(5))\n",
        "A['summary_conditions'].shape\n",
        "A['data'][0][1].shape\n",
        "\n",
        "# A = self.pre_amortizer(self.model_generative_model(n_prior_draws=3, n_sims=2))\n",
        "# A['summary_conditions'].shape, A['parameters'].shape"
      ],
      "metadata": {
        "id": "OslPZ3H3wwf6"
      },
      "id": "OslPZ3H3wwf6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['sim_std']"
      ],
      "metadata": {
        "id": "OswOP3MHvL0c"
      },
      "id": "OswOP3MHvL0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(self.generator(n_prior_draws=10)['data'][0])\n",
        "# display(self.generator(n_prior_draws=10)['data'][0][0])\n",
        "A = self.generator(n_sims=2, n_prior_draws=3)\n",
        "dfs = dict()\n",
        "dfs['prior_draws'] = A['prior_draws'][0]\n",
        "dfs['data'] = pd.concat([pd.concat([C.reset_index().assign(sim_idx=j, prior_idx=i).set_index(['sim_idx','prior_idx','t']) for i, C in enumerate(B)]) for j, B in enumerate(A['data'])])\n",
        "dfs['data']\n",
        "\n",
        "# D\n",
        "# display(pd.concat(D[0]))"
      ],
      "metadata": {
        "id": "VlmRDDhYrvx-"
      },
      "id": "VlmRDDhYrvx-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dct = self.generator(n_sims=2, n_prior_draws=1)\n",
        "# dct = self.get_real_data()\n",
        "dct = self.pre_amortizer(dct)\n",
        "dct['summary_conditions'].shape\n",
        "dct['parameters'].shape\n",
        "self.model.amortizer(dct, n_samples=7)"
      ],
      "metadata": {
        "id": "GgyLSxGelJI9"
      },
      "id": "GgyLSxGelJI9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_prior_draws=2\n",
        "n_sims = 3\n",
        "p = self.prior(n_prior_draws)\n",
        "prior_draws = [p for j in range(n_sims)]\n",
        "len(prior_draws[0])\n",
        "prior_draws[0].shape\n",
        "# [[self.sir(p) for i,p in prior_draws.iterrows()] for j in range(n_sims)]\n",
        "[[self.sir(p) for i,p in P.iterrows()] for P in prior_draws]"
      ],
      "metadata": {
        "id": "9Qlm2ZNXo6Jw"
      },
      "id": "9Qlm2ZNXo6Jw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_sims = 3\n",
        "[[self.sir(p) for i,p in prior_draws.iterrows()] for j in range(n_sims)]"
      ],
      "metadata": {
        "id": "woYGppDaoltJ"
      },
      "id": "woYGppDaoltJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.generator()['prior_draws'][0]"
      ],
      "metadata": {
        "id": "V8ZAa9dboQWK"
      },
      "id": "V8ZAa9dboQWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGhIhFAmn9by"
      },
      "id": "OGhIhFAmn9by",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dct = self.get_real_data()\n",
        "# pad(dct['data'], 4)[...,-self.n_obs:].shape\n",
        "# pad(self.calibration['sim_mean'], 4)[...,-self.n_obs:].shape\n",
        "# # self.n_obs"
      ],
      "metadata": {
        "id": "SNrNrtjbmFiT"
      },
      "id": "SNrNrtjbmFiT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self.prior_fun()\n",
        "# self.sir(self.prior_fun())\n",
        "# len(self.generator(n_prior_draws=3,n_sims=2)['data'])#[0][0]"
      ],
      "metadata": {
        "id": "GHVlBVi9kfI7"
      },
      "id": "GHVlBVi9kfI7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad(dct['data'], 3)[...,-self.n_obs]\n",
        "dct = self.generator(n_sims=2, n_prior_draws=3)\n",
        "# dct = dict(data =\n",
        "# self.real_data = self.load_data()\n",
        "# dict(prior_draws=np.full([1, self.n_params], np.nan), data=[self.real_data])#, obs_data=obs_data)\n",
        "\n",
        "n_prior_draws = 10\n",
        "prior_draws = self.prior(n_prior_draws)\n",
        "# data = [self.sir(p) for p in prior_draws]\n",
        "dct = dict(prior_draws=prior_draws, data=[self.sir(p) for i,p in prior_draws.iterrows()])\n",
        "\n",
        "x   = pad(dct['data'], 3)[...,-self.n_obs:]\n",
        "mu  = pad(self.calibration['sim_mean'], 3)[...,-self.n_obs:]\n",
        "std = pad(self.calibration['sim_std' ], 3)[...,-self.n_obs:]\n",
        "(x - mu)/ std"
      ],
      "metadata": {
        "id": "uMpfJ5bXhU1-"
      },
      "id": "uMpfJ5bXhU1-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.pre_amortizer(self.generator(n_sims=2, n_prior_draws=3))\n",
        "self.pre_amortizer(self.generator(n_sims=2, n_prior_draws=3))"
      ],
      "metadata": {
        "id": "JcNHH0Q9hEO_"
      },
      "id": "JcNHH0Q9hEO_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['sim_std'].shape"
      ],
      "metadata": {
        "id": "w3lpYQuThOre"
      },
      "id": "w3lpYQuThOre",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['prior_mean']"
      ],
      "metadata": {
        "id": "y_5r8lwwg0Gg"
      },
      "id": "y_5r8lwwg0Gg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = self.generator()['data'][0].columns.tolist()\n",
        "c\n",
        "# c.\n",
        "# c.filter(like='obs').shape[1]"
      ],
      "metadata": {
        "id": "tMyEbcRDfOgp"
      },
      "id": "tMyEbcRDfOgp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prior_draws = self.prior(3)\n",
        "[x for i, x in prior_draws.iterrows()]"
      ],
      "metadata": {
        "id": "AiKQyWyldm-y"
      },
      "id": "AiKQyWyldm-y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self.prior(5)\n",
        "n_prior_draws=3\n",
        "n_sims = 2\n",
        "# self.prior_draws = [[param.assign(prior_idx=i, sim_idx=j).set_index(['prior_idx','sim_idx']) for j in range(n_sims)] for i, param in self.prior(n_prior_draws).iterrows()]\n",
        "# self.prior_draws = pd.concat([[param for j in range(n_sims)] for i, param in self.prior(n_prior_draws).iterrows()])\n",
        "\n",
        "# [prior_idx, p in self.prior(n_prior_draws).to_dict('index').items()\n",
        "# a = [[(sim_idx, prior_idx, p) for prior_idx, p in self.prior(n_prior_draws).iterrows()] for sim_idx in range(n_sims)]\n",
        "a = [[(sim_idx, prior_idx, p) for sim_idx in range(n_sims)] for prior_idx, p in self.prior(n_prior_draws).iterrows()]\n",
        "a = [[p.to_frame().assign(sim_idx=sim_idx, prior_idx=prior_idx) for sim_idx in range(n_sims)] for prior_idx, p in self.prior(n_prior_draws).iterrows()]\n",
        "a = [[p for sim_idx in range(n_sims)] for prior_idx, p in self.prior(n_prior_draws).iterrows()]\n",
        "a[0]\n",
        "# pd.concat(a[0])\n",
        "\n",
        "# prior_draws = [p.assign(sim_idx=sim_idx).set_index(['sim_idx','prior_idx']) for sim_idx in range(n_sims)]\n",
        "# # prior = pd.concat(prior_draws)\n",
        "# # prior_draws = np.array(prior_draws)\n",
        "\n",
        "# a = [[p for prior_idx, p in enumerate(sim)] for sim_idx, sim in enumerate(prior_draws)]\n",
        "# a[0]\n",
        "# data = [[self.sir(p).assign(sim_idx=sim_idx).set_index(['sim_idx','prior_idx']) for prior_idx, p in enumerate(sim)] for sim_idx, sim in enumerate(prior_draws)]\n",
        "# ensemble = pd.concat([pd.concat(x) for x in data])\n",
        "# data = np.array(data)\n",
        "# prior_draws.shape, data.shape\n",
        "# ensemble\n",
        "\n",
        "# [self.sir(p).reset_index().assign(sim_idx=i,prior_idx=j).set_index(['sim_idx','prior_idx','t']) for (i,j),p in parameters.iterrows()]\n",
        "\n",
        "# prior_draws = pd.concat([params.assign(sim_idx=i).set_index('sim_idx',append=True) for i in range(n_sims)])\n",
        "# prior_draws = [params.assign(sim_idx=i).set_index('sim_idx',append=True) for i in range(n_sims)]\n",
        "# np.array(prior_draws).shape\n",
        "# self.prior_draws = [[param.assign( for j in range(n_sims)] for i, param in self.prior(n_prior_draws)]\n",
        "\n",
        "# len(self.prior_draws[0])\n",
        "# self.prior_draws"
      ],
      "metadata": {
        "id": "1jX2_cNtvHO-"
      },
      "id": "1jX2_cNtvHO-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# v = self.model.generative_model(5)\n",
        "v = self.generator(n_sims=2, n_prior_draws=3)\n",
        "# c = self.configurator(v)\n",
        "# n_posterior_draws=7\n",
        "# s = self.model.amortizer.sample(c, n_samples=n_posterior_draws)\n",
        "# # c.keys()\n",
        "# s.shape\n",
        "v['ensemble']\n",
        "# v['parameters']"
      ],
      "metadata": {
        "id": "PuH6CasG-fx_"
      },
      "id": "PuH6CasG-fx_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([pd.DataFrame([params], columns=self.param_names).assign( for param_idx, sim in enumerate(s) for j, draw in enumerate(sim)])"
      ],
      "metadata": {
        "id": "plP0TaP0JSzs"
      },
      "id": "plP0TaP0JSzs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.model.loss_history."
      ],
      "metadata": {
        "id": "0yXFI6Xg7Y-Q"
      },
      "id": "0yXFI6Xg7Y-Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.model.train_online??"
      ],
      "metadata": {
        "id": "h7uzrS4C9Fx6"
      },
      "id": "h7uzrS4C9Fx6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['prior_max']"
      ],
      "metadata": {
        "id": "D537rnVZ51Q4"
      },
      "id": "D537rnVZ51Q4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.model.loss_history.get_plottable()"
      ],
      "metadata": {
        "id": "vaBoZ1Y_5eKL"
      },
      "id": "vaBoZ1Y_5eKL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fig = diag.plot_losses(train_losses=self.model.loss_history.get_plottable()['train_losses'])\n",
        "fig = diag.plot_losses(**self.model.loss_history.get_plottable())"
      ],
      "metadata": {
        "id": "Y7SXguYN5aTn"
      },
      "id": "Y7SXguYN5aTn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = self.configurator(self.generator(n_prior_draws=5))\n",
        "c.keys()"
      ],
      "metadata": {
        "id": "O_mgyynH2sik"
      },
      "id": "O_mgyynH2sik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def g(x):\n",
        "    if x%2==0:\n",
        "        return x\n",
        "\n",
        "[g(x) for x in range(10)]"
      ],
      "metadata": {
        "id": "wsvw3bqkzVEB"
      },
      "id": "wsvw3bqkzVEB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forward_dict = {'data':self.real_data}\n",
        "c = self.configurator(forward_dict)\n",
        "print(c.keys(), c['parameters'].shape, c['summary_conditions'].shape)\n",
        "forward_dict = self.generator(n_sims=1, n_prior_draws=1)\n",
        "c = self.configurator(forward_dict)\n",
        "print(c.keys(), c['parameters'].shape, c['summary_conditions'].shape)\n"
      ],
      "metadata": {
        "id": "f-e-NCgmweYh"
      },
      "id": "f-e-NCgmweYh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.real_data#.describe()\n",
        "# R"
      ],
      "metadata": {
        "id": "sOcOIY4zrEC-"
      },
      "id": "sOcOIY4zrEC-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['sim_mean']"
      ],
      "metadata": {
        "id": "baZMhg3zvy_T"
      },
      "id": "baZMhg3zvy_T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = self.generator(n_sims=2, n_prior_draws=3)\n",
        "g['data'].shape\n",
        "g['ensemble']\n",
        "g['parameters']\n",
        "# E = pd.concat(D)\n",
        "# E.groupby(['sim_idx','param_idx'])\n",
        "# D[0]\n",
        "# len(D)\n",
        "# pd.concat()"
      ],
      "metadata": {
        "id": "x3TeD5OFkfHh"
      },
      "id": "x3TeD5OFkfHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.model"
      ],
      "metadata": {
        "id": "3ZIxPA6tkfn1"
      },
      "id": "3ZIxPA6tkfn1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.model.loss_history.get_plottable()"
      ],
      "metadata": {
        "id": "ED6-J64LkQas"
      },
      "id": "ED6-J64LkQas",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = lambda n: self.generator(n_prior_draws=n)\n",
        "w = self.configurator(g(10))\n",
        "w['summary_conditions'].shape, w['parameters'].shape"
      ],
      "metadata": {
        "id": "TbFUH6pJPQWj"
      },
      "id": "TbFUH6pJPQWj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sel"
      ],
      "metadata": {
        "id": "KNu-_CIQQf3Y"
      },
      "id": "KNu-_CIQQf3Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def to_df(pd.concat([fun(val).assign(sim=i, sample=j).set_index(['sim', 'sample'], append=append) for i, sim in enumerate(arr) for j, val in enumerate(sim)])\n",
        "\n",
        "c = self.generator(n_prior_draws=3, n_sims=2)\n",
        "\n",
        "c['parameters']\n",
        "c['data'][1]\n",
        "# self.configurator(c)\n",
        "# d = (pad(c['data'],4)[...,-self.n_obs:] - pad(self.calibration['sim_mean'],3)[...,-self.n_obs:]) / pad(self.calibration['sim_std'],3)[...,-self.n_obs:]\n",
        "# (pad(c['parameters'],3) - pad(self.calibration['prior_mean'],1)) / pad(self.calibration['prior_std'],1)\n",
        "# self.calibration['prior_mean'].shape\n",
        "# d.shape\n",
        "# (pad(forward_dict['data'],4)[...,-self.n_obs] - self.calibration['sim_mean'].iloc[:,-self.n_obs]) / self.calibration['sim_std'].iloc[:,-self.n_obs]"
      ],
      "metadata": {
        "id": "GBwf8F7DnWRQ"
      },
      "id": "GBwf8F7DnWRQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = self.prior(5)\n",
        "A = pd.concat([w]*2)\n",
        "for prior_idx, p in A.iterrows():\n",
        "    print(prior_idx)"
      ],
      "metadata": {
        "id": "lhvW9pwz07uA"
      },
      "id": "lhvW9pwz07uA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = self.calibration['sim_mean'].columns.str.contains('obs')\n",
        "S[...,m].shape"
      ],
      "metadata": {
        "id": "1JCxwtHEwYPM"
      },
      "id": "1JCxwtHEwYPM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['sim_mean']"
      ],
      "metadata": {
        "id": "JWzGQrvhmYuh"
      },
      "id": "JWzGQrvhmYuh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# self.calibration['sim_mean']\n",
        "params = self.prior(2)\n",
        "# p\n",
        "c = self.generator(params=params, n_sims=2)\n",
        "# c = self.generator(n_prior_draws=10, n_sims=2)\n",
        "# # c['sim_data'][0]\n",
        "c['sim_data'][0][0].filter(like='obs')\n",
        "# [pd.concat(x).assign(prior_idx=i) for i, x in enumerate(c['sim_data'])][0]\n",
        "\n",
        "#  .to_dict('records'))]\n",
        "# [x for i, x in c['prior_draws'].to_dict('index').items()]\n",
        "# [pd.concat(x).assign(prior_idx=i) for i, x in c['sim_data'].to_dict('index').items()]\n",
        "# pd.concat([pd.concat(x) for x in c['sim_data'].to_dict('index')])\n",
        "# S = pd.concat([[pd.concat(y) for y in x] for x in c['sim_data']])\n",
        "# S = pd.concat([[pd.concat(y) for y in x] for x in c['sim_data']])"
      ],
      "metadata": {
        "id": "x1qQHHHNkFTL"
      },
      "id": "x1qQHHHNkFTL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calib = self.generator(n_prior_draws=10)\n",
        "P = calib['prior_draws']\n",
        "S = calib['sim_data']\n",
        "calib['prior_mean'] = np.mean(P, axis=0)\n",
        "calib['prior_std'] = np.std(P, axis=0)\n",
        "calib['prior_min'] = np.min(P, axis=0)\n",
        "calib['prior_max'] = np.max(P, axis=0)\n",
        "calib['sim_mean'] = np.mean(S, axis=(0,1))\n",
        "calib['sim_std'] = np.std(S, axis=(0,1))\n",
        "calib['sim_min'] = np.min(S, axis=(0,1))\n",
        "calib['sim_max'] = np.max(S, axis=(0,1))\n",
        "\n",
        "\n",
        "# d = dict(posterior=g['prior_draws'], sim=g['sim_data'])\n",
        "# {key:{stat:df\n",
        "# P =\n",
        "\n",
        "# # len(g['sim_data'])\n",
        "# # np.array(g['sim_data']).mean(axis=(0,1)).shape\n",
        "# S = g['sim_data']\n",
        "# cols = S[0][0].columns\n",
        "# pd.DataFrame(np.mean(S, axis=(0,1)), columns=cols)\n",
        "\n"
      ],
      "metadata": {
        "id": "X_Mb4sZYaAm8"
      },
      "id": "X_Mb4sZYaAm8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draws = self.prior(100)\n",
        "self.prior(100)['prior_draws'].shape, self.prior_fun(100).shape\n",
        "# self.simulator(self.prior(10))"
      ],
      "metadata": {
        "id": "eelnm46AXEcg"
      },
      "id": "eelnm46AXEcg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['const']"
      ],
      "metadata": {
        "id": "9K0pw1R7T4va"
      },
      "id": "9K0pw1R7T4va",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame([self.prior_fun() for _ in range(100)])"
      ],
      "metadata": {
        "id": "c2FuxJUFSMFT"
      },
      "id": "c2FuxJUFSMFT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autotime\n",
        "import os, datetime, pathlib, shutil, google.colab, dataclasses, pickle, wbgapi\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "import numpy as np, pandas as pd, tensorflow as tf\n",
        "from functools import partial\n",
        "from scipy import stats\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, LSTM\n",
        "from bayesflow.networks import InvertibleNetwork\n",
        "from bayesflow.coupling_networks import CouplingLayer\n",
        "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
        "from bayesflow.amortizers import AmortizedLikelihood, AmortizedPosterior, AmortizedPosteriorLikelihood\n",
        "from bayesflow.trainers import Trainer\n",
        "from bayesflow import default_settings\n",
        "from bayesflow.helper_functions import build_meta_dict\n",
        "from bayesflow.diagnostics import plot_sbc_ecdf, plot_sbc_histograms, plot_losses\n",
        "from bayesflow.computational_utilities import maximum_mean_discrepancy\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "plt.rcParams.update({\"text.usetex\": False, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{{amsmath}}\"})\n",
        "mnt = '/content/drive'\n",
        "google.colab.drive.mount(mnt)\n",
        "RNG = np.random.default_rng(42)\n",
        "SCALE = 1000\n",
        "EPS = 1e-6\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class COVID():\n",
        "    country: str = 'Germany'\n",
        "    name: str = 'covid_000'\n",
        "    n_steps: int = 100\n",
        "    refresh: bool = False\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.root_path = pathlib.Path(mnt + f'/MyDrive/bayesian disease modeling/{self.name}')\n",
        "        if self.refresh:\n",
        "            # delete root_path and everything in it\n",
        "            shutil.rmtree(self.root_path, ignore_errors=True)\n",
        "        self.model_path = self.root_path / f'model/'\n",
        "        self.model_path.mkdir(exist_ok=True, parents=True)\n",
        "        self.file = {key: self.model_path / f'{key}.pkl' for key in ['calibration','diagnostic','predictive','error']}\n",
        "\n",
        "        self.iso = wbgapi.economy.coder(self.country)\n",
        "        assert self.iso, f'Unrecognized country {self.country}'\n",
        "        self.tot_pop = next(wbgapi.data.fetch('SP.POP.TOTL', economy=self.iso, time=2020))['value']\n",
        "\n",
        "        self.calibration = self.read_or_create(self.file['calibration'], fun=self.calibrate, refresh=False)\n",
        "        self.param_names = [key for key in self.calibration['draws'].columns if key not in self.calibration['const']]\n",
        "        self.param_latex = [f'${key}$' if key in ['E_0','lag_I','lag_R','lag_D'] else f'$\\{key}$' for key in self.param_names]\n",
        "        self.prior = lambda batch_size: self.prior_fun(batch_size)[self.param_names]\n",
        "\n",
        "    def calibrate(self):\n",
        "        draws = self.prior_fun(10000)\n",
        "        stats = draws.agg(['mean','std']).T\n",
        "        mask = stats['std'] > EPS\n",
        "        prior_means, prior_stds = stats[mask].T.values\n",
        "        const = stats.loc[~mask,'mean'].to_dict()\n",
        "        return dict(draws=draws, stats=stats, prior_means=prior_means, prior_stds=prior_means, const=const)\n",
        "\n",
        "    def read_or_create(self, file, fun=None, refresh=False):\n",
        "        \"\"\"Read or create pickle for simulation results\"\"\"\n",
        "        try:\n",
        "            assert not refresh\n",
        "            with open(file, \"rb\") as f:\n",
        "                sims = pickle.load(f)\n",
        "            print(f'{file} successfully read')\n",
        "        except Exception as e:\n",
        "            print(f'Running sims due to error: {e}')\n",
        "            with open(file, \"wb\") as f:\n",
        "                sims = fun()\n",
        "                pickle.dump(sims, f)\n",
        "        return sims\n",
        "\n",
        "    def check_params(self, p):\n",
        "        for key in ['E_0','t_1','t_2','t_3','t_4','t_5','delta_1','delta_2','delta_3','delta_4','lag_I','lag_R','lag_D']:\n",
        "            p[key] = int(round(p[key]))\n",
        "        p['E_0'] = max(p['E_0'], 1)\n",
        "        valid = all(val > 0 for key, val in p.items() if key[:3] != 'phi')\n",
        "        for i in range(1,4):\n",
        "            valid &= p[f't_{i}'] + p[f'delta_{i}'] <= p[f't_{i+1}']\n",
        "        return p if valid else valid\n",
        "\n",
        "    def prior_fun(self, batch_size=1):\n",
        "        alpha_f = (0.7**2) * ((1 - 0.7) / (0.17**2) - (1 - 0.7))\n",
        "        beta_f = alpha_f * (1 / 0.7 - 1)\n",
        "        L = []\n",
        "        while len(L) < batch_size:\n",
        "            p = self.check_params({\n",
        "                'N'       :self.tot_pop,\n",
        "                'E_0'     :RNG.gamma(shape=2, scale=30),\n",
        "                'alpha'   :RNG.uniform(low=0.005, high=0.99),\n",
        "                'beta'    :RNG.lognormal(mean=np.log(0.25), sigma=0.3),\n",
        "                'gamma'   :RNG.lognormal(mean=np.log(1/6.5), sigma=0.5),\n",
        "                'delta'   :RNG.uniform(low=0.01, high=0.3),\n",
        "                'epsilon' :RNG.uniform(low=1/14, high=1/3),\n",
        "                'eta'     :RNG.lognormal(mean=np.log(1/3.2), sigma=0.5),\n",
        "                'lambda'  :RNG.lognormal(mean=np.log(1.2), sigma=0.5),\n",
        "                'mu'      :RNG.lognormal(mean=np.log(1/8), sigma=0.2),\n",
        "                'theta'   :RNG.uniform(low=1/14, high=1/3),\n",
        "                't_1'     :RNG.normal(loc=8, scale=3),\n",
        "                't_2'     :RNG.normal(loc=15, scale=3),\n",
        "                't_3'     :RNG.normal(loc=22, scale=3),\n",
        "                't_4'     :RNG.normal(loc=66, scale=3),\n",
        "                't_5'     :self.n_steps,\n",
        "                'delta_1' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_2' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_3' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_4' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'lambda_0':RNG.lognormal(mean=np.log(1.20), sigma=0.5),\n",
        "                'lambda_1':RNG.lognormal(mean=np.log(0.60), sigma=0.5),\n",
        "                'lambda_2':RNG.lognormal(mean=np.log(0.30), sigma=0.5),\n",
        "                'lambda_3':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "                # 'lambda_4':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "                'lambda_4':RNG.lognormal(mean=np.log(0.15), sigma=0.5),\n",
        "                'f_I'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'f_R'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'f_D'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'phi_I'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'phi_R'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'phi_D'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'lag_I'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'lag_R'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'lag_D'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'sigma_I' :RNG.gamma(shape=1, scale=5),\n",
        "                'sigma_R' :RNG.gamma(shape=1, scale=5),\n",
        "                'sigma_D' :RNG.gamma(shape=1, scale=5),\n",
        "            })\n",
        "            if p:\n",
        "                L.append(p)\n",
        "        return pd.DataFrame(L)\n",
        "        # return L)\n",
        "\n",
        "    def calc_lambda_array(self, p):\n",
        "        \"\"\"Computes the array of time-varying contact rates/transimission probabilities.\"\"\"\n",
        "        assert self.check_params(p)\n",
        "        # Array of initial lambdas\n",
        "        lambd0_arr = np.array([p['lambda_0']] * (p['t_1'] + p['sim_lag']))\n",
        "\n",
        "        # Compute lambd1 array\n",
        "        if p['delta_1'] == 1:\n",
        "            lambd1_arr = np.array([p['lambda_1']] * (p['t_2'] - p['t_1']))\n",
        "        else:\n",
        "            lambd1_arr = np.linspace(p['lambda_0'], p['lambda_1'], p['delta_1'])\n",
        "            lambd1_arr = np.append(lambd1_arr, [p['lambda_1']] * (p['t_2'] - p['t_1'] - p['delta_1']))\n",
        "\n",
        "        # Compute lambd2 array\n",
        "        if p['delta_2'] == 1:\n",
        "            lambd2_arr = np.array([p['lambda_2']] * (p['t_3'] - p['t_2']))\n",
        "        else:\n",
        "            lambd2_arr = np.linspace(p['lambda_1'], p['lambda_2'], p['delta_2'])\n",
        "            lambd2_arr = np.append(lambd2_arr, [p['lambda_2']] * (p['t_3'] - p['t_2'] - p['delta_2']))\n",
        "\n",
        "        # Compute lambd3 array\n",
        "        if p['delta_3'] == 1:\n",
        "            lambd3_arr = np.array([p['lambda_3']] * (p['t_4'] - p['t_3']))\n",
        "        else:\n",
        "            lambd3_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_3'])\n",
        "            lambd3_arr = np.append(lambd3_arr, [p['lambda_3']] * (p['t_4'] - p['t_3'] - p['delta_3']))\n",
        "\n",
        "        # Compute lambd4 array\n",
        "        if p['delta_4'] == 1:\n",
        "            lambd4_arr = np.array([p['lambda_4']] * (p['t_5'] - p['t_4']))\n",
        "        else:\n",
        "            lambd4_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_4'])\n",
        "            lambd4_arr = np.append(lambd4_arr, [p['lambda_4']] * (p['t_5'] - p['t_4'] - p['delta_4']))\n",
        "\n",
        "        return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
        "\n",
        "\n",
        "    def sir(self, prior_draw, sim_diff=16, observation_model=True):\n",
        "        for k,v in self.calibration['const'].items():\n",
        "            prior_draw[k] = v\n",
        "\n",
        "        # assert self.check_params(p)\n",
        "        assert sim_diff > max(p['lag_I'],p['lag_R'],p['lag_D'])\n",
        "        p['sim_lag'] = sim_diff - 1\n",
        "        lambd_arr = self.calc_lambda_array(p)\n",
        "\n",
        "        # Initial conditions\n",
        "        S, E, C, I, R, D = [self.tot_pop - p['E_0']], [p['E_0']], [0], [0], [0], [0]\n",
        "\n",
        "        # Containers\n",
        "        I_news = []\n",
        "        R_news = []\n",
        "        D_news = []\n",
        "\n",
        "        # Reported new cases\n",
        "        I_data = np.zeros(p['t_5'])\n",
        "        R_data = np.zeros(p['t_5'])\n",
        "        D_data = np.zeros(p['t_5'])\n",
        "        fs_I = np.zeros(p['t_5'])\n",
        "        fs_R = np.zeros(p['t_5'])\n",
        "        fs_D = np.zeros(p['t_5'])\n",
        "\n",
        "        # Simulate T-1 tiemsteps\n",
        "        for t in range(p['t_5'] + p['sim_lag']):\n",
        "            # Calculate new exposed cases\n",
        "            E_new = lambd_arr[t] * ((C[t] + p['beta'] * I[t]) / self.tot_pop) * S[t]\n",
        "\n",
        "            # Remove exposed from susceptible\n",
        "            S_t = S[t] - E_new\n",
        "\n",
        "            # Calculate current exposed by adding new exposed and\n",
        "            # subtracting the exposed becoming carriers.\n",
        "            E_t = E[t] + E_new - p['gamma'] * E[t]\n",
        "\n",
        "            # Calculate current carriers by adding the new exposed and subtracting\n",
        "            # those who will develop symptoms and become detected and those who\n",
        "            # will go through the disease asymptomatically.\n",
        "            C_t = C[t] + p['gamma'] * E[t] - (1 - p['alpha']) * p['eta'] * C[t] - p['alpha'] * p['theta'] * C[t]\n",
        "\n",
        "            # Calculate current infected by adding the symptomatic carriers and\n",
        "            # subtracting the dead and recovered. The newly infected are just the\n",
        "            # carriers who get detected.\n",
        "            I_t = I[t] + (1 - p['alpha']) * p['eta'] * C[t] - (1 - p['delta']) * p['mu'] * I[t] - p['delta'] * p['epsilon'] * I[t]\n",
        "            I_new = (1 - p['alpha']) * p['eta'] * C[t]\n",
        "\n",
        "            # Calculate current recovered by adding the symptomatic and asymptomatic\n",
        "            # recovered. The newly recovered are only the detected recovered\n",
        "            R_t = R[t] + p['alpha'] * p['theta'] * C[t] + (1 - p['delta']) * p['mu'] * I[t]\n",
        "            R_new = (1 - p['delta']) * p['mu'] * I[t]\n",
        "\n",
        "            # Calculate the current dead\n",
        "            D_t = D[t] + p['delta'] * p['epsilon'] * I[t]\n",
        "            D_new = p['delta'] * p['epsilon'] * I[t]\n",
        "\n",
        "            # Ensure some numerical onstraints\n",
        "            S_t = np.clip(S_t, 0, self.tot_pop)\n",
        "            E_t = np.clip(E_t, 0, self.tot_pop)\n",
        "            C_t = np.clip(C_t, 0, self.tot_pop)\n",
        "            I_t = np.clip(I_t, 0, self.tot_pop)\n",
        "            R_t = np.clip(R_t, 0, self.tot_pop)\n",
        "            D_t = np.clip(D_t, 0, self.tot_pop)\n",
        "\n",
        "            # Keep track of process over time\n",
        "            S.append(S_t)\n",
        "            E.append(E_t)\n",
        "            C.append(C_t)\n",
        "            I.append(I_t)\n",
        "            R.append(R_t)\n",
        "            D.append(D_t)\n",
        "            I_news.append(I_new)\n",
        "            R_news.append(R_new)\n",
        "            D_news.append(D_new)\n",
        "\n",
        "            # From here, start adding new cases with delay D\n",
        "            # Note, we assume the same delay\n",
        "            if t >= p['sim_lag']:\n",
        "                # Compute lags and add to data arrays\n",
        "                fs_I[t - p['sim_lag']] = (1 - p['f_I']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - p['sim_lag']) - 0.5 * p['phi_I']))\n",
        "                )\n",
        "                fs_R[t - p['sim_lag']] = (1 - p['f_R']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - p['sim_lag']) - 0.5 * p['phi_R']))\n",
        "                )\n",
        "                fs_D[t - p['sim_lag']] = (1 - p['f_D']) * (\n",
        "                    1 - np.abs(np.sin((np.pi / 7) * (t - p['sim_lag']) - 0.5 * p['phi_D']))\n",
        "                )\n",
        "                I_data[t - p['sim_lag']] = I_news[t - p['lag_I']]\n",
        "                R_data[t - p['sim_lag']] = R_news[t - p['lag_R']]\n",
        "                D_data[t - p['sim_lag']] = D_news[t - p['lag_D']]\n",
        "\n",
        "        # Compute weekly modulation\n",
        "        I_data = (1 - fs_I) * I_data\n",
        "        R_data = (1 - fs_R) * R_data\n",
        "        D_data = (1 - fs_D) * D_data\n",
        "\n",
        "        # Add noise\n",
        "        # I_data = stats.t(df=4, loc=I_data, scale=np.sqrt(I_data) * scale_I).rvs()\n",
        "        # R_data = stats.t(df=4, loc=R_data, scale=np.sqrt(R_data) * scale_R).rvs()\n",
        "        # D_data = stats.t(df=4, loc=D_data, scale=np.sqrt(D_data) * scale_D).rvs()\n",
        "        I_data = I_data + RNG.standard_t(4) * np.sqrt(I_data) * p['sigma_I']\n",
        "        R_data = R_data + RNG.standard_t(4) * np.sqrt(R_data) * p['sigma_R']\n",
        "        D_data = D_data + RNG.standard_t(4) * np.sqrt(D_data) * p['sigma_D']\n",
        "        n = I_data.shape[0]\n",
        "        Y = pd.DataFrame({'S':S[-n:],'E':E[-n:],'C':C[-n:],'I':I[-n:],'R':R[-n:],'D':D[-n:],'dI_obs':I_data,'dR_obs':R_data,'dD_obs':D_data}).clip(0, self.tot_pop) / SCALE\n",
        "        if observation_model:\n",
        "            Y = Y.filter(like='obs')\n",
        "        return Y\n",
        "\n",
        "\n",
        "    def generator(self, n_sims=1, n_priors=1, prior_draws=None):\n",
        "\n",
        "        prior_draws = self.prior(batch_size)\n",
        "        # self.sir(prior_draws)\n",
        "        return prior_draws\n",
        "\n",
        "        # return [v for k,v in param.items()]\n",
        "    #     return params\n",
        "\n",
        "\n",
        "self = COVID(\n",
        "    # refresh=True,\n",
        ")\n",
        "prior_draws = self.prior(3)\n",
        "for k,v in self.calibration['const'].items():\n",
        "    prior_draws[k] = v\n",
        "\n",
        "prior_draws.to_dict('records')\n",
        "x = [self.sir(p, observation_model=False) for p in prior_draws.to_dict('records')]\n",
        "# x = [self.sir(p, observation_model=False) for p in prior_draws.to_dict('index').values()]\n",
        "# x = [p for i,p in prior_draws.to_dict('index')]\n",
        "x[0][0]\n",
        "\n",
        "# prior_draws.to_dict('index')\n",
        "# self.check_params(prior_draws.iloc[0])\n",
        "# self.sir(self.generator(5).iloc[0])\n",
        "# self.prior(5)\n",
        "# self.calibration['const']\n",
        "# for params in self.prior(5).to\n",
        "# calibration_draws = pd.DataFrame([self.prior_fun() for k in range(100)])"
      ],
      "metadata": {
        "id": "5aQI2uIdCR2D"
      },
      "id": "5aQI2uIdCR2D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.calibration['const']"
      ],
      "metadata": {
        "id": "Wa1TFsEB51C6"
      },
      "id": "Wa1TFsEB51C6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = self.prior(5)\n",
        "# [v for k,v in df.to_dict(orient='index').items()]\n",
        "[x for i,x in df.iterrows()]\n",
        "# f = lambda x: [x.values, x.values]\n",
        "# df.apply(f, axis=1)"
      ],
      "metadata": {
        "id": "XxaRLCSd4gx_"
      },
      "id": "XxaRLCSd4gx_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.const_params"
      ],
      "metadata": {
        "id": "fk7gy220vlqm"
      },
      "id": "fk7gy220vlqm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prior_fun():\n",
        "    alpha_f = (0.7**2) * ((1 - 0.7) / (0.17**2) - (1 - 0.7))\n",
        "    beta_f = alpha_f * (1 / 0.7 - 1)\n",
        "    while True:\n",
        "        p = self.check_params({\n",
        "            'N'       :self.tot_pop,\n",
        "            'E_0'     :RNG.gamma(shape=2, scale=30),\n",
        "            'alpha'   :RNG.uniform(low=0.005, high=0.99),\n",
        "            'beta'    :RNG.lognormal(mean=np.log(0.25), sigma=0.3),\n",
        "            'gamma'   :RNG.lognormal(mean=np.log(1/6.5), sigma=0.5),\n",
        "            'delta'   :RNG.uniform(low=0.01, high=0.3),\n",
        "            'epsilon' :RNG.uniform(low=1/14, high=1/3),\n",
        "            'eta'     :RNG.lognormal(mean=np.log(1/3.2), sigma=0.5),\n",
        "            'lambda'  :RNG.lognormal(mean=np.log(1.2), sigma=0.5),\n",
        "            'mu'      :RNG.lognormal(mean=np.log(1/8), sigma=0.2),\n",
        "            'theta'   :RNG.uniform(low=1/14, high=1/3),\n",
        "            't_1'     :RNG.normal(loc=8, scale=3),\n",
        "            't_2'     :RNG.normal(loc=15, scale=3),\n",
        "            't_3'     :RNG.normal(loc=22, scale=3),\n",
        "            't_4'     :RNG.normal(loc=66, scale=3),\n",
        "            't_5'     :self.n_steps,\n",
        "            'delta_1' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "            'delta_2' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "            'delta_3' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "            'delta_4' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "            'lambda_0':RNG.lognormal(mean=np.log(1.20), sigma=0.5),\n",
        "            'lambda_1':RNG.lognormal(mean=np.log(0.60), sigma=0.5),\n",
        "            'lambda_2':RNG.lognormal(mean=np.log(0.30), sigma=0.5),\n",
        "            'lambda_3':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "            # 'lambda_4':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "            'lambda_4':RNG.lognormal(mean=np.log(0.15), sigma=0.5),\n",
        "            'f_I'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "            'f_R'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "            'f_D'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "            'phi_I'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "            'phi_R'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "            'phi_D'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "            'lag_I'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "            'lag_R'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "            'lag_D'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "            'sigma_I' :RNG.gamma(shape=1, scale=5),\n",
        "            'sigma_R' :RNG.gamma(shape=1, scale=5),\n",
        "            'sigma_D' :RNG.gamma(shape=1, scale=5),\n",
        "        })\n",
        "        if p:\n",
        "            return p"
      ],
      "metadata": {
        "id": "ZGDr8dyEu2wf"
      },
      "id": "ZGDr8dyEu2wf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self.const_params"
      ],
      "metadata": {
        "id": "SWsV6BGetNC3"
      },
      "id": "SWsV6BGetNC3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPS = 1e-4\n",
        "stat = calibration_draws.agg(['mean','std']).T\n",
        "mask = stat['std'] > EPS\n",
        "c = stat[~mask].index.tolist()\n",
        "stat[mask].T.to_dict(orient='index')\n",
        "mean, stds = stat[mask].T.values\n",
        "mean\n",
        "\n",
        "# mean = stat.loc[mask,'mean'].to_dict()\n",
        "# mean\n",
        "# mean = stat"
      ],
      "metadata": {
        "id": "PaWoJiwepQ3E"
      },
      "id": "PaWoJiwepQ3E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ode_params = ['alpha','beta','gamma','delta','epsilon','eta','lambda','mu','theta']\n",
        "# self.lower_bound = np.array([EPS for key in self.param_names])\n",
        "# self.upper_bound = np.array([1-EPS if key in ['alpha','delta','psi_I','psi_R','psi_D'] else np.inf for key in self.param_names])\n",
        "# self.classes = self.sir(prior_draws[0]).columns.tolist()"
      ],
      "metadata": {
        "id": "GHp4hno-GVhX"
      },
      "id": "GHp4hno-GVhX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_lambda_array(p):\n",
        "    \"\"\"Computes the array of time-varying contact rates/transimission probabilities.\"\"\"\n",
        "    assert check_params(p)\n",
        "    # Array of initial lambdas\n",
        "    lambd0_arr = np.array([p['lambda_0']] * (p['t_1'] + p['sim_lag']))\n",
        "\n",
        "    # Compute lambd1 array\n",
        "    if p['delta_1'] == 1:\n",
        "        lambd1_arr = np.array([p['lambda_1']] * (p['t_2'] - p['t_1']))\n",
        "    else:\n",
        "        lambd1_arr = np.linspace(p['lambda_0'], p['lambda_1'], p['delta_1'])\n",
        "        lambd1_arr = np.append(lambd1_arr, [p['lambda_1']] * (p['t_2'] - p['t_1'] - p['delta_1']))\n",
        "\n",
        "    # Compute lambd2 array\n",
        "    if p['delta_2'] == 1:\n",
        "        lambd2_arr = np.array([p['lambda_2']] * (p['t_3'] - p['t_2']))\n",
        "    else:\n",
        "        lambd2_arr = np.linspace(p['lambda_1'], p['lambda_2'], p['delta_2'])\n",
        "        lambd2_arr = np.append(lambd2_arr, [p['lambda_2']] * (p['t_3'] - p['t_2'] - p['delta_2']))\n",
        "\n",
        "    # Compute lambd3 array\n",
        "    if p['delta_3'] == 1:\n",
        "        lambd3_arr = np.array([p['lambda_3']] * (p['t_4'] - p['t_3']))\n",
        "    else:\n",
        "        lambd3_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_3'])\n",
        "        lambd3_arr = np.append(lambd3_arr, [p['lambda_3']] * (p['t_4'] - p['t_3'] - p['delta_3']))\n",
        "\n",
        "    # Compute lambd4 array\n",
        "    if p['delta_4'] == 1:\n",
        "        lambd4_arr = np.array([p['lambda_4']] * (p['t_5'] - p['t_4']))\n",
        "    else:\n",
        "        lambd4_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_4'])\n",
        "        lambd4_arr = np.append(lambd4_arr, [p['lambda_4']] * (p['t_5'] - p['t_4'] - p['delta_4']))\n",
        "\n",
        "    return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
        "\n",
        "\n",
        "def non_stationary_SEICR(prior_draw, N, T, sim_diff=16, observation_model=True):\n",
        "    try:\n",
        "        p = const_params | prior_draw  # if prior_values was passed as dict\n",
        "    except:\n",
        "        p = const_params | dict(zip(param_names, prior_draw))  # if prior_values was passed as list\n",
        "    assert check_params(p)\n",
        "    assert sim_diff > max(p['lag_I'],p['lag_R'],p['lag_D'])\n",
        "    p['sim_lag'] = sim_diff - 1\n",
        "    lambd_arr = calc_lambda_array(p)\n",
        "\n",
        "    # Initial conditions\n",
        "    S, E, C, I, R, D = [N - p['E_0']], [p['E_0']], [0], [0], [0], [0]\n",
        "\n",
        "    # Containers\n",
        "    I_news = []\n",
        "    R_news = []\n",
        "    D_news = []\n",
        "\n",
        "    # Reported new cases\n",
        "    I_data = np.zeros(p['t_5'])\n",
        "    R_data = np.zeros(p['t_5'])\n",
        "    D_data = np.zeros(p['t_5'])\n",
        "    fs_I = np.zeros(p['t_5'])\n",
        "    fs_R = np.zeros(p['t_5'])\n",
        "    fs_D = np.zeros(p['t_5'])\n",
        "\n",
        "    # Simulate T-1 tiemsteps\n",
        "    for t in range(p['t_5'] + p['sim_lag']):\n",
        "        # Calculate new exposed cases\n",
        "        E_new = lambd_arr[t] * ((C[t] + p['beta'] * I[t]) / N) * S[t]\n",
        "\n",
        "        # Remove exposed from susceptible\n",
        "        S_t = S[t] - E_new\n",
        "\n",
        "        # Calculate current exposed by adding new exposed and\n",
        "        # subtracting the exposed becoming carriers.\n",
        "        E_t = E[t] + E_new - p['gamma'] * E[t]\n",
        "\n",
        "        # Calculate current carriers by adding the new exposed and subtracting\n",
        "        # those who will develop symptoms and become detected and those who\n",
        "        # will go through the disease asymptomatically.\n",
        "        C_t = C[t] + p['gamma'] * E[t] - (1 - p['alpha']) * p['eta'] * C[t] - p['alpha'] * p['theta'] * C[t]\n",
        "\n",
        "        # Calculate current infected by adding the symptomatic carriers and\n",
        "        # subtracting the dead and recovered. The newly infected are just the\n",
        "        # carriers who get detected.\n",
        "        I_t = I[t] + (1 - p['alpha']) * p['eta'] * C[t] - (1 - p['delta']) * p['mu'] * I[t] - p['delta'] * p['epsilon'] * I[t]\n",
        "        I_new = (1 - p['alpha']) * p['eta'] * C[t]\n",
        "\n",
        "        # Calculate current recovered by adding the symptomatic and asymptomatic\n",
        "        # recovered. The newly recovered are only the detected recovered\n",
        "        R_t = R[t] + p['alpha'] * p['theta'] * C[t] + (1 - p['delta']) * p['mu'] * I[t]\n",
        "        R_new = (1 - p['delta']) * p['mu'] * I[t]\n",
        "\n",
        "        # Calculate the current dead\n",
        "        D_t = D[t] + p['delta'] * p['epsilon'] * I[t]\n",
        "        D_new = p['delta'] * p['epsilon'] * I[t]\n",
        "\n",
        "        # Ensure some numerical onstraints\n",
        "        S_t = np.clip(S_t, 0, N)\n",
        "        E_t = np.clip(E_t, 0, N)\n",
        "        C_t = np.clip(C_t, 0, N)\n",
        "        I_t = np.clip(I_t, 0, N)\n",
        "        R_t = np.clip(R_t, 0, N)\n",
        "        D_t = np.clip(D_t, 0, N)\n",
        "\n",
        "        # Keep track of process over time\n",
        "        S.append(S_t)\n",
        "        E.append(E_t)\n",
        "        C.append(C_t)\n",
        "        I.append(I_t)\n",
        "        R.append(R_t)\n",
        "        D.append(D_t)\n",
        "        I_news.append(I_new)\n",
        "        R_news.append(R_new)\n",
        "        D_news.append(D_new)\n",
        "\n",
        "        # From here, start adding new cases with delay D\n",
        "        # Note, we assume the same delay\n",
        "        if t >= p['sim_lag']:\n",
        "            # Compute lags and add to data arrays\n",
        "            fs_I[t - p['sim_lag']] = (1 - p['f_I']) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - p['sim_lag']) - 0.5 * p['phi_I']))\n",
        "            )\n",
        "            fs_R[t - p['sim_lag']] = (1 - p['f_R']) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - p['sim_lag']) - 0.5 * p['phi_R']))\n",
        "            )\n",
        "            fs_D[t - p['sim_lag']] = (1 - p['f_D']) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - p['sim_lag']) - 0.5 * p['phi_D']))\n",
        "            )\n",
        "            I_data[t - p['sim_lag']] = I_news[t - p['lag_I']]\n",
        "            R_data[t - p['sim_lag']] = R_news[t - p['lag_R']]\n",
        "            D_data[t - p['sim_lag']] = D_news[t - p['lag_D']]\n",
        "\n",
        "    # Compute weekly modulation\n",
        "    I_data = (1 - fs_I) * I_data\n",
        "    R_data = (1 - fs_R) * R_data\n",
        "    D_data = (1 - fs_D) * D_data\n",
        "\n",
        "    # Add noise\n",
        "    # I_data = stats.t(df=4, loc=I_data, scale=np.sqrt(I_data) * scale_I).rvs()\n",
        "    # R_data = stats.t(df=4, loc=R_data, scale=np.sqrt(R_data) * scale_R).rvs()\n",
        "    # D_data = stats.t(df=4, loc=D_data, scale=np.sqrt(D_data) * scale_D).rvs()\n",
        "    I_data = I_data + RNG.standard_t(4) * np.sqrt(I_data) * p['sigma_I']\n",
        "    R_data = R_data + RNG.standard_t(4) * np.sqrt(R_data) * p['sigma_R']\n",
        "    D_data = D_data + RNG.standard_t(4) * np.sqrt(D_data) * p['sigma_D']\n",
        "\n",
        "    Y = pd.DataFrame({'S':S,'E':E,'C':C,'I':I,'R':R,'D':D,'dI_obs':I_data,'dR_obs':R_data,'dD_obs':D_data}).clip(0, N) / SCALE\n",
        "    if observation_model:\n",
        "        Y = Y.filter(like='obs')\n",
        "    return Y\n",
        "non_stationary_SEICR(prior_fun(), real_data['N'], real_data['T'])\n",
        "# prior_fun()"
      ],
      "metadata": {
        "id": "6z3L8a48GqtW"
      },
      "id": "6z3L8a48GqtW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    \"\"\"Download and prepare data from Johns Hopkins\"\"\"\n",
        "    N = 83e6  / SCALE\n",
        "    T = 100\n",
        "    start = 39\n",
        "    def fetch(cl):\n",
        "        return (\n",
        "            pd.read_csv(f'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_{cl}_global.csv', sep=\",\")\n",
        "            .drop(columns=['Province/State','Lat','Long'])\n",
        "            .groupby('Country/Region').sum()\n",
        "            .loc['Germany']\n",
        "        )\n",
        "    data = (\n",
        "        pd.DataFrame(\n",
        "        {'dI_real':fetch('confirmed'), 'dR_real':fetch('recovered'), 'dD_real':fetch('deaths')})\n",
        "        .iloc[start:start+T].assign(date = lambda x: pd.to_datetime(x.index)).set_index('date')\n",
        "        .diff().dropna().div(SCALE).clip(0, N)\n",
        "        )\n",
        "    return dict(x=data, T=T, N=N, mean=data.mean(), std=data.std())\n",
        "real_data = load_data()"
      ],
      "metadata": {
        "id": "ympMav2ztdC1"
      },
      "id": "ympMav2ztdC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# non_stationary_SEICR(\n"
      ],
      "metadata": {
        "id": "TCOiXDD1M2gI"
      },
      "id": "TCOiXDD1M2gI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def prior_sir():\n",
        "    \"\"\"\n",
        "    Implements batch sampling from a stationary prior over the parameters\n",
        "    of the non-stationary SIR model.\n",
        "    \"\"\"\n",
        "\n",
        "    t1 = np.random.normal(loc=8, scale=3)\n",
        "    t2 = np.random.normal(loc=15, scale=1)\n",
        "    t3 = np.random.normal(loc=22, scale=1)\n",
        "    t4 = np.random.normal(loc=66, scale=1)\n",
        "    delta_t1 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
        "    delta_t2 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
        "    delta_t3 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
        "    delta_t4 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
        "    lambd0 = np.random.lognormal(mean=np.log(1.2), sigma=0.5)\n",
        "    lambd1 = np.random.lognormal(mean=np.log(0.6), sigma=0.5)\n",
        "    lambd2 = np.random.lognormal(mean=np.log(0.3), sigma=0.5)\n",
        "    lambd3 = np.random.lognormal(mean=np.log(0.1), sigma=0.5)\n",
        "    lambd4 = np.random.lognormal(mean=np.log(0.1), sigma=0.5)\n",
        "    mu = np.random.lognormal(mean=np.log(1 / 8), sigma=0.2)\n",
        "    f_i = np.random.beta(a=alpha_f, b=beta_f)\n",
        "    phi_i = stats.vonmises(kappa=0.01).rvs()\n",
        "    f_r = np.random.beta(a=alpha_f, b=beta_f)\n",
        "    phi_r = stats.vonmises(kappa=0.01).rvs()\n",
        "    f_d = np.random.beta(a=alpha_f, b=beta_f)\n",
        "    phi_d = stats.vonmises(kappa=0.01).rvs()\n",
        "    D_i = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
        "    D_r = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
        "    D_d = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
        "    E0 = np.random.gamma(shape=2, scale=30)\n",
        "    scale_I = np.random.gamma(shape=1, scale=5)\n",
        "    scale_R = np.random.gamma(shape=1, scale=5)\n",
        "    scale_D = np.random.gamma(shape=1, scale=5)\n",
        "    return [\n",
        "        t1,\n",
        "        t2,\n",
        "        t3,\n",
        "        t4,\n",
        "        delta_t1,\n",
        "        delta_t2,\n",
        "        delta_t3,\n",
        "        delta_t4,\n",
        "        lambd0,\n",
        "        lambd1,\n",
        "        lambd2,\n",
        "        lambd3,\n",
        "        lambd4,\n",
        "        mu,\n",
        "        f_i,\n",
        "        phi_i,\n",
        "        f_r,\n",
        "        phi_r,\n",
        "        f_d,\n",
        "        phi_d,\n",
        "        D_i,\n",
        "        D_r,\n",
        "        D_d,\n",
        "        E0,\n",
        "        scale_I,\n",
        "        scale_R,\n",
        "        scale_D,\n",
        "    ]\n",
        "\n",
        "\n",
        "def prior_secir():\n",
        "    \"\"\"\n",
        "    Implements batch sampling from a stationary prior over the parameters\n",
        "    of the non-stationary SIR model.\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = np.random.uniform(low=0.005, high=0.9)\n",
        "    beta = np.random.lognormal(mean=np.log(0.25), sigma=0.3)\n",
        "    gamma = np.random.lognormal(mean=np.log(1 / 6.5), sigma=0.5)\n",
        "    eta = np.random.lognormal(mean=np.log(1 / 3.2), sigma=0.3)\n",
        "    theta = np.random.uniform(low=1 / 14, high=1 / 3)\n",
        "    delta = np.random.uniform(low=0.01, high=0.3)\n",
        "    d = np.random.uniform(low=1 / 14, high=1 / 3)\n",
        "    return [alpha, beta, gamma, eta, theta, delta, d]\n",
        "\n",
        "\n",
        "def calc_lambda_array(\n",
        "    sim_lag,\n",
        "    lambd0,\n",
        "    lambd1,\n",
        "    lambd2,\n",
        "    lambd3,\n",
        "    lambd4,\n",
        "    t1,\n",
        "    t2,\n",
        "    t3,\n",
        "    t4,\n",
        "    delta_t1,\n",
        "    delta_t2,\n",
        "    delta_t3,\n",
        "    delta_t4,\n",
        "    T,\n",
        "):\n",
        "    \"\"\"Computes the array of time-varying contact rates/transimission probabilities.\"\"\"\n",
        "\n",
        "    # Array of initial lambdas\n",
        "    lambd0_arr = np.array([lambd0] * (t1 + sim_lag))\n",
        "\n",
        "    # Compute lambd1 array\n",
        "    if delta_t1 == 1:\n",
        "        lambd1_arr = np.array([lambd1] * (t2 - t1))\n",
        "    else:\n",
        "        lambd1_arr = np.linspace(lambd0, lambd1, delta_t1)\n",
        "        lambd1_arr = np.append(lambd1_arr, [lambd1] * (t2 - t1 - delta_t1))\n",
        "\n",
        "    # Compute lambd2 array\n",
        "    if delta_t2 == 1:\n",
        "        lambd2_arr = np.array([lambd2] * (t3 - t2))\n",
        "    else:\n",
        "        lambd2_arr = np.linspace(lambd1, lambd2, delta_t2)\n",
        "        lambd2_arr = np.append(lambd2_arr, [lambd2] * (t3 - t2 - delta_t2))\n",
        "\n",
        "    # Compute lambd3 array\n",
        "    if delta_t3 == 1:\n",
        "        lambd3_arr = np.array([lambd3] * (t4 - t3))\n",
        "    else:\n",
        "        lambd3_arr = np.linspace(lambd3, lambd4, delta_t3)\n",
        "        lambd3_arr = np.append(lambd3_arr, [lambd3] * (t4 - t3 - delta_t3))\n",
        "\n",
        "    # Compute lambd4 array\n",
        "    if delta_t4 == 1:\n",
        "        lambd4_arr = np.array([lambd4] * (T - t4))\n",
        "    else:\n",
        "        lambd4_arr = np.linspace(lambd3, lambd4, delta_t4)\n",
        "        lambd4_arr = np.append(lambd4_arr, [lambd4] * (T - t4 - delta_t4))\n",
        "\n",
        "    return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
        "\n",
        "\n",
        "def non_stationary_SEICR(\n",
        "    params_sir, params_secir, N, T, sim_diff=16, observation_model=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs a forward simulation from the stationary SIR model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract parameters\n",
        "    (\n",
        "        t1,\n",
        "        t2,\n",
        "        t3,\n",
        "        t4,\n",
        "        delta_t1,\n",
        "        delta_t2,\n",
        "        delta_t3,\n",
        "        delta_t4,\n",
        "        lambd0,\n",
        "        lambd1,\n",
        "        lambd2,\n",
        "        lambd3,\n",
        "        lambd4,\n",
        "        mu,\n",
        "        f_i,\n",
        "        phi_i,\n",
        "        f_r,\n",
        "        phi_r,\n",
        "        f_d,\n",
        "        phi_d,\n",
        "        delay_i,\n",
        "        delay_r,\n",
        "        delay_d,\n",
        "        E0,\n",
        "        scale_I,\n",
        "        scale_R,\n",
        "        scale_D,\n",
        "    ) = params_sir\n",
        "    alpha, beta, gamma, eta, theta, delta, d = params_secir\n",
        "\n",
        "    # Round integer parameters\n",
        "    t1, t2, t3, t4 = int(round(t1)), int(round(t2)), int(round(t3)), int(round(t4))\n",
        "    delta_t1, delta_t2, delta_t3, delta_t4 = (\n",
        "        int(round(delta_t1)),\n",
        "        int(round(delta_t2)),\n",
        "        int(round(delta_t3)),\n",
        "        int(round(delta_t4)),\n",
        "    )\n",
        "    E0 = max(1, np.round(E0))\n",
        "    delay_i = int(round(delay_i))\n",
        "    delay_r = int(round(delay_r))\n",
        "    delay_d = int(round(delay_d))\n",
        "\n",
        "    # Impose constraints\n",
        "    assert sim_diff > delay_i\n",
        "    assert sim_diff > delay_r\n",
        "    assert sim_diff > delay_d\n",
        "    assert t1 > 0 and t2 > 0 and t3 > 0 and t4 > 0\n",
        "    assert t1 < t2 < t3 < t4\n",
        "    assert delta_t1 > 0 and delta_t2 > 0 and delta_t3 > 0 and delta_t4 > 0\n",
        "    assert (\n",
        "        t2 - t1 >= delta_t1\n",
        "        and t3 - t2 >= delta_t2\n",
        "        and t4 - t3 >= delta_t3\n",
        "        and T - t4 >= delta_t4\n",
        "    )\n",
        "\n",
        "    # Calculate lambda arrays\n",
        "    # Lambda0 is the initial contact rate which will be consecutively\n",
        "    # reduced via the government measures\n",
        "    sim_lag = sim_diff - 1\n",
        "    lambd_arr = calc_lambda_array(\n",
        "        sim_lag,\n",
        "        lambd0,\n",
        "        lambd1,\n",
        "        lambd2,\n",
        "        lambd3,\n",
        "        lambd4,\n",
        "        t1,\n",
        "        t2,\n",
        "        t3,\n",
        "        t4,\n",
        "        delta_t1,\n",
        "        delta_t2,\n",
        "        delta_t3,\n",
        "        delta_t4,\n",
        "        T,\n",
        "    )\n",
        "\n",
        "    # Initial conditions\n",
        "    S, E, C, I, R, D = [N - E0], [E0], [0], [0], [0], [0]\n",
        "\n",
        "    # Containers\n",
        "    I_news = []\n",
        "    R_news = []\n",
        "    D_news = []\n",
        "\n",
        "    # Reported new cases\n",
        "    I_data = np.zeros(T)\n",
        "    R_data = np.zeros(T)\n",
        "    D_data = np.zeros(T)\n",
        "    fs_i = np.zeros(T)\n",
        "    fs_r = np.zeros(T)\n",
        "    fs_d = np.zeros(T)\n",
        "\n",
        "    # Simulate T-1 tiemsteps\n",
        "    for t in range(T + sim_lag):\n",
        "        # Calculate new exposed cases\n",
        "        E_new = lambd_arr[t] * ((C[t] + beta * I[t]) / N) * S[t]\n",
        "\n",
        "        # Remove exposed from susceptible\n",
        "        S_t = S[t] - E_new\n",
        "\n",
        "        # Calculate current exposed by adding new exposed and\n",
        "        # subtracting the exposed becoming carriers.\n",
        "        E_t = E[t] + E_new - gamma * E[t]\n",
        "\n",
        "        # Calculate current carriers by adding the new exposed and subtracting\n",
        "        # those who will develop symptoms and become detected and those who\n",
        "        # will go through the disease asymptomatically.\n",
        "        C_t = C[t] + gamma * E[t] - (1 - alpha) * eta * C[t] - alpha * theta * C[t]\n",
        "\n",
        "        # Calculate current infected by adding the symptomatic carriers and\n",
        "        # subtracting the dead and recovered. The newly infected are just the\n",
        "        # carriers who get detected.\n",
        "        I_t = (\n",
        "            I[t] + (1 - alpha) * eta * C[t] - (1 - delta) * mu * I[t] - delta * d * I[t]\n",
        "        )\n",
        "        I_new = (1 - alpha) * eta * C[t]\n",
        "\n",
        "        # Calculate current recovered by adding the symptomatic and asymptomatic\n",
        "        # recovered. The newly recovered are only the detected recovered\n",
        "        R_t = R[t] + alpha * theta * C[t] + (1 - delta) * mu * I[t]\n",
        "        R_new = (1 - delta) * mu * I[t]\n",
        "\n",
        "        # Calculate the current dead\n",
        "        D_t = D[t] + delta * d * I[t]\n",
        "        D_new = delta * d * I[t]\n",
        "\n",
        "        # Ensure some numerical onstraints\n",
        "        S_t = np.clip(S_t, 0, N)\n",
        "        E_t = np.clip(E_t, 0, N)\n",
        "        C_t = np.clip(C_t, 0, N)\n",
        "        I_t = np.clip(I_t, 0, N)\n",
        "        R_t = np.clip(R_t, 0, N)\n",
        "        D_t = np.clip(D_t, 0, N)\n",
        "\n",
        "        # Keep track of process over time\n",
        "        S.append(S_t)\n",
        "        E.append(E_t)\n",
        "        C.append(C_t)\n",
        "        I.append(I_t)\n",
        "        R.append(R_t)\n",
        "        D.append(D_t)\n",
        "        I_news.append(I_new)\n",
        "        R_news.append(R_new)\n",
        "        D_news.append(D_new)\n",
        "\n",
        "        # From here, start adding new cases with delay D\n",
        "        # Note, we assume the same delay\n",
        "        if t >= sim_lag:\n",
        "            # Compute lags and add to data arrays\n",
        "            fs_i[t - sim_lag] = (1 - f_i) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * phi_i))\n",
        "            )\n",
        "            fs_r[t - sim_lag] = (1 - f_r) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * phi_r))\n",
        "            )\n",
        "            fs_d[t - sim_lag] = (1 - f_d) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * phi_d))\n",
        "            )\n",
        "            I_data[t - sim_lag] = I_news[t - delay_i]\n",
        "            R_data[t - sim_lag] = R_news[t - delay_r]\n",
        "            D_data[t - sim_lag] = D_news[t - delay_d]\n",
        "\n",
        "    # Compute weekly modulation\n",
        "    I_data = (1 - fs_i) * I_data\n",
        "    R_data = (1 - fs_r) * R_data\n",
        "    D_data = (1 - fs_d) * D_data\n",
        "\n",
        "    # Add noise\n",
        "    I_data = stats.t(df=4, loc=I_data, scale=np.sqrt(I_data) * scale_I).rvs()\n",
        "    R_data = stats.t(df=4, loc=R_data, scale=np.sqrt(R_data) * scale_R).rvs()\n",
        "    D_data = stats.t(df=4, loc=D_data, scale=np.sqrt(D_data) * scale_D).rvs()\n",
        "\n",
        "    Y = pd.DataFrame({'S':S,'E':E,'C':C,'I':I,'R':R,'D':D,'dI_obs':I_data,'dR_obs':R_data,'dD_obs':D_data}).clip(0, N) / SCALE\n",
        "    if observation_model:\n",
        "        Y = Y.filter(like='obs')\n",
        "    return Y\n",
        "\n",
        "def simulate(params=None, n_params=1, n_sim, params, N, T, sim_diff=21, observation_model=True):\n",
        "\n",
        "\n",
        "def simulate(*, N, T, sim_diff=21, observation_model=True, n_sim=1, n_params=1, params=None):\n",
        "    if params is None:\n",
        "\n",
        "\n",
        "\n",
        "    x = []\n",
        "    theta1, theta2 = params[:-7], params[-7:]\n",
        "    return np.array([non_stationary_SEICR(theta1, theta2, N=N, T=T, sim_diff=sim_diff, observation_model=observation_model) for _ in range(n_sim)])\n",
        "    # for _ in range(n_sim):\n",
        "    #     x_i = non_stationary_SEICR(\n",
        "    #         theta1,\n",
        "    #         theta2,\n",
        "    #         N=N,\n",
        "    #         T=T,\n",
        "    #         sim_diff=sim_diff,\n",
        "    #         observation_model=observation_model,\n",
        "    #     )\n",
        "    #     x.append(x_i)\n",
        "    # return np.clip(np.array(x), 0, np.inf)\n",
        "\n",
        "def data_generator(batch_size, T=None, N=None, sim_diff=21, observation_model=True):\n",
        "    \"\"\"\n",
        "    Runs the forward model 'batch_size' times by first sampling fromt the prior\n",
        "    theta ~ p(theta) and running x ~ p(x|theta).\n",
        "    ----------\n",
        "\n",
        "    Arguments:\n",
        "    batch_size : int -- the number of samples to draw from the prior\n",
        "    ----------\n",
        "\n",
        "    Output:\n",
        "    forward_dict : dict\n",
        "        The expected outputs for a BayesFlow pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # Generate data\n",
        "    # x is a np.ndarray of shape (batch_size, n_obs, x_dim)\n",
        "    x = []\n",
        "    theta = []\n",
        "    for i in range(batch_size):\n",
        "        # Reject meaningless simulaitons\n",
        "        x_i = None\n",
        "        while x_i is None:\n",
        "            try:\n",
        "                theta1 = prior_sir()\n",
        "                theta2 = prior_secir()\n",
        "                x_i = non_stationary_SEICR(theta1, theta2, N, T, sim_diff=sim_diff)\n",
        "            except:\n",
        "                pass\n",
        "        # Simulate SECIR\n",
        "        x.append(x_i)\n",
        "        theta.append(theta1 + theta2)\n",
        "\n",
        "    # Clip negative and normalize\n",
        "    x = np.clip(np.array(x), 0.0, np.inf) / scale\n",
        "    theta = np.array(theta)\n",
        "\n",
        "    forward_dict = {\"prior_draws\": theta, \"sim_data\": x}\n",
        "    return forward_dict\n",
        "\n",
        "class MultiConvLayer(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_filters=32, strides=1):\n",
        "        super(MultiConvLayer, self).__init__()\n",
        "\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(\n",
        "                n_filters // 2,\n",
        "                kernel_size=f,\n",
        "                strides=strides,\n",
        "                padding=\"causal\",\n",
        "                activation=\"relu\",\n",
        "                kernel_initializer=\"glorot_uniform\",\n",
        "            )\n",
        "            for f in range(2, 8)\n",
        "        ]\n",
        "        self.dim_red = tf.keras.layers.Conv1D(\n",
        "            n_filters, 1, 1, activation=\"relu\", kernel_initializer=\"glorot_uniform\"\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = tf.concat([conv(x) for conv in self.convs], axis=-1)\n",
        "        out = self.dim_red(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiConvNet(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_layers=3, n_filters=64, strides=1):\n",
        "        super(MultiConvNet, self).__init__()\n",
        "\n",
        "        self.net = tf.keras.Sequential(\n",
        "            [MultiConvLayer(n_filters, strides) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        self.lstm = LSTM(n_filters)\n",
        "\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = self.net(x)\n",
        "        out = self.lstm(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SummaryNet(tf.keras.Model):\n",
        "    def __init__(self, n_summary):\n",
        "        super(SummaryNet, self).__init__()\n",
        "        self.net_I = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_R = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_D = MultiConvNet(n_filters=n_summary // 3)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        x = tf.split(x, 3, axis=-1)\n",
        "        x_i = self.net_I(x[0])\n",
        "        x_r = self.net_R(x[1])\n",
        "        x_d = self.net_D(x[2])\n",
        "        return tf.concat([x_i, x_r, x_d], axis=-1)\n",
        "\n",
        "class MemoryNetwork(tf.keras.Model):\n",
        "    def __init__(self, meta):\n",
        "        super(MemoryNetwork, self).__init__()\n",
        "\n",
        "        self.gru = GRU(meta[\"n_hidden\"], return_sequences=True, return_state=True)\n",
        "        self.h = meta[\"n_hidden\"]\n",
        "        self.n_params = meta[\"n_params\"]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, target, condition):\n",
        "        \"\"\"Performs a forward pass through the network.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        target    : tf.Tesnor of shape (batch_size, time_stes, dim)\n",
        "            The time-dependent signal to process.\n",
        "        condition : tf.Tensor of shape (batch_size, cond_dim)\n",
        "            The conditional (static) variables, e.g., parameters.\n",
        "        \"\"\"\n",
        "        shift_target = target[:, :-1, :]\n",
        "        init = tf.zeros((target.shape[0], 1, target.shape[2]))\n",
        "        inp_teacher = tf.concat([init, shift_target], axis=1)\n",
        "        inp_teacher_c = tf.concat([inp_teacher, condition], axis=-1)\n",
        "        out, _ = self.gru(inp_teacher_c)\n",
        "        return out\n",
        "\n",
        "    def step_loop(self, target, condition, state):\n",
        "        out, new_state = self.gru(\n",
        "            tf.concat([target, condition], axis=-1), initial_state=state\n",
        "        )\n",
        "        return out, new_state\n",
        "\n",
        "class InvertibleNetworkWithMemory(tf.keras.Model):\n",
        "    \"\"\"Implements a chain of conditional invertible blocks for Bayesian parameter inference.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_params,\n",
        "        num_coupling_layers=4,\n",
        "        coupling_settings=None,\n",
        "        coupling_design=\"affine\",\n",
        "        permutation=\"fixed\",\n",
        "        use_act_norm=True,\n",
        "        act_norm_init=None,\n",
        "        use_soft_flow=False,\n",
        "        soft_flow_bounds=(1e-3, 5e-2),\n",
        "    ):\n",
        "        \"\"\"Initializes a custom invertible network with recurrent memory.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Create settings dict for coupling layer\n",
        "        settings = dict(\n",
        "            latent_dim=num_params,\n",
        "            coupling_settings=coupling_settings,\n",
        "            coupling_design=coupling_design,\n",
        "            permutation=permutation,\n",
        "            use_act_norm=use_act_norm,\n",
        "            act_norm_init=act_norm_init,\n",
        "        )\n",
        "\n",
        "        # Create sequence of coupling layers and store reference to dimensionality\n",
        "        self.coupling_layers = [\n",
        "            CouplingLayer(**settings) for _ in range(num_coupling_layers)\n",
        "        ]\n",
        "\n",
        "        # Store attributes\n",
        "        self.soft_flow = use_soft_flow\n",
        "        self.soft_low = soft_flow_bounds[0]\n",
        "        self.soft_high = soft_flow_bounds[1]\n",
        "        self.use_act_norm = use_act_norm\n",
        "        self.latent_dim = num_params\n",
        "        self.dynamic_summary_net = MemoryNetwork({\"n_hidden\": 256, \"n_params\": 3})\n",
        "        self.latent_dim = num_params\n",
        "\n",
        "    def call(self, targets, condition, inverse=False):\n",
        "        \"\"\"Performs one pass through an invertible chain (either inverse or forward).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        targets   : tf.Tensor\n",
        "            The estimation quantities of interest, shape (batch_size, ...)\n",
        "        condition : tf.Tensor\n",
        "            The conditional data x, shape (batch_size, summary_dim)\n",
        "        inverse   : bool, default: False\n",
        "            Flag indicating whether to run the chain forward or backwards\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (z, log_det_J)  :  tuple(tf.Tensor, tf.Tensor)\n",
        "            If inverse=False: The transformed input and the corresponding Jacobian of the transformation,\n",
        "            v shape: (batch_size, ...), log_det_J shape: (batch_size, ...)\n",
        "\n",
        "        target          :  tf.Tensor\n",
        "            If inverse=True: The transformed out, shape (batch_size, ...)\n",
        "\n",
        "        Important\n",
        "        ---------\n",
        "        If ``inverse=False``, the return is ``(z, log_det_J)``.\\n\n",
        "        If ``inverse=True``, the return is ``target``.\n",
        "        \"\"\"\n",
        "\n",
        "        if inverse:\n",
        "            return self.inverse(targets, condition)\n",
        "        return self.forward(targets, condition)\n",
        "\n",
        "    @tf.function\n",
        "    def forward(self, targets, condition, **kwargs):\n",
        "        \"\"\"Performs a forward pass though the chain.\"\"\"\n",
        "\n",
        "        # Add memory condition\n",
        "        memory = self.dynamic_summary_net(targets, condition)\n",
        "        condition = tf.concat([memory, condition], axis=-1)\n",
        "\n",
        "        z = targets\n",
        "        log_det_Js = []\n",
        "        for layer in self.coupling_layers:\n",
        "            z, log_det_J = layer(z, condition, **kwargs)\n",
        "            log_det_Js.append(log_det_J)\n",
        "        # Sum Jacobian determinants for all layers (coupling blocks) to obtain total Jacobian.\n",
        "        log_det_J = tf.add_n(log_det_Js)\n",
        "        return z, log_det_J\n",
        "\n",
        "    @tf.function\n",
        "    def inverse(self, z, condition, **kwargs):\n",
        "        \"\"\"Performs a reverse pass through the chain.\"\"\"\n",
        "\n",
        "        target = z\n",
        "        T = z.shape[1]\n",
        "        gru_inp = tf.zeros((z.shape[0], 1, z.shape[-1]))\n",
        "        state = tf.zeros((z.shape[0], self.dynamic_summary_net.h))\n",
        "        outs = []\n",
        "        for t in range(T):\n",
        "            # One step condition\n",
        "            memory, state = self.dynamic_summary_net.step_loop(\n",
        "                gru_inp, condition[:, t : t + 1, :], state\n",
        "            )\n",
        "            condition_t = tf.concat([memory, condition[:, t : t + 1, :]], axis=-1)\n",
        "            target_t = target[:, t : t + 1, :]\n",
        "            for layer in reversed(self.coupling_layers):\n",
        "                target_t = layer(target_t, condition_t, inverse=True, **kwargs)\n",
        "            outs.append(target_t)\n",
        "            gru_inp = target_t\n",
        "        return tf.concat(outs, axis=1)\n",
        "\n",
        "# def configurator(forward_dict):\n",
        "#     \"\"\"Customized preprocessing for the Covid simulator.\"\"\"\n",
        "\n",
        "#     out = {\"posterior_inputs\": {}, \"likelihood_inputs\": {}}\n",
        "\n",
        "#     # Extract data\n",
        "#     x = forward_dict[\"sim_data\"].astype(np.float32)\n",
        "#     x_means = np.mean(x, axis=1, keepdims=True)\n",
        "#     x_std = np.std(x, axis=1, keepdims=True)\n",
        "#     x = (x - x_means) / x_std\n",
        "#     log_mu = np.log2(1 + x_means[:, 0, :])\n",
        "#     log_std = np.log2(1 + x_std[:, 0, :])\n",
        "\n",
        "#     # Extract params\n",
        "#     p = forward_dict[\"prior_draws\"].astype(np.float32)\n",
        "#     p = (p - theta_mu) / theta_std\n",
        "\n",
        "#     # Repeat condition\n",
        "#     cond = np.concatenate([p, log_mu, log_std], axis=-1)\n",
        "#     cond = np.stack([cond] * x.shape[1], axis=1)\n",
        "\n",
        "#     # Likelihood inputs\n",
        "#     out[\"likelihood_inputs\"][\"observables\"] = x.astype(np.float32)\n",
        "#     out[\"likelihood_inputs\"][\"conditions\"] = np.concatenate([cond], axis=-1).astype(\n",
        "#         np.float32\n",
        "#     )\n",
        "\n",
        "#     # Posterior inputs\n",
        "#     out[\"posterior_inputs\"][\"parameters\"] = p\n",
        "#     out[\"posterior_inputs\"][\"summary_conditions\"] = out[\"likelihood_inputs\"][\n",
        "#         \"observables\"\n",
        "#     ]\n",
        "#     out[\"posterior_inputs\"][\"direct_conditions\"] = np.concatenate(\n",
        "#         [log_mu, log_std], axis=-1\n",
        "#     )\n",
        "\n",
        "#     return out\n",
        "\n",
        "\n",
        "def configurator(forward_dict):\n",
        "    \"\"\"Customized preprocessing for the Covid simulator.\"\"\"\n",
        "\n",
        "    out = {\"posterior_inputs\": {}, \"likelihood_inputs\": {}}\n",
        "\n",
        "    # Extract data\n",
        "    x = forward_dict[\"sim_data\"].astype(np.float32)\n",
        "    x_means = np.mean(x, axis=1, keepdims=True)\n",
        "    x_std = np.std(x, axis=1, keepdims=True)\n",
        "    x = (x - x_means) / x_std\n",
        "    log_mu = np.log2(1 + x_means[:, 0, :])\n",
        "    log_std = np.log2(1 + x_std[:, 0, :])\n",
        "\n",
        "    # Extract params\n",
        "    p = forward_dict[\"prior_draws\"].astype(np.float32)\n",
        "    p = (p - theta_mu) / theta_std\n",
        "\n",
        "    # Repeat condition\n",
        "    cond = np.concatenate([p, log_mu, log_std], axis=-1)\n",
        "    cond = np.stack([cond] * x.shape[1], axis=1)\n",
        "\n",
        "    # Likelihood inputs\n",
        "    forward_dict['likelihood_inputs'] = {\n",
        "        'observables': x.astype(np.float32),\n",
        "        'conditions' : np.float32(np.concatenate([cond], axis=-1)),\n",
        "        }\n",
        "\n",
        "    forward_dict['posterior_inputs'] = {\n",
        "        'parameters': p,\n",
        "        'summary_conditions': forward_dict['likelihood_inputs']['observables'],\n",
        "        'direct_conditions': np.concatenate([log_mu, log_std], axis=-1,)\n",
        "        }\n",
        "    return forward_dict\n",
        "\n",
        "# param_names = [\n",
        "#     r\"$t_1$\",\n",
        "#     r\"$t_2$\",\n",
        "#     r\"$t_3$\",\n",
        "#     r\"$t_4$\",\n",
        "#     r\"$\\Delta t_1$\",\n",
        "#     r\"$\\Delta t_2$\",\n",
        "#     r\"$\\Delta t_3$\",\n",
        "#     r\"$\\Delta t_4$\",\n",
        "#     r\"$\\lambda_0$\",\n",
        "#     r\"$\\lambda_1$\",\n",
        "#     r\"$\\lambda_2$\",\n",
        "#     r\"$\\lambda_3$\",\n",
        "#     r\"$\\lambda_4$\",\n",
        "#     r\"$\\mu$\",\n",
        "#     r\"$A_I$\",\n",
        "#     r\"$\\phi_I$\",\n",
        "#     r\"$A_R$\",\n",
        "#     r\"$\\phi_R$\",\n",
        "#     r\"$A_D$\",\n",
        "#     r\"$\\phi_D$\",\n",
        "#     r\"$L_I$\",\n",
        "#     r\"$L_R$\",\n",
        "#     r\"$L_D$\",\n",
        "#     r\"$E_0$\",\n",
        "#     r\"$\\sigma_I$\",\n",
        "#     r\"$\\sigma_R$\",\n",
        "#     r\"$\\sigma_D$\",\n",
        "#     r\"$\\alpha$\",\n",
        "#     r\"$\\beta$\",\n",
        "#     r\"$\\gamma$\",\n",
        "#     r\"$\\eta$\",\n",
        "#     r\"$\\theta$\",\n",
        "#     r\"$\\delta$\",\n",
        "#     r\"$d$\",\n",
        "# ]\n",
        "\n",
        "name = 'radev_model'\n",
        "root_path = pathlib.Path(mnt + f'/MyDrive/bayesian disease modeling/{name}')\n",
        "model_path = root_path / f'model/'\n",
        "model_path.mkdir(exist_ok=True, parents=True)\n",
        "real_data = load_data()"
      ],
      "metadata": {
        "id": "_9Tnvu6yTP7e"
      },
      "id": "_9Tnvu6yTP7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERxTa9qhDPu_"
      },
      "id": "ERxTa9qhDPu_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta1_s = np.array([prior_sir() for _ in range(5000)])\n",
        "theta2_s = np.array([prior_secir() for _ in range(5000)])\n",
        "theta1_mu = np.mean(theta1_s, axis=0, keepdims=True)\n",
        "theta2_mu = np.mean(theta2_s, axis=0, keepdims=True)\n",
        "theta1_std = np.std(theta1_s, axis=0, keepdims=True)\n",
        "theta2_std = np.std(theta2_s, axis=0, keepdims=True)\n",
        "theta_mu = np.c_[theta1_mu, theta2_mu]\n",
        "theta_std = np.c_[theta1_std, theta2_std]"
      ],
      "metadata": {
        "id": "FnLemCGwoc34"
      },
      "id": "FnLemCGwoc34",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generative_model = partial(data_generator, N=real_data[\"N\"], T=real_data[\"T\"])\n",
        "coupling_settings = {\n",
        "    \"dense_args\": dict(units=128, activation=\"swish\", kernel_regularizer=None),\n",
        "    \"num_dense\": 2,\n",
        "    \"dropout\": False,\n",
        "}\n",
        "likelihood_net = InvertibleNetworkWithMemory(\n",
        "    num_params=3, num_coupling_layers=8, coupling_settings=coupling_settings\n",
        ")\n",
        "posterior_net = InvertibleNetwork(\n",
        "    num_params=len(param_names),\n",
        "    num_coupling_layers=6,\n",
        "    coupling_settings=coupling_settings,\n",
        ")\n",
        "summary_net = SummaryNet(n_summary=192)\n",
        "amortized_posterior = AmortizedPosterior(\n",
        "    posterior_net, summary_net, summary_loss_fun=\"MMD\"\n",
        ")\n",
        "amortized_likelihood = AmortizedLikelihood(likelihood_net)\n",
        "joint_amortizer = AmortizedPosteriorLikelihood(\n",
        "    amortized_posterior, amortized_likelihood\n",
        ")\n",
        "model = Trainer(\n",
        "    amortizer=joint_amortizer,\n",
        "    generative_model=generative_model,\n",
        "    configurator=configurator,\n",
        "    checkpoint_path=model_path,\n",
        "    memory=False,\n",
        "    max_to_keep=1,\n",
        ")\n",
        "# # Uncomment for training\n",
        "h = model.train_online(epochs=100, iterations_per_epoch=32*31, batch_size=32, validation_sims=150)"
      ],
      "metadata": {
        "id": "QmLucn3BcfJl"
      },
      "id": "QmLucn3BcfJl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Uncomment for training\n",
        "h = model.train_online(epochs=100, iterations_per_epoch=32*31, batch_size=32, validation_sims=150)"
      ],
      "metadata": {
        "id": "g8qWQnIkVxf2"
      },
      "id": "g8qWQnIkVxf2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1fa3e70c-eae0-4be1-aaa2-9374b741b1a8",
      "metadata": {
        "id": "1fa3e70c-eae0-4be1-aaa2-9374b741b1a8"
      },
      "source": [
        "# Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db483f1b-3cf7-46fc-b8d5-adc566ccf56b",
      "metadata": {
        "id": "db483f1b-3cf7-46fc-b8d5-adc566ccf56b"
      },
      "source": [
        "## Loss Trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae86710-8599-48a3-acfb-2b1d18a0ed54",
      "metadata": {
        "id": "5ae86710-8599-48a3-acfb-2b1d18a0ed54"
      },
      "outputs": [],
      "source": [
        "# Use loaded history, since reference 'h' will only exist after training\n",
        "h = model.loss_history.get_plottable()\n",
        "f = plot_losses(h[\"train_losses\"], h[\"val_losses\"])\n",
        "f.savefig(root_path / \"loss_history.pdf\", dpi=300);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_sims = 3\n",
        "n_samples = 2\n",
        "samples = model.generative_model(n_sims)\n",
        "samples = model.configurator(samples)\n",
        "samples['parameters_out'] = model.amortizer.sample(samples, n_samples=n_samples)\n",
        "samples.keys()"
      ],
      "metadata": {
        "id": "sUCVcMf2kivQ"
      },
      "id": "sUCVcMf2kivQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "965474a0-6ab9-41f9-9e19-b0f4930fb9ea",
      "metadata": {
        "id": "965474a0-6ab9-41f9-9e19-b0f4930fb9ea"
      },
      "source": [
        "# Paper Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5fb221d-1147-4c59-a530-7a6a97d241f5",
      "metadata": {
        "id": "a5fb221d-1147-4c59-a530-7a6a97d241f5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.ticker as ticker\n",
        "\n",
        "\n",
        "def publication_plot(to_plot, sim_out, real_data):\n",
        "    \"\"\"Helper function to generate pretty simulation vs. re-simulation plots.\"\"\"\n",
        "\n",
        "    colors = [\"#000080\", \"#008000\", \"#800000\"]\n",
        "    titles = [\"Infected\", \"Recovered\", \"Dead\"]\n",
        "    f, axarr = plt.subplots(2, 3, figsize=(15, 9))\n",
        "    time = np.arange(1, real_data[\"T\"] + 1)\n",
        "    sur_med = np.median(to_plot, axis=0)\n",
        "    sur_q_95 = np.quantile(to_plot, axis=0, q=(0.025, 0.975))\n",
        "    sur_q_50 = np.quantile(to_plot, axis=0, q=(0.25, 0.75))\n",
        "    sim_med = np.median(sim_out, axis=0)\n",
        "    sim_q_95 = np.quantile(sim_out, axis=0, q=(0.025, 0.975))\n",
        "    sim_q_50 = np.quantile(sim_out, axis=0, q=(0.25, 0.75))\n",
        "\n",
        "    for i, ax in enumerate(axarr.flat[:3]):\n",
        "        # Surrogate outputs\n",
        "        ax.plot(\n",
        "            time, sur_med[:, i], color=colors[i], lw=4, linestyle=\"dotted\", alpha=0.9\n",
        "        )\n",
        "        ax.fill_between(\n",
        "            time, sur_q_50[0, :, i], sur_q_50[1, :, i], color=colors[i], alpha=0.5\n",
        "        )\n",
        "        ax.fill_between(\n",
        "            time, sur_q_95[0, :, i], sur_q_95[1, :, i], color=colors[i], alpha=0.3\n",
        "        )\n",
        "\n",
        "        # General plot settings\n",
        "        sns.despine(ax=ax)\n",
        "        ax.grid(alpha=0.25)\n",
        "        ax.set_ylabel(f\"# {titles[i]}\", fontsize=25)\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=17)\n",
        "        ax.yaxis.set_major_formatter(\n",
        "            ticker.FuncFormatter(lambda x, pos: \"{:,.0f}\".format(x) + \"k\")\n",
        "        )\n",
        "\n",
        "    for i, ax in enumerate(axarr.flat[3:]):\n",
        "        # Surrogate outputs\n",
        "        ax.plot(\n",
        "            time,\n",
        "            sim_med[:, i],\n",
        "            color=colors[i],\n",
        "            lw=4,\n",
        "            linestyle=\"dotted\",\n",
        "            alpha=0.9,\n",
        "            label=\"Median\",\n",
        "        )\n",
        "        ax.fill_between(\n",
        "            time,\n",
        "            sim_q_50[0, :, i],\n",
        "            sim_q_50[1, :, i],\n",
        "            color=colors[i],\n",
        "            alpha=0.5,\n",
        "            label=\"50%-CI\",\n",
        "        )\n",
        "        ax.fill_between(\n",
        "            time,\n",
        "            sim_q_95[0, :, i],\n",
        "            sim_q_95[1, :, i],\n",
        "            color=colors[i],\n",
        "            alpha=0.3,\n",
        "            label=\"95%-CI\",\n",
        "        )\n",
        "\n",
        "        # General plot settings\n",
        "        sns.despine(ax=ax)\n",
        "        ax.grid(alpha=0.25)\n",
        "        ax.set_xlabel(\"Time (days)\", fontsize=25)\n",
        "        ax.set_ylabel(f\"# {titles[i]}\", fontsize=25)\n",
        "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=17)\n",
        "        ax.yaxis.set_major_formatter(\n",
        "            ticker.FuncFormatter(lambda x, pos: \"{:,.0f}\".format(x) + \"k\")\n",
        "        )\n",
        "        # ax.legend(fontsize=15)\n",
        "\n",
        "    axarr[0, 0].text(\n",
        "        -0.42,\n",
        "        0.5,\n",
        "        \"Surrogate\",\n",
        "        horizontalalignment=\"left\",\n",
        "        verticalalignment=\"center\",\n",
        "        rotation=90,\n",
        "        fontsize=30,\n",
        "        transform=axarr[0, 0].transAxes,\n",
        "    )\n",
        "\n",
        "    axarr[1, 0].text(\n",
        "        -0.42,\n",
        "        0.5,\n",
        "        \"Simulator\",\n",
        "        horizontalalignment=\"left\",\n",
        "        verticalalignment=\"center\",\n",
        "        rotation=90,\n",
        "        fontsize=30,\n",
        "        transform=axarr[1, 0].transAxes,\n",
        "    )\n",
        "    f.tight_layout()\n",
        "    return f"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "587b6295-2ad2-49de-9a09-6b13234f6e5c",
      "metadata": {
        "id": "587b6295-2ad2-49de-9a09-6b13234f6e5c"
      },
      "source": [
        "### Teaser Figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0570afc-5031-4596-b0a3-8e535ccca716",
      "metadata": {
        "id": "f0570afc-5031-4596-b0a3-8e535ccca716"
      },
      "outputs": [],
      "source": [
        "# Set number of simulated trajectories\n",
        "n_trajectories = 1000\n",
        "\n",
        "# Simulate data and format for likelihood network\n",
        "\n",
        "# Generate from prior for new figure (Uncomment)\n",
        "out = generative_model(1)\n",
        "pars = np.array([out['prior_draws'][0].astype(np.float32)] * n_trajectories)\n",
        "\n",
        "# Or load for paper figure (Comment out)\n",
        "# Note, that the parameters are already repeated n_trajectories = 1000 times over axis = 0\n",
        "# pars = np.load(\"assets/parameters.npy\")\n",
        "\n",
        "# Generate simulations from the true simulator given parameters\n",
        "sim_out = (\n",
        "    simulate_given_params(n_trajectories, pars[0], real_data[\"N\"], real_data[\"T\"])\n",
        "    / 1000\n",
        ")\n",
        "net_in = configurator({\"sim_data\": sim_out, \"prior_draws\": pars})\n",
        "means_out = np.mean(sim_out, axis=1)\n",
        "stds_out = np.std(sim_out, axis=1)\n",
        "\n",
        "# Generate surrogate simulations given parameters\n",
        "net_out = amortized_likelihood.sample(net_in[\"likelihood_inputs\"], real_data[\"T\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ecf980-bb55-4f6c-b28d-09a22339aee2",
      "metadata": {
        "id": "42ecf980-bb55-4f6c-b28d-09a22339aee2"
      },
      "outputs": [],
      "source": [
        "# Create summary representations\n",
        "rep_sim = summary_net(net_in[\"posterior_inputs\"][\"summary_conditions\"])\n",
        "rep_sur = summary_net(net_out)\n",
        "\n",
        "# Compute individual MMDs\n",
        "total_mmd = maximum_mean_discrepancy(rep_sim, rep_sur)\n",
        "mmds_all = [\n",
        "    maximum_mean_discrepancy(rep_sim, rep_sur[i : (i + 1), :]).numpy()\n",
        "    for i in range(n_trajectories)\n",
        "]\n",
        "\n",
        "# Remove 1% with a highest MMD - posterior net criticizes likelihood net\n",
        "idx_good = np.argsort(mmds_all)[:990]\n",
        "\n",
        "to_plot = (\n",
        "    net_out[idx_good] * stds_out[idx_good, np.newaxis, :]\n",
        "    + means_out[idx_good, np.newaxis, :]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b7668b-f569-4c0f-b010-7c43bdb1d8ac",
      "metadata": {
        "id": "98b7668b-f569-4c0f-b010-7c43bdb1d8ac"
      },
      "outputs": [],
      "source": [
        "f = publication_plot(to_plot, sim_out, real_data)\n",
        "# f.savefig('figures/covid_teaser_nolegend.pdf', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c30f7bd-ca42-4ec2-85a0-a84167b9bd6b",
      "metadata": {
        "id": "2c30f7bd-ca42-4ec2-85a0-a84167b9bd6b"
      },
      "source": [
        "### Appendix Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d971ca66-5403-49f2-a031-be3d514c9831",
      "metadata": {
        "id": "d971ca66-5403-49f2-a031-be3d514c9831"
      },
      "outputs": [],
      "source": [
        "n_sims = 10\n",
        "n_trajectories = 1000\n",
        "\n",
        "for sim in range(n_sims):\n",
        "    # Sample from simulator\n",
        "    out = generative_model(1)\n",
        "    pars = np.array([out[\"prior_draws\"][0].astype(np.float32)] * n_trajectories)\n",
        "\n",
        "    # Generate simulations from the true simulator given parameters\n",
        "    sim_out = (\n",
        "        simulate_given_params(n_trajectories, pars[0], real_data[\"N\"], real_data[\"T\"])\n",
        "        / 1000\n",
        "    )\n",
        "    net_in = configurator({\"sim_data\": sim_out, \"prior_draws\": pars})\n",
        "    means_out = np.mean(sim_out, axis=1)\n",
        "    stds_out = np.std(sim_out, axis=1)\n",
        "\n",
        "    # Generate surrogate simulations given parameters\n",
        "    net_out = amortized_likelihood.sample(net_in[\"likelihood_inputs\"], real_data[\"T\"])\n",
        "\n",
        "    # Create summary representations and compute MMDs\n",
        "    rep_sim = summary_net(net_in[\"posterior_inputs\"][\"summary_conditions\"])\n",
        "    rep_sur = summary_net(net_out)\n",
        "    total_mmd = maximum_mean_discrepancy(rep_sim, rep_sur)\n",
        "    mmds_all = [\n",
        "        maximum_mean_discrepancy(rep_sim, rep_sur[i : (i + 1), :]).numpy()\n",
        "        for i in range(n_trajectories)\n",
        "    ]\n",
        "\n",
        "    # Remove 1% with a highest MMD - posterior net criticizes likelihood net\n",
        "    idx_good = np.argsort(mmds_all)[:990]\n",
        "    to_plot = (\n",
        "        net_out[idx_good] * stds_out[idx_good, np.newaxis, :]\n",
        "        + means_out[idx_good, np.newaxis, :]\n",
        "    )\n",
        "\n",
        "    f = publication_plot(to_plot, sim_out, real_data)\n",
        "    f.savefig(f\"figures/surrogate_{sim}.pdf\", dpi=300, bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0315d80a-8df0-4cbf-8e4a-1a834f8ae6c8",
      "metadata": {
        "id": "0315d80a-8df0-4cbf-8e4a-1a834f8ae6c8"
      },
      "source": [
        "# Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf9007c4-0264-41e1-afb8-0ed5406ac7cc",
      "metadata": {
        "id": "bf9007c4-0264-41e1-afb8-0ed5406ac7cc"
      },
      "outputs": [],
      "source": [
        "n_test_cal = 1000\n",
        "n_posterior_samples = 100\n",
        "gen_out = generative_model(n_test_cal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0a0c39c-cbaa-4d50-8974-41e87168c4ed",
      "metadata": {
        "id": "c0a0c39c-cbaa-4d50-8974-41e87168c4ed"
      },
      "outputs": [],
      "source": [
        "# Configure simulator output\n",
        "conf = configurator(gen_out)\n",
        "\n",
        "# Obtain surrogate time series given prior draws\n",
        "means_out = np.mean(gen_out[\"sim_data\"], axis=1)\n",
        "stds_out = np.std(gen_out[\"sim_data\"], axis=1)\n",
        "x_sim_s_u = joint_amortizer.sample_data(\n",
        "    conf[\"likelihood_inputs\"], n_samples=real_data[\"T\"]\n",
        ")\n",
        "x_sim_s = x_sim_s_u * stds_out[:, np.newaxis, :] + means_out[:, np.newaxis, :]\n",
        "\n",
        "# Configure surrogate outputs\n",
        "conf_s = configurator({\"sim_data\": x_sim_s, \"prior_draws\": gen_out[\"prior_draws\"]})\n",
        "\n",
        "# Sample from approx. posteriors given surrogate simulator outputs\n",
        "post_samples_s = joint_amortizer.sample_parameters(\n",
        "    conf_s[\"posterior_inputs\"], n_samples=n_posterior_samples\n",
        ")\n",
        "\n",
        "# Sample from approx. posteriors given true simulator outputs\n",
        "post_samples_t = joint_amortizer.sample_parameters(\n",
        "    conf[\"posterior_inputs\"], n_samples=n_posterior_samples\n",
        ")\n",
        "\n",
        "# Extract prior samples\n",
        "prior_samples = conf[\"posterior_inputs\"][\"parameters\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbd8ddcb-a1a2-45ab-99fe-5fb087624cfc",
      "metadata": {
        "id": "dbd8ddcb-a1a2-45ab-99fe-5fb087624cfc"
      },
      "source": [
        "## Joint Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79761ec7-8669-40a1-9ad6-2f2e23381f41",
      "metadata": {
        "id": "79761ec7-8669-40a1-9ad6-2f2e23381f41"
      },
      "outputs": [],
      "source": [
        "from assets.custom_plots import plot_sbc_ecdf_appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482e14ef-c132-44a5-b1a4-d06a44ef6103",
      "metadata": {
        "id": "482e14ef-c132-44a5-b1a4-d06a44ef6103"
      },
      "source": [
        "### Posterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b07f0ab-3c5a-4a5c-846a-42a390bd016e",
      "metadata": {
        "id": "5b07f0ab-3c5a-4a5c-846a-42a390bd016e"
      },
      "outputs": [],
      "source": [
        "f = plot_sbc_ecdf_appendix(\n",
        "    post_samples_t,\n",
        "    prior_samples,\n",
        "    param_names=param_names,\n",
        "    difference=True,\n",
        "    rank_ecdf_color=\"#000080\",\n",
        "    label_fontsize=24,\n",
        "    legend_fontsize=24,\n",
        "    title_fontsize=40,\n",
        ")\n",
        "f.savefig(\"figures/sbc_post_ecdf.pdf\", dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cba455f-a50f-4cf0-84c0-595dea799eb7",
      "metadata": {
        "id": "6cba455f-a50f-4cf0-84c0-595dea799eb7"
      },
      "source": [
        "### Joint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "806e5156-3c27-4adb-a7b9-a6d8bd6e6e2f",
      "metadata": {
        "id": "806e5156-3c27-4adb-a7b9-a6d8bd6e6e2f"
      },
      "outputs": [],
      "source": [
        "f = plot_sbc_ecdf_appendix(\n",
        "    post_samples_s,\n",
        "    prior_samples,\n",
        "    param_names=param_names,\n",
        "    difference=True,\n",
        "    rank_ecdf_color=\"#800000\",\n",
        "    label_fontsize=24,\n",
        "    legend_fontsize=24,\n",
        "    title_fontsize=40,\n",
        ")\n",
        "f.savefig(\"figures/sbc_joint_ecdf.pdf\", dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4e24e9-bc79-4e25-a036-5e7f51473cbd",
      "metadata": {
        "id": "fa4e24e9-bc79-4e25-a036-5e7f51473cbd"
      },
      "outputs": [],
      "source": [
        "import os, datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from functools import partial\n",
        "from scipy import stats\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "# Comment out, if you want tensorflow warnings\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, LSTM\n",
        "from bayesflow.networks import InvertibleNetwork\n",
        "from bayesflow.coupling_networks import CouplingLayer\n",
        "from bayesflow.amortizers import (\n",
        "    AmortizedLikelihood,\n",
        "    AmortizedPosterior,\n",
        "    AmortizedPosteriorLikelihood,\n",
        ")\n",
        "from bayesflow.trainers import Trainer\n",
        "from bayesflow import default_settings\n",
        "from bayesflow.helper_functions import build_meta_dict\n",
        "from bayesflow.diagnostics import plot_sbc_ecdf, plot_sbc_histograms, plot_losses\n",
        "from bayesflow.computational_utilities import maximum_mean_discrepancy\n",
        "\n",
        "def load_data():\n",
        "    confirmed_cases_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
        "    recovered_cases_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\"\n",
        "    dead_cases_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\"\n",
        "\n",
        "    confirmed_cases = pd.read_csv(confirmed_cases_url, sep=\",\")\n",
        "    recovered_cases = pd.read_csv(recovered_cases_url, sep=\",\")\n",
        "    dead_cases = pd.read_csv(dead_cases_url, sep=\",\")\n",
        "\n",
        "    date_data_begin = datetime.date(2020, 3, 1)\n",
        "    date_data_end = datetime.date(2020, 5, 21)\n",
        "\n",
        "    format_date = lambda date_py: \"{}/{}/{}\".format(\n",
        "        date_py.month, date_py.day, str(date_py.year)[2:4]\n",
        "    )\n",
        "    date_formatted_begin = format_date(date_data_begin)\n",
        "    date_formatted_end = format_date(date_data_end)\n",
        "\n",
        "    cases_obs = np.array(\n",
        "        confirmed_cases.loc[\n",
        "            confirmed_cases[\"Country/Region\"] == \"Germany\",\n",
        "            date_formatted_begin:date_formatted_end,\n",
        "        ]\n",
        "    )[0]\n",
        "    recovered_obs = np.array(\n",
        "        recovered_cases.loc[\n",
        "            recovered_cases[\"Country/Region\"] == \"Germany\",\n",
        "            date_formatted_begin:date_formatted_end,\n",
        "        ]\n",
        "    )[0]\n",
        "\n",
        "    dead_obs = np.array(\n",
        "        dead_cases.loc[\n",
        "            dead_cases[\"Country/Region\"] == \"Germany\",\n",
        "            date_formatted_begin:date_formatted_end,\n",
        "        ]\n",
        "    )[0]\n",
        "\n",
        "    data_germany = np.stack([cases_obs, recovered_obs, dead_obs]).T\n",
        "    data_germany = np.diff(data_germany, axis=0)\n",
        "    T_germany = data_germany.shape[0]\n",
        "    N_germany = 83e6\n",
        "    mean_g = np.mean(data_germany, axis=0)\n",
        "    std_g = np.std(data_germany, axis=0)\n",
        "    out = dict(x=data_germany, T=T_germany, N=N_germany, Mean=mean_g, Std=std_g)\n",
        "    return out\n",
        "\n",
        "plt.rcParams.update(\n",
        "    {\n",
        "        \"text.usetex\": False,\n",
        "        \"font.family\": \"serif\",\n",
        "        \"text.latex.preamble\": r\"\\usepackage{{amsmath}}\",\n",
        "    }\n",
        ")\n",
        "\n",
        "alpha_f = (0.7**2) * ((1 - 0.7) / (0.17**2) - (1 - 0.7))\n",
        "beta_f = alpha_f * (1 / 0.7 - 1)\n",
        "\n",
        "\n",
        "def prior_sir():\n",
        "    \"\"\"\n",
        "    Implements batch sampling from a stationary prior over the parameters\n",
        "    of the non-stationary SIR model.\n",
        "    \"\"\"\n",
        "\n",
        "    t1 = np.random.normal(loc=8, scale=3)\n",
        "    t2 = np.random.normal(loc=15, scale=1)\n",
        "    t3 = np.random.normal(loc=22, scale=1)\n",
        "    t4 = np.random.normal(loc=66, scale=1)\n",
        "    delta_t1 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
        "    delta_t2 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
        "    delta_t3 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
        "    delta_t4 = np.random.lognormal(mean=np.log(3), sigma=0.3)\n",
        "    lambd0 = np.random.lognormal(mean=np.log(1.2), sigma=0.5)\n",
        "    lambd1 = np.random.lognormal(mean=np.log(0.6), sigma=0.5)\n",
        "    lambd2 = np.random.lognormal(mean=np.log(0.3), sigma=0.5)\n",
        "    lambd3 = np.random.lognormal(mean=np.log(0.1), sigma=0.5)\n",
        "    lambd4 = np.random.lognormal(mean=np.log(0.1), sigma=0.5)\n",
        "    mu = np.random.lognormal(mean=np.log(1 / 8), sigma=0.2)\n",
        "    f_i = np.random.beta(a=alpha_f, b=beta_f)\n",
        "    phi_i = stats.vonmises(kappa=0.01).rvs()\n",
        "    f_r = np.random.beta(a=alpha_f, b=beta_f)\n",
        "    phi_r = stats.vonmises(kappa=0.01).rvs()\n",
        "    f_d = np.random.beta(a=alpha_f, b=beta_f)\n",
        "    phi_d = stats.vonmises(kappa=0.01).rvs()\n",
        "    D_i = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
        "    D_r = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
        "    D_d = np.random.lognormal(mean=np.log(8), sigma=0.2)\n",
        "    E0 = np.random.gamma(shape=2, scale=30)\n",
        "    scale_I = np.random.gamma(shape=1, scale=5)\n",
        "    scale_R = np.random.gamma(shape=1, scale=5)\n",
        "    scale_D = np.random.gamma(shape=1, scale=5)\n",
        "    return [\n",
        "        t1,\n",
        "        t2,\n",
        "        t3,\n",
        "        t4,\n",
        "        delta_t1,\n",
        "        delta_t2,\n",
        "        delta_t3,\n",
        "        delta_t4,\n",
        "        lambd0,\n",
        "        lambd1,\n",
        "        lambd2,\n",
        "        lambd3,\n",
        "        lambd4,\n",
        "        mu,\n",
        "        f_i,\n",
        "        phi_i,\n",
        "        f_r,\n",
        "        phi_r,\n",
        "        f_d,\n",
        "        phi_d,\n",
        "        D_i,\n",
        "        D_r,\n",
        "        D_d,\n",
        "        E0,\n",
        "        scale_I,\n",
        "        scale_R,\n",
        "        scale_D,\n",
        "    ]\n",
        "\n",
        "\n",
        "def prior_secir():\n",
        "    \"\"\"\n",
        "    Implements batch sampling from a stationary prior over the parameters\n",
        "    of the non-stationary SIR model.\n",
        "    \"\"\"\n",
        "\n",
        "    alpha = np.random.uniform(low=0.005, high=0.9)\n",
        "    beta = np.random.lognormal(mean=np.log(0.25), sigma=0.3)\n",
        "    gamma = np.random.lognormal(mean=np.log(1 / 6.5), sigma=0.5)\n",
        "    eta = np.random.lognormal(mean=np.log(1 / 3.2), sigma=0.3)\n",
        "    theta = np.random.uniform(low=1 / 14, high=1 / 3)\n",
        "    delta = np.random.uniform(low=0.01, high=0.3)\n",
        "    d = np.random.uniform(low=1 / 14, high=1 / 3)\n",
        "    return [alpha, beta, gamma, eta, theta, delta, d]\n",
        "\n",
        "\n",
        "def calc_lambda_array(\n",
        "    sim_lag,\n",
        "    lambd0,\n",
        "    lambd1,\n",
        "    lambd2,\n",
        "    lambd3,\n",
        "    lambd4,\n",
        "    t1,\n",
        "    t2,\n",
        "    t3,\n",
        "    t4,\n",
        "    delta_t1,\n",
        "    delta_t2,\n",
        "    delta_t3,\n",
        "    delta_t4,\n",
        "    T,\n",
        "):\n",
        "    \"\"\"Computes the array of time-varying contact rates/transimission probabilities.\"\"\"\n",
        "\n",
        "    # Array of initial lambdas\n",
        "    lambd0_arr = np.array([lambd0] * (t1 + sim_lag))\n",
        "\n",
        "    # Compute lambd1 array\n",
        "    if delta_t1 == 1:\n",
        "        lambd1_arr = np.array([lambd1] * (t2 - t1))\n",
        "    else:\n",
        "        lambd1_arr = np.linspace(lambd0, lambd1, delta_t1)\n",
        "        lambd1_arr = np.append(lambd1_arr, [lambd1] * (t2 - t1 - delta_t1))\n",
        "\n",
        "    # Compute lambd2 array\n",
        "    if delta_t2 == 1:\n",
        "        lambd2_arr = np.array([lambd2] * (t3 - t2))\n",
        "    else:\n",
        "        lambd2_arr = np.linspace(lambd1, lambd2, delta_t2)\n",
        "        lambd2_arr = np.append(lambd2_arr, [lambd2] * (t3 - t2 - delta_t2))\n",
        "\n",
        "    # Compute lambd3 array\n",
        "    if delta_t3 == 1:\n",
        "        lambd3_arr = np.array([lambd3] * (t4 - t3))\n",
        "    else:\n",
        "        lambd3_arr = np.linspace(lambd3, lambd4, delta_t3)\n",
        "        lambd3_arr = np.append(lambd3_arr, [lambd3] * (t4 - t3 - delta_t3))\n",
        "\n",
        "    # Compute lambd4 array\n",
        "    if delta_t4 == 1:\n",
        "        lambd4_arr = np.array([lambd4] * (T - t4))\n",
        "    else:\n",
        "        lambd4_arr = np.linspace(lambd3, lambd4, delta_t4)\n",
        "        lambd4_arr = np.append(lambd4_arr, [lambd4] * (T - t4 - delta_t4))\n",
        "\n",
        "    return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
        "\n",
        "\n",
        "def non_stationary_SEICR(\n",
        "    params_sir, params_secir, N, T, sim_diff=16, observation_model=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs a forward simulation from the stationary SIR model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract parameters\n",
        "    (\n",
        "        t1,\n",
        "        t2,\n",
        "        t3,\n",
        "        t4,\n",
        "        delta_t1,\n",
        "        delta_t2,\n",
        "        delta_t3,\n",
        "        delta_t4,\n",
        "        lambd0,\n",
        "        lambd1,\n",
        "        lambd2,\n",
        "        lambd3,\n",
        "        lambd4,\n",
        "        mu,\n",
        "        f_i,\n",
        "        phi_i,\n",
        "        f_r,\n",
        "        phi_r,\n",
        "        f_d,\n",
        "        phi_d,\n",
        "        delay_i,\n",
        "        delay_r,\n",
        "        delay_d,\n",
        "        E0,\n",
        "        scale_I,\n",
        "        scale_R,\n",
        "        scale_D,\n",
        "    ) = params_sir\n",
        "    alpha, beta, gamma, eta, theta, delta, d = params_secir\n",
        "\n",
        "    # Round integer parameters\n",
        "    t1, t2, t3, t4 = int(round(t1)), int(round(t2)), int(round(t3)), int(round(t4))\n",
        "    delta_t1, delta_t2, delta_t3, delta_t4 = (\n",
        "        int(round(delta_t1)),\n",
        "        int(round(delta_t2)),\n",
        "        int(round(delta_t3)),\n",
        "        int(round(delta_t4)),\n",
        "    )\n",
        "    E0 = max(1, np.round(E0))\n",
        "    delay_i = int(round(delay_i))\n",
        "    delay_r = int(round(delay_r))\n",
        "    delay_d = int(round(delay_d))\n",
        "\n",
        "    # Impose constraints\n",
        "    assert sim_diff > delay_i\n",
        "    assert sim_diff > delay_r\n",
        "    assert sim_diff > delay_d\n",
        "    assert t1 > 0 and t2 > 0 and t3 > 0 and t4 > 0\n",
        "    assert t1 < t2 < t3 < t4\n",
        "    assert delta_t1 > 0 and delta_t2 > 0 and delta_t3 > 0 and delta_t4 > 0\n",
        "    assert (\n",
        "        t2 - t1 >= delta_t1\n",
        "        and t3 - t2 >= delta_t2\n",
        "        and t4 - t3 >= delta_t3\n",
        "        and T - t4 >= delta_t4\n",
        "    )\n",
        "\n",
        "    # Calculate lambda arrays\n",
        "    # Lambda0 is the initial contact rate which will be consecutively\n",
        "    # reduced via the government measures\n",
        "    sim_lag = sim_diff - 1\n",
        "    lambd_arr = calc_lambda_array(\n",
        "        sim_lag,\n",
        "        lambd0,\n",
        "        lambd1,\n",
        "        lambd2,\n",
        "        lambd3,\n",
        "        lambd4,\n",
        "        t1,\n",
        "        t2,\n",
        "        t3,\n",
        "        t4,\n",
        "        delta_t1,\n",
        "        delta_t2,\n",
        "        delta_t3,\n",
        "        delta_t4,\n",
        "        T,\n",
        "    )\n",
        "\n",
        "    # Initial conditions\n",
        "    S, E, C, I, R, D = [N - E0], [E0], [0], [0], [0], [0]\n",
        "\n",
        "    # Containers\n",
        "    I_news = []\n",
        "    R_news = []\n",
        "    D_news = []\n",
        "\n",
        "    # Reported new cases\n",
        "    I_data = np.zeros(T)\n",
        "    R_data = np.zeros(T)\n",
        "    D_data = np.zeros(T)\n",
        "    fs_i = np.zeros(T)\n",
        "    fs_r = np.zeros(T)\n",
        "    fs_d = np.zeros(T)\n",
        "\n",
        "    # Simulate T-1 tiemsteps\n",
        "    for t in range(T + sim_lag):\n",
        "        # Calculate new exposed cases\n",
        "        E_new = lambd_arr[t] * ((C[t] + beta * I[t]) / N) * S[t]\n",
        "\n",
        "        # Remove exposed from susceptible\n",
        "        S_t = S[t] - E_new\n",
        "\n",
        "        # Calculate current exposed by adding new exposed and\n",
        "        # subtracting the exposed becoming carriers.\n",
        "        E_t = E[t] + E_new - gamma * E[t]\n",
        "\n",
        "        # Calculate current carriers by adding the new exposed and subtracting\n",
        "        # those who will develop symptoms and become detected and those who\n",
        "        # will go through the disease asymptomatically.\n",
        "        C_t = C[t] + gamma * E[t] - (1 - alpha) * eta * C[t] - alpha * theta * C[t]\n",
        "\n",
        "        # Calculate current infected by adding the symptomatic carriers and\n",
        "        # subtracting the dead and recovered. The newly infected are just the\n",
        "        # carriers who get detected.\n",
        "        I_t = (\n",
        "            I[t] + (1 - alpha) * eta * C[t] - (1 - delta) * mu * I[t] - delta * d * I[t]\n",
        "        )\n",
        "        I_new = (1 - alpha) * eta * C[t]\n",
        "\n",
        "        # Calculate current recovered by adding the symptomatic and asymptomatic\n",
        "        # recovered. The newly recovered are only the detected recovered\n",
        "        R_t = R[t] + alpha * theta * C[t] + (1 - delta) * mu * I[t]\n",
        "        R_new = (1 - delta) * mu * I[t]\n",
        "\n",
        "        # Calculate the current dead\n",
        "        D_t = D[t] + delta * d * I[t]\n",
        "        D_new = delta * d * I[t]\n",
        "\n",
        "        # Ensure some numerical onstraints\n",
        "        S_t = np.clip(S_t, 0, N)\n",
        "        E_t = np.clip(E_t, 0, N)\n",
        "        C_t = np.clip(C_t, 0, N)\n",
        "        I_t = np.clip(I_t, 0, N)\n",
        "        R_t = np.clip(R_t, 0, N)\n",
        "        D_t = np.clip(D_t, 0, N)\n",
        "\n",
        "        # Keep track of process over time\n",
        "        S.append(S_t)\n",
        "        E.append(E_t)\n",
        "        C.append(C_t)\n",
        "        I.append(I_t)\n",
        "        R.append(R_t)\n",
        "        D.append(D_t)\n",
        "        I_news.append(I_new)\n",
        "        R_news.append(R_new)\n",
        "        D_news.append(D_new)\n",
        "\n",
        "        # From here, start adding new cases with delay D\n",
        "        # Note, we assume the same delay\n",
        "        if t >= sim_lag:\n",
        "            # Compute lags and add to data arrays\n",
        "            fs_i[t - sim_lag] = (1 - f_i) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * phi_i))\n",
        "            )\n",
        "            fs_r[t - sim_lag] = (1 - f_r) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * phi_r))\n",
        "            )\n",
        "            fs_d[t - sim_lag] = (1 - f_d) * (\n",
        "                1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * phi_d))\n",
        "            )\n",
        "            I_data[t - sim_lag] = I_news[t - delay_i]\n",
        "            R_data[t - sim_lag] = R_news[t - delay_r]\n",
        "            D_data[t - sim_lag] = D_news[t - delay_d]\n",
        "\n",
        "    # Compute weekly modulation\n",
        "    I_data = (1 - fs_i) * I_data\n",
        "    R_data = (1 - fs_r) * R_data\n",
        "    D_data = (1 - fs_d) * D_data\n",
        "\n",
        "    # Add noise\n",
        "    I_data = stats.t(df=4, loc=I_data, scale=np.sqrt(I_data) * scale_I).rvs()\n",
        "    R_data = stats.t(df=4, loc=R_data, scale=np.sqrt(R_data) * scale_R).rvs()\n",
        "    D_data = stats.t(df=4, loc=D_data, scale=np.sqrt(D_data) * scale_D).rvs()\n",
        "\n",
        "    if observation_model:\n",
        "        return np.stack((I_data, R_data, D_data)).T\n",
        "    return np.stack((S, E, I, C, R, D)).T\n",
        "\n",
        "\n",
        "def simulate_given_params(n_sim, params, N, T, sim_diff=21, observation_model=True):\n",
        "    \"\"\"Simulated multiple trajectories from the full model given a fixed parameter configuration.\"\"\"\n",
        "\n",
        "    x = []\n",
        "    theta1, theta2 = params[:-7], params[-7:]\n",
        "    for _ in range(n_sim):\n",
        "        x_i = non_stationary_SEICR(\n",
        "            theta1,\n",
        "            theta2,\n",
        "            N=N,\n",
        "            T=T,\n",
        "            sim_diff=sim_diff,\n",
        "            observation_model=observation_model,\n",
        "        )\n",
        "        x.append(x_i)\n",
        "    return np.clip(np.array(x), 0, np.inf)\n",
        "\n",
        "def data_generator(batch_size, T=None, N=None, sim_diff=21, seed=None, scale=1000):\n",
        "    \"\"\"\n",
        "    Runs the forward model 'batch_size' times by first sampling fromt the prior\n",
        "    theta ~ p(theta) and running x ~ p(x|theta).\n",
        "    ----------\n",
        "\n",
        "    Arguments:\n",
        "    batch_size : int -- the number of samples to draw from the prior\n",
        "    ----------\n",
        "\n",
        "    Output:\n",
        "    forward_dict : dict\n",
        "        The expected outputs for a BayesFlow pipeline\n",
        "    \"\"\"\n",
        "\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    # Generate data\n",
        "    # x is a np.ndarray of shape (batch_size, n_obs, x_dim)\n",
        "    x = []\n",
        "    theta = []\n",
        "    for i in range(batch_size):\n",
        "        # Reject meaningless simulaitons\n",
        "        x_i = None\n",
        "        while x_i is None:\n",
        "            try:\n",
        "                theta1 = prior_sir()\n",
        "                theta2 = prior_secir()\n",
        "                x_i = non_stationary_SEICR(theta1, theta2, N, T, sim_diff=sim_diff)\n",
        "            except:\n",
        "                pass\n",
        "        # Simulate SECIR\n",
        "        x.append(x_i)\n",
        "        theta.append(theta1 + theta2)\n",
        "\n",
        "    # Clip negative and normalize\n",
        "    x = np.clip(np.array(x), 0.0, np.inf) / scale\n",
        "    theta = np.array(theta)\n",
        "\n",
        "    forward_dict = {\"prior_draws\": theta, \"sim_data\": x}\n",
        "    return forward_dict\n",
        "\n",
        "np.random.seed(42)\n",
        "theta1_s = np.array([prior_sir() for _ in range(500)])\n",
        "theta2_s = np.array([prior_secir() for _ in range(500)])\n",
        "theta1_mu = np.mean(theta1_s, axis=0, keepdims=True)\n",
        "theta2_mu = np.mean(theta2_s, axis=0, keepdims=True)\n",
        "theta1_std = np.std(theta1_s, axis=0, keepdims=True)\n",
        "theta2_std = np.std(theta2_s, axis=0, keepdims=True)\n",
        "\n",
        "theta_mu = np.c_[theta1_mu, theta2_mu]\n",
        "theta_std = np.c_[theta1_std, theta2_std]\n",
        "\n",
        "class MultiConvLayer(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_filters=32, strides=1):\n",
        "        super(MultiConvLayer, self).__init__()\n",
        "\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(\n",
        "                n_filters // 2,\n",
        "                kernel_size=f,\n",
        "                strides=strides,\n",
        "                padding=\"causal\",\n",
        "                activation=\"relu\",\n",
        "                kernel_initializer=\"glorot_uniform\",\n",
        "            )\n",
        "            for f in range(2, 8)\n",
        "        ]\n",
        "        self.dim_red = tf.keras.layers.Conv1D(\n",
        "            n_filters, 1, 1, activation=\"relu\", kernel_initializer=\"glorot_uniform\"\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = tf.concat([conv(x) for conv in self.convs], axis=-1)\n",
        "        out = self.dim_red(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiConvNet(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_layers=3, n_filters=64, strides=1):\n",
        "        super(MultiConvNet, self).__init__()\n",
        "\n",
        "        self.net = tf.keras.Sequential(\n",
        "            [MultiConvLayer(n_filters, strides) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        self.lstm = LSTM(n_filters)\n",
        "\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = self.net(x)\n",
        "        out = self.lstm(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SummaryNet(tf.keras.Model):\n",
        "    def __init__(self, n_summary):\n",
        "        super(SummaryNet, self).__init__()\n",
        "        self.net_I = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_R = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_D = MultiConvNet(n_filters=n_summary // 3)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        x = tf.split(x, 3, axis=-1)\n",
        "        x_i = self.net_I(x[0])\n",
        "        x_r = self.net_R(x[1])\n",
        "        x_d = self.net_D(x[2])\n",
        "        return tf.concat([x_i, x_r, x_d], axis=-1)\n",
        "\n",
        "\n",
        "class MemoryNetwork(tf.keras.Model):\n",
        "    def __init__(self, meta):\n",
        "        super(MemoryNetwork, self).__init__()\n",
        "\n",
        "        self.gru = GRU(meta[\"n_hidden\"], return_sequences=True, return_state=True)\n",
        "        self.h = meta[\"n_hidden\"]\n",
        "        self.n_params = meta[\"n_params\"]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, target, condition):\n",
        "        \"\"\"Performs a forward pass through the network.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        target    : tf.Tesnor of shape (batch_size, time_stes, dim)\n",
        "            The time-dependent signal to process.\n",
        "        condition : tf.Tensor of shape (batch_size, cond_dim)\n",
        "            The conditional (static) variables, e.g., parameters.\n",
        "        \"\"\"\n",
        "        shift_target = target[:, :-1, :]\n",
        "        init = tf.zeros((target.shape[0], 1, target.shape[2]))\n",
        "        inp_teacher = tf.concat([init, shift_target], axis=1)\n",
        "        inp_teacher_c = tf.concat([inp_teacher, condition], axis=-1)\n",
        "        out, _ = self.gru(inp_teacher_c)\n",
        "        return out\n",
        "\n",
        "    def step_loop(self, target, condition, state):\n",
        "        out, new_state = self.gru(\n",
        "            tf.concat([target, condition], axis=-1), initial_state=state\n",
        "        )\n",
        "        return out, new_state\n",
        "\n",
        "class InvertibleNetworkWithMemory(tf.keras.Model):\n",
        "    \"\"\"Implements a chain of conditional invertible blocks for Bayesian parameter inference.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_params,\n",
        "        num_coupling_layers=4,\n",
        "        coupling_settings=None,\n",
        "        coupling_design=\"affine\",\n",
        "        permutation=\"fixed\",\n",
        "        use_act_norm=True,\n",
        "        act_norm_init=None,\n",
        "        use_soft_flow=False,\n",
        "        soft_flow_bounds=(1e-3, 5e-2),\n",
        "    ):\n",
        "        \"\"\"Initializes a custom invertible network with recurrent memory.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Create settings dict for coupling layer\n",
        "        settings = dict(\n",
        "            latent_dim=num_params,\n",
        "            coupling_settings=coupling_settings,\n",
        "            coupling_design=coupling_design,\n",
        "            permutation=permutation,\n",
        "            use_act_norm=use_act_norm,\n",
        "            act_norm_init=act_norm_init,\n",
        "        )\n",
        "\n",
        "        # Create sequence of coupling layers and store reference to dimensionality\n",
        "        self.coupling_layers = [\n",
        "            CouplingLayer(**settings) for _ in range(num_coupling_layers)\n",
        "        ]\n",
        "\n",
        "        # Store attributes\n",
        "        self.soft_flow = use_soft_flow\n",
        "        self.soft_low = soft_flow_bounds[0]\n",
        "        self.soft_high = soft_flow_bounds[1]\n",
        "        self.use_act_norm = use_act_norm\n",
        "        self.latent_dim = num_params\n",
        "        self.dynamic_summary_net = MemoryNetwork({\"n_hidden\": 256, \"n_params\": 3})\n",
        "        self.latent_dim = num_params\n",
        "\n",
        "    def call(self, targets, condition, inverse=False):\n",
        "        \"\"\"Performs one pass through an invertible chain (either inverse or forward).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        targets   : tf.Tensor\n",
        "            The estimation quantities of interest, shape (batch_size, ...)\n",
        "        condition : tf.Tensor\n",
        "            The conditional data x, shape (batch_size, summary_dim)\n",
        "        inverse   : bool, default: False\n",
        "            Flag indicating whether to run the chain forward or backwards\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (z, log_det_J)  :  tuple(tf.Tensor, tf.Tensor)\n",
        "            If inverse=False: The transformed input and the corresponding Jacobian of the transformation,\n",
        "            v shape: (batch_size, ...), log_det_J shape: (batch_size, ...)\n",
        "\n",
        "        target          :  tf.Tensor\n",
        "            If inverse=True: The transformed out, shape (batch_size, ...)\n",
        "\n",
        "        Important\n",
        "        ---------\n",
        "        If ``inverse=False``, the return is ``(z, log_det_J)``.\\n\n",
        "        If ``inverse=True``, the return is ``target``.\n",
        "        \"\"\"\n",
        "\n",
        "        if inverse:\n",
        "            return self.inverse(targets, condition)\n",
        "        return self.forward(targets, condition)\n",
        "\n",
        "    @tf.function\n",
        "    def forward(self, targets, condition, **kwargs):\n",
        "        \"\"\"Performs a forward pass though the chain.\"\"\"\n",
        "\n",
        "        # Add memory condition\n",
        "        memory = self.dynamic_summary_net(targets, condition)\n",
        "        condition = tf.concat([memory, condition], axis=-1)\n",
        "\n",
        "        z = targets\n",
        "        log_det_Js = []\n",
        "        for layer in self.coupling_layers:\n",
        "            z, log_det_J = layer(z, condition, **kwargs)\n",
        "            log_det_Js.append(log_det_J)\n",
        "        # Sum Jacobian determinants for all layers (coupling blocks) to obtain total Jacobian.\n",
        "        log_det_J = tf.add_n(log_det_Js)\n",
        "        return z, log_det_J\n",
        "\n",
        "    @tf.function\n",
        "    def inverse(self, z, condition, **kwargs):\n",
        "        \"\"\"Performs a reverse pass through the chain.\"\"\"\n",
        "\n",
        "        target = z\n",
        "        T = z.shape[1]\n",
        "        gru_inp = tf.zeros((z.shape[0], 1, z.shape[-1]))\n",
        "        state = tf.zeros((z.shape[0], self.dynamic_summary_net.h))\n",
        "        outs = []\n",
        "        for t in range(T):\n",
        "            # One step condition\n",
        "            memory, state = self.dynamic_summary_net.step_loop(\n",
        "                gru_inp, condition[:, t : t + 1, :], state\n",
        "            )\n",
        "            condition_t = tf.concat([memory, condition[:, t : t + 1, :]], axis=-1)\n",
        "            target_t = target[:, t : t + 1, :]\n",
        "            for layer in reversed(self.coupling_layers):\n",
        "                target_t = layer(target_t, condition_t, inverse=True, **kwargs)\n",
        "            outs.append(target_t)\n",
        "            gru_inp = target_t\n",
        "        return tf.concat(outs, axis=1)\n",
        "\n",
        "def configurator(forward_dict):\n",
        "    \"\"\"Customized preprocessing for the Covid simulator.\"\"\"\n",
        "\n",
        "    out = {\"posterior_inputs\": {}, \"likelihood_inputs\": {}}\n",
        "\n",
        "    # Extract data\n",
        "    x = forward_dict[\"sim_data\"].astype(np.float32)\n",
        "    x_means = np.mean(x, axis=1, keepdims=True)\n",
        "    x_std = np.std(x, axis=1, keepdims=True)\n",
        "    x = (x - x_means) / x_std\n",
        "    log_mu = np.log2(1 + x_means[:, 0, :])\n",
        "    log_std = np.log2(1 + x_std[:, 0, :])\n",
        "\n",
        "    # Extract params\n",
        "    p = forward_dict[\"prior_draws\"].astype(np.float32)\n",
        "    p = (p - theta_mu) / theta_std\n",
        "\n",
        "    # Repeat condition\n",
        "    cond = np.concatenate([p, log_mu, log_std], axis=-1)\n",
        "    cond = np.stack([cond] * x.shape[1], axis=1)\n",
        "\n",
        "    # Likelihood inputs\n",
        "    out[\"likelihood_inputs\"][\"observables\"] = x.astype(np.float32)\n",
        "    out[\"likelihood_inputs\"][\"conditions\"] = np.concatenate([cond], axis=-1).astype(\n",
        "        np.float32\n",
        "    )\n",
        "\n",
        "    # Posterior inputs\n",
        "    out[\"posterior_inputs\"][\"parameters\"] = p\n",
        "    out[\"posterior_inputs\"][\"summary_conditions\"] = out[\"likelihood_inputs\"][\n",
        "        \"observables\"\n",
        "    ]\n",
        "    out[\"posterior_inputs\"][\"direct_conditions\"] = np.concatenate(\n",
        "        [log_mu, log_std], axis=-1\n",
        "    )\n",
        "\n",
        "    return out\n",
        "\n",
        "real_data = load_data()\n",
        "generative_model = partial(data_generator, N=real_data[\"N\"], T=real_data[\"T\"])\n",
        "\n",
        "param_names = [\n",
        "    r\"$t_1$\",\n",
        "    r\"$t_2$\",\n",
        "    r\"$t_3$\",\n",
        "    r\"$t_4$\",\n",
        "    r\"$\\Delta t_1$\",\n",
        "    r\"$\\Delta t_2$\",\n",
        "    r\"$\\Delta t_3$\",\n",
        "    r\"$\\Delta t_4$\",\n",
        "    r\"$\\lambda_0$\",\n",
        "    r\"$\\lambda_1$\",\n",
        "    r\"$\\lambda_2$\",\n",
        "    r\"$\\lambda_3$\",\n",
        "    r\"$\\lambda_4$\",\n",
        "    r\"$\\mu$\",\n",
        "    r\"$A_I$\",\n",
        "    r\"$\\phi_I$\",\n",
        "    r\"$A_R$\",\n",
        "    r\"$\\phi_R$\",\n",
        "    r\"$A_D$\",\n",
        "    r\"$\\phi_D$\",\n",
        "    r\"$L_I$\",\n",
        "    r\"$L_R$\",\n",
        "    r\"$L_D$\",\n",
        "    r\"$E_0$\",\n",
        "    r\"$\\sigma_I$\",\n",
        "    r\"$\\sigma_R$\",\n",
        "    r\"$\\sigma_D$\",\n",
        "    r\"$\\alpha$\",\n",
        "    r\"$\\beta$\",\n",
        "    r\"$\\gamma$\",\n",
        "    r\"$\\eta$\",\n",
        "    r\"$\\theta$\",\n",
        "    r\"$\\delta$\",\n",
        "    r\"$d$\",\n",
        "]\n",
        "\n",
        "coupling_settings = {\n",
        "    \"dense_args\": dict(units=128, activation=\"swish\", kernel_regularizer=None),\n",
        "    \"num_dense\": 2,\n",
        "    \"dropout\": False,\n",
        "}\n",
        "\n",
        "likelihood_net = InvertibleNetworkWithMemory(\n",
        "    num_params=3, num_coupling_layers=8, coupling_settings=coupling_settings\n",
        ")\n",
        "posterior_net = InvertibleNetwork(\n",
        "    num_params=len(param_names),\n",
        "    num_coupling_layers=6,\n",
        "    coupling_settings=coupling_settings,\n",
        ")\n",
        "summary_net = SummaryNet(n_summary=192)\n",
        "amortized_posterior = AmortizedPosterior(\n",
        "    posterior_net, summary_net, summary_loss_fun=\"MMD\"\n",
        ")\n",
        "amortized_likelihood = AmortizedLikelihood(likelihood_net)\n",
        "joint_amortizer = AmortizedPosteriorLikelihood(\n",
        "    amortized_posterior, amortized_likelihood\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    amortizer=joint_amortizer,\n",
        "    generative_model=generative_model,\n",
        "    configurator=configurator,\n",
        "    checkpoint_path=\"content/temp\",\n",
        "    memory=False,\n",
        "    max_to_keep=1,\n",
        ")\n",
        "\n",
        "\n",
        "print('done')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = trainer.train_online(epochs=100, iterations_per_epoch=32, batch_size=32, validation_sims=150)"
      ],
      "metadata": {
        "id": "NKhexGpVD_AX"
      },
      "id": "NKhexGpVD_AX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = generative_model(1)\n",
        "out"
      ],
      "metadata": {
        "id": "3QWbgyxvFvUX"
      },
      "id": "3QWbgyxvFvUX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OsiFrWsZEfwL"
      },
      "id": "OsiFrWsZEfwL",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}