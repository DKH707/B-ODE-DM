{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DKH707/B-ODE-DM/blob/main/EBS/EBS_SIR_10_Epoch_Run_No_Loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this amazing Github repo: https://github.com/stefanradev93/BayesFlow/tree/master/docs/source/tutorial_notebooks"
      ],
      "metadata": {
        "id": "1UdSbVpaBlfm"
      },
      "id": "1UdSbVpaBlfm"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q 'git+https://github.com/drscook/helpers'\n",
        "! git clone https://github.com/stefanradev93/BayesFlow"
      ],
      "metadata": {
        "id": "o7m6wr1b6cG6",
        "outputId": "3a766407-ab22-4519-daef-529643b399af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "o7m6wr1b6cG6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for helpers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'BayesFlow'...\n",
            "remote: Enumerating objects: 5274, done.\u001b[K\n",
            "remote: Counting objects: 100% (5274/5274), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1885/1885), done.\u001b[K\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "classical-alcohol",
      "metadata": {
        "id": "classical-alcohol"
      },
      "outputs": [],
      "source": [
        "%reload_ext autotime\n",
        "import os, sys\n",
        "sys.path.append(os.path.abspath(os.path.join('../../..')))\n",
        "sys.path.append('/content/BayesFlow')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from functools import partial\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "presidential-density",
      "metadata": {
        "id": "presidential-density"
      },
      "outputs": [],
      "source": [
        "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
        "from bayesflow.networks import SequentialNetwork, InvertibleNetwork\n",
        "from bayesflow.amortizers import AmortizedPosterior\n",
        "from bayesflow.trainers import Trainer\n",
        "import bayesflow.diagnostics as diag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "polished-warning",
      "metadata": {
        "id": "polished-warning"
      },
      "source": [
        "<h1>Introduction</h1>\n",
        "<br>\n",
        "In this tutorial, we will illustrate how to perform posterior inference on simple, stationary SIR-like models (complex models will be tackled in a further notebook). SIR-like models comprise suitable illustrative examples, since they generate time-series and their outputs represent the results of solving a system of ordinary differential equations (ODEs).\n",
        "\n",
        "The details for tackling stochastic epidemiological models are described in our corresponding paper, which you can consult for a more formal exposition and a more comprehensive treatment of neural architectures:\n",
        "\n",
        "<em>OutbreakFlow: Model-based Bayesian inference of disease outbreak dynamics with invertible neural networks and its application to the COVID-19 pandemics in Germany</em> https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009472"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "permanent-auditor",
      "metadata": {
        "id": "permanent-auditor"
      },
      "outputs": [],
      "source": [
        "RNG = np.random.default_rng(2022)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "corrected-little",
      "metadata": {
        "id": "corrected-little"
      },
      "source": [
        "## Defining the Generative Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "appropriate-chester",
      "metadata": {
        "id": "appropriate-chester"
      },
      "source": [
        "As described in our [very first notebook](Intro_Amortized_Posterior_Estimation.ipynb), a generative model consists of a prior (encoding suitable parameter ranges) and a simulator (generating data given simulations). Our underlying model distinguishes between susceptible, $S$, infected, $I$, and recovered, $R$, individuals with infection and recovery occurring at a constant transmission rate $\\lambda$ and constant recovery rate $\\mu$, respectively. The model dynamics are governed by the following system of ODEs:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\frac{dS}{dt} &= -\\lambda\\,\\left(\\frac{S\\,I}{N}\\right) \\\\\n",
        "    \\frac{dI}{dt} &= \\lambda\\,\\left(\\frac{S\\,I}{N}\\right) - \\mu\\,I \\\\\n",
        "    \\frac{dR}{dt} &= \\mu\\,I,\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "with $N = S + I + R$ denoting the total population size. For the purpose of forward inference (simulation), we will use a time step of $dt = 1$, corresponding to daily case reports. In addition to the ODE parameters $\\lambda$ and $\\mu$, we consider a reporting delay parameter $L$ and a dispersion parameter $\\psi$, which affect the number of reported infected individuals via a negative binomial disttribution (https://en.wikipedia.org/wiki/Negative_binomial_distribution):\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    I_t^{(obs)} \\sim \\textrm{NegBinomial}(I^{(new)}_{t-L}, \\psi),\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "In this way, we connect the latent disease model to an observation model, which renders the relationship between parameters and data a stochastic one. Note, that the observation model induces a further parameter $\\psi$, responsible for the dispersion of the noise.\n",
        "Finally, we will also treat the number of initially infected individuals, $I_0$ as an unknown parameter (having its own prior distribution)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "killing-feelings",
      "metadata": {
        "id": "killing-feelings"
      },
      "source": [
        "### Prior"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ideal-median",
      "metadata": {
        "id": "ideal-median"
      },
      "source": [
        "We will place the following prior distributions over the five model parameters, summarized in the table below:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\text {Table 1. Description of model parameters and corresponding prior distributions}\\\\\n",
        "&\\begin{array}{lcl}\n",
        "\\hline \\hline \\text { Description} & \\text { Symbol } & \\text { Prior Distribution } \\\\\n",
        "\\hline \\hline \\text{Initial transmission rate} & \\text{$\\lambda$} & \\text{$\\textrm{LogNormal}(\\log(0.4), 0.5)$} \\\\\n",
        "\\text{Recovery rate of infected individuals} & \\text{$\\mu$} & \\text{$\\textrm{LogNormal}(\\log(1/8), 0.2)$} \\\\\n",
        "\\text{Reporting delay (lag)} & \\text{$L$} & \\text{$\\textrm{LogNormal}(\\log(8), 0.2)$} \\\\\n",
        "\\text{Number of initially infected individuals} & \\text{$I_0$} & \\text{$\\textrm{Gamma}(2, 20)$} \\\\\n",
        "\\text{Dispersion of the negative binomial distribution} & \\text{$\\psi$} & \\text{$\\textrm{Exponential}(5)$} \\\\\n",
        "\\hline\n",
        "\\end{array}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "How did we come up with these priors? In this case, we rely on the domain expertise and previous research by https://www.science.org/doi/10.1126/science.abb9789. In addition, the new parameter $\\psi$ follows an exponential distribution, which restricts it to positive numbers. Below is the implementation of these priors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "supported-builder",
      "metadata": {
        "id": "supported-builder"
      },
      "outputs": [],
      "source": [
        "def model_prior():\n",
        "    \"\"\"Generates random draws from the prior.\"\"\"\n",
        "    #beta = RNG.lognormal(mean=-5.9478, sigma=0.0658)\n",
        "    beta = RNG.uniform(0,0.01)\n",
        "    #alpha = RNG.lognormal(mean=-0.5262, sigma=0.1388)\n",
        "    alpha = RNG.uniform(0,1)\n",
        "    #psi = RNG.lognormal(mean=-1.4088, sigma=0.1199)\n",
        "    psi = RNG.uniform(0,0.5)\n",
        "    return np.array([beta, alpha, psi])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interesting-variation",
      "metadata": {
        "id": "interesting-variation"
      },
      "outputs": [],
      "source": [
        "prior = Prior(prior_fun=model_prior, param_names=[r'$\\beta$', r'$\\alpha$', r'$\\psi$'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "retained-namibia",
      "metadata": {
        "id": "retained-namibia"
      },
      "source": [
        "During training, we will also standardize the prior draws, that is, ensure zero means and unit scale. We will do this purely for technical reasons - neural networks like scaled values. In addition, our current prior ranges differ vastly, so each parameter will contribute disproportionately to the loss function.\n",
        "\n",
        "Here, we will use the `estimate_means_and_stds()` method of a `Prior` instance, which will estimate the prior means and standard deviations from random draws. We could have also just taken the analytic means and standard deviations, but these may not be available in all settings (e.g., implicit priors).\n",
        "\n",
        "<strong>Caution:</strong> Make sure you have a seed or you set a seed whenever you are doing a Monte-Carlo estimation, since your results might differ slightly due to the empirical variation of the estimates!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lightweight-thought",
      "metadata": {
        "id": "lightweight-thought"
      },
      "outputs": [],
      "source": [
        "prior_means, prior_stds = prior.estimate_means_and_stds()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sized-tamil",
      "metadata": {
        "id": "sized-tamil"
      },
      "source": [
        "### Simulator (Implicit Likelihood Function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fourth-reconstruction",
      "metadata": {
        "code_folding": [
          2,
          14
        ],
        "id": "fourth-reconstruction"
      },
      "outputs": [],
      "source": [
        "from scipy.integrate import odeint\n",
        "\n",
        "# def convert_params(mu, phi):\n",
        "#     \"\"\" Helper function to convert mean/dispersion parameterization of a negative binomial to N and p, \n",
        "#     as expected by numpy.\n",
        "    \n",
        "#     See https://en.wikipedia.org/wiki/Negative_binomial_distribution#Alternative_formulations\n",
        "#     \"\"\"\n",
        "    \n",
        "#     r = phi\n",
        "#     var = mu + 1 / r * mu ** 2\n",
        "#     p = (var - mu) / var\n",
        "#     return r, 1 - p\n",
        "\n",
        "def stationary_SIR(params, N, T, eps=1e-5,Class=1):\n",
        "    \"\"\"Performs a forward simulation from the stationary SIR model given a random draw from the prior,\n",
        "    \"\"\"\n",
        "    \n",
        "    # Extract parameters and round I0\n",
        "    beta,alpha,psi = params\n",
        "    I0 = 25\n",
        "   #I0 = np.ceil(I0)\n",
        "    \n",
        "    \n",
        "    # Initial conditions\n",
        "    S, I, R = [738], [I0], [0]\n",
        "    \n",
        "    # Reported new cases\n",
        "    C = [I0]\n",
        "    \n",
        "    def diffSIR(y, t, p):\n",
        "        ds = -p[0]*y[0]*y[1]\n",
        "        di = p[0]*y[0]*y[1]-p[1]*y[1]\n",
        "        dr = p[1] * y[1]\n",
        "        return [ds, di, dr]\n",
        "    times = np.arange(1, T+1)\n",
        "    y = odeint(diffSIR, t=times, y0=[*S,*I,*R], args=((beta,alpha, N),), rtol=1e-8)\n",
        "  \n",
        "\n",
        "    # Simulate T-1 timesteps\n",
        "    for t in range(1, T):\n",
        "        # C.append(lambd * (I[-1]*S[-1]/N))\n",
        "        S.append(y[t,0])\n",
        "        I.append(y[t,1])\n",
        "        R.append(y[t,2])\n",
        "        C.append(S[-2]-S[-1])\n",
        "        \n",
        "        # # Calculate new cases\n",
        "        # I_new = lambd * (I[-1]*S[-1]/N)\n",
        "        \n",
        "        # # SIR equations\n",
        "        # S_t = S[-1] - I_new\n",
        "        # I_t = np.clip(I[-1] + I_new - mu*I[-1], 0., N)\n",
        "        # R_t = np.clip(R[-1] + mu*I[-1], 0., N)\n",
        "        \n",
        "        # # Track\n",
        "        # S.append(S_t)\n",
        "        # I.append(I_t)\n",
        "        # R.append(R_t)\n",
        "        # C.append(I_new)\n",
        "    \n",
        "    # reparam = convert_params(np.clip(np.array(C), 0, N) + eps, psi)\n",
        "    # C_obs = RNG.negative_binomial(reparam[0], reparam[1])\n",
        "    S_obs = RNG.lognormal(mean=np.log(y[:,[0]]+eps), sigma=psi)\n",
        "    I_obs = RNG.lognormal(mean=np.log(y[:,[1]]+eps), sigma=psi)\n",
        "    R_obs = RNG.lognormal(mean=np.log(y[:,[2]]+eps), sigma=psi)\n",
        "    # return C_obs[:, np.newaxis],y[:,1]\n",
        "    #C_obs = y[:,1]\n",
        "    if Class == 0:\n",
        "        return S_obs\n",
        "    elif Class == 1:\n",
        "        return I_obs\n",
        "    elif Class == 2: \n",
        "        return R_obs\n",
        "    #return I_obs\n",
        "\n",
        "\n",
        "# stationary_SIR([0.002,1/2.5,25.0],763,12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "characteristic-reasoning",
      "metadata": {
        "id": "characteristic-reasoning"
      },
      "source": [
        "As you can see, in addition to the parameters, our simulator requires two further arguments: the total population size $N$ and the time horizon $T$. These are quantities over which we can amortize (i.e., context variables), but for this example, we will just use the population of Germany and the first two weeks of the pandemics (i.e., $T=14$), in the same vein as https://www.science.org/doi/10.1126/science.abb9789."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rotary-queen",
      "metadata": {
        "id": "rotary-queen"
      },
      "source": [
        "### Loading Real Data\n",
        "\n",
        "We will define a simple helper function to load the actually reported cases for the first 2 weeks in Germany."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adolescent-employee",
      "metadata": {
        "id": "adolescent-employee"
      },
      "outputs": [],
      "source": [
        "# def load_data():\n",
        "#     \"\"\"Helper function to load cumulative cases and transform them to new cases.\"\"\"\n",
        "    \n",
        "#     confirmed_cases_url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
        "#     confirmed_cases = pd.read_csv(confirmed_cases_url, sep=',')\n",
        "\n",
        "#     date_data_begin = datetime.date(2020,3,1)\n",
        "#     date_data_end = datetime.date(2020,3,15)\n",
        "#     format_date = lambda date_py: '{}/{}/{}'.format(date_py.month, date_py.day,\n",
        "#                                                      str(date_py.year)[2:4])\n",
        "#     date_formatted_begin = format_date(date_data_begin)\n",
        "#     date_formatted_end = format_date(date_data_end)\n",
        "\n",
        "#     cases_obs =  np.array(\n",
        "#         confirmed_cases.loc[confirmed_cases[\"Country/Region\"] == \"Germany\", \n",
        "#                             date_formatted_begin:date_formatted_end])[0]\n",
        "#     new_cases_obs = np.diff(cases_obs)\n",
        "#     return new_cases_obs\n",
        "def load_data():\n",
        "    obs_cases = np.array([25, 72, 222, 282, 256, 233, 189, 123, 70, 25, 11, 4])\n",
        "    # new_cases_obs = np.diff(obs_cases)\n",
        "    # return new_cases_obs\n",
        "    return obs_cases\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_data().shape"
      ],
      "metadata": {
        "id": "pAUu0xj4gmkX"
      },
      "id": "pAUu0xj4gmkX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "municipal-viking",
      "metadata": {
        "id": "municipal-viking"
      },
      "source": [
        "We then collect the context and real data into a Python dictionary for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "desperate-baghdad",
      "metadata": {
        "id": "desperate-baghdad"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    # T is for time\n",
        "    'T': 12,\n",
        "    # N is for total population\n",
        "    'N': 763,\n",
        "    'obs_data': load_data()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "infinite-cincinnati",
      "metadata": {
        "id": "infinite-cincinnati"
      },
      "source": [
        "Since we won't vary the context variables during training, we can also define our simulator with fixed keyword arguments with the help of the `partial` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imperial-extreme",
      "metadata": {
        "id": "imperial-extreme"
      },
      "outputs": [],
      "source": [
        "simulator = Simulator(\n",
        "    simulator_fun=partial(stationary_SIR, T=config['T'], N=config['N'])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exempt-repeat",
      "metadata": {
        "id": "exempt-repeat"
      },
      "source": [
        "Thus, whenever we call the `simulator` object, it will always use the keyword arguments provided to the `partial` function. Also, pay attention that we are passing the simulator function as a `simulator_fun` argument. A `Simulator` instance can also be initialized with a `batched_simulator_fun`, which implies that the simulator works on multiple (batched), instead of single, random draws from the prior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "legal-tokyo",
      "metadata": {
        "id": "legal-tokyo"
      },
      "source": [
        "### Generative Model\n",
        "\n",
        "We now connect the prior and the simulator through the `GenerativeModel` wrapper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adequate-fleece",
      "metadata": {
        "id": "adequate-fleece"
      },
      "outputs": [],
      "source": [
        "model = GenerativeModel(prior, simulator, name='basic_EBS_simulator')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chemical-cleaner",
      "metadata": {
        "id": "chemical-cleaner"
      },
      "source": [
        "## Prior Checking\n",
        "\n",
        "Any principled Bayesian workflow requires some prior predictive or prior pushforward checks to ensure that the prior specification is consistent with domain expertise (see https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html). The BayesFlow library provides some rudimentary visual tools for performing prior checking. For instance, we can visually inspect the joint prior in the form of bivariate plots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rental-burner",
      "metadata": {
        "id": "rental-burner"
      },
      "outputs": [],
      "source": [
        "# As per default, the plot_prior2d function will obtain 1000 draws from the joint prior.\n",
        "f = prior.plot_prior2d()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "removed-discrimination",
      "metadata": {
        "id": "removed-discrimination"
      },
      "source": [
        "## Defining the Neural Approximator\n",
        "\n",
        "We can now proceed to define our BayesFlow neural architecture, that is, combine a summary network with an invertible inference network."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "traditional-colors",
      "metadata": {
        "id": "traditional-colors"
      },
      "source": [
        "### Summary Network\n",
        "\n",
        "Since our simulator outputs 3D tensors of shape ``(batch_size, T = 14, 1)``, we need to reduce this three-dimensional tensor into a two-dimensional tensor of shape ``(batch_size, summary_dim)``. Our model outputs are actually so simple that we could have just removed the trailing dimension of the raw outputs and simply fed the data directly to the inference network.\n",
        "\n",
        "However, for the purpose of illustration (and generalization), we will create a more elaborate summary network consisting of 1D convolutional layers (https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/1d-convolution) followed by a Long Short-Term Memory (LSTM) network (https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Such an architecture not only does what we want, but also generalizes to much more complex models and longer time-series of varying time steps, see for instance our ``OutbreakFlow`` architecture:\n",
        "\n",
        "https://arxiv.org/abs/2010.00300\n",
        "\n",
        "Feel free to experiment with different summary architectures as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gross-screen",
      "metadata": {
        "id": "gross-screen"
      },
      "outputs": [],
      "source": [
        "summary_net = SequentialNetwork()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifteen-contract",
      "metadata": {
        "id": "fifteen-contract"
      },
      "source": [
        "### Inference Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decreased-flooring",
      "metadata": {
        "id": "decreased-flooring"
      },
      "outputs": [],
      "source": [
        "inference_net = InvertibleNetwork(num_params=len(prior.param_names), num_coupling_layers=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "loose-surprise",
      "metadata": {
        "id": "loose-surprise"
      },
      "source": [
        "### Amortized Posterior\n",
        "\n",
        "We can now connect the summary and inference networks via the `AmortizedPosterior` wrapper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exempt-details",
      "metadata": {
        "id": "exempt-details"
      },
      "outputs": [],
      "source": [
        "amortizer = AmortizedPosterior(inference_net, summary_net, name='EBS_amortizer')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "incoming-donor",
      "metadata": {
        "id": "incoming-donor"
      },
      "source": [
        "Note, that the `name` keyword argument is optional, but it is good practice to name your models and amortizers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "muslim-nicaragua",
      "metadata": {
        "id": "muslim-nicaragua"
      },
      "source": [
        "## Defining the Configurator\n",
        "\n",
        "As a reminder, a configurator acts as an intermediary between a generative model and an amortizer:\n",
        "\n",
        "<img src=https://github.com/drscook/BayesFlow/blob/master/docs/source/tutorial_notebooks/img/trainer_connection.png?raw=1 width=75%>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "humanitarian-combining",
      "metadata": {
        "id": "humanitarian-combining"
      },
      "source": [
        "In other words, we need to ensure the outputs of the forward model are suitable for processing with neural networks. Currently, they are not, since our data $\\boldsymbol{x}_{1:T}$ consists of large integer (count) values. However, neural networks like scaled data. Furthermore, our parameters $\\boldsymbol{\\theta}$ exhibit widely different scales due to their prior specification and role in the simulator, so we will standardize them using our previously computed prior means and standard deviations. In addition, ODE models are prone to divergences and exploding outputs, which will mess up our training. In sum, our configurator does the following:\n",
        "\n",
        "1. Initializes a new dictionary (line 7).\n",
        "2. Performs a log-transform on the simulated data and convert it to `float32` type (line 10).\n",
        "3. Converts the prior draws to `float32` type and standardizes them (lines 13 - 14).\n",
        "4. Removes potentially problematic simulations from the batch (lines 17 - 19)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "minimal-manner",
      "metadata": {
        "id": "minimal-manner"
      },
      "outputs": [],
      "source": [
        "def configure_input(forward_dict):\n",
        "    \"\"\" Function to configure the simulated quantities (i.e., simulator outputs)\n",
        "    into a neural network-friendly (BayesFlow) format.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Prepare placeholder dict\n",
        "    out_dict = {}\n",
        "    \n",
        "    # Convert data to logscale \n",
        "    logdata = np.log1p(forward_dict['sim_data']).astype(np.float32)\n",
        "    \n",
        "    # Extract prior draws and z-standardize with previously computed means\n",
        "    params = forward_dict['prior_draws'].astype(np.float32)\n",
        "    params = (params - prior_means) / prior_stds\n",
        "    \n",
        "    # Remove a batch if it contains nan, inf or -inf\n",
        "    idx_keep = np.all(np.isfinite(logdata), axis=(1, 2))\n",
        "    # idx_keep = np.all(np.isfinite(logdata), axis=(0, 1)) \n",
        "    if not np.all(idx_keep):\n",
        "        print('Invalid value encountered...removing from batch')\n",
        "    \n",
        "    # Add to keys\n",
        "    out_dict['summary_conditions'] = logdata[idx_keep]\n",
        "    out_dict['parameters'] = params[idx_keep]\n",
        "    \n",
        "    return out_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amortizer"
      ],
      "metadata": {
        "id": "p_5W7bTKhHeW"
      },
      "id": "p_5W7bTKhHeW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "welsh-eclipse",
      "metadata": {
        "id": "welsh-eclipse"
      },
      "source": [
        "## Defining the Trainer\n",
        "\n",
        "Finally, we are in a position to define our `Trainer` instance. Notice that we also pass out custom `configurator` function to the constructer. The default configurator won't do in this case!\n",
        "\n",
        "Note, that you should supply a `checkpoint_path` for the `Trainer` instance, if you don't want to save the neural approximators manually!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "growing-somerset",
      "metadata": {
        "id": "growing-somerset"
      },
      "outputs": [],
      "source": [
        "# change var_obs\n",
        "trainer = Trainer(amortizer=amortizer, \n",
        "                  generative_model=model, \n",
        "                  configurator=configure_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "falling-healthcare",
      "metadata": {
        "id": "falling-healthcare"
      },
      "source": [
        "Great, the trainer informs us that the consistency check (i.e., simulation -> configuration -> transformation -> loss computation) was successful. We can now train our networks on epidemiological simulations. We can also check out the number of trainable neural network parameters for the composite approximator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mysterious-guyana",
      "metadata": {
        "id": "mysterious-guyana"
      },
      "outputs": [],
      "source": [
        "amortizer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "endless-teens",
      "metadata": {
        "id": "endless-teens"
      },
      "source": [
        "## Training Phase\n",
        "\n",
        "Ready to train! Since our simulator is pretty fast, we can safely go with online training. Let's glean the time taken for a batch of $32$ simulations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abroad-market",
      "metadata": {
        "id": "abroad-market"
      },
      "outputs": [],
      "source": [
        "model(32);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "straight-display",
      "metadata": {
        "id": "straight-display"
      },
      "source": [
        "We will train for $10$ epochs using $500$ iterations of $32$ simulations which amounts to a total of $10 \\times 500 \\times 32 = 160000$ simulations performed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "regional-airfare",
      "metadata": {
        "id": "regional-airfare"
      },
      "outputs": [],
      "source": [
        "h = trainer.train_online(epochs=10, iterations_per_epoch=500, batch_size=32, validation_sims=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "practical-cardiff",
      "metadata": {
        "id": "practical-cardiff"
      },
      "source": [
        "### Inspecting the Loss\n",
        "\n",
        "Following our online simulation-based training, we can quickly visualize the loss trajectory using the `plot_losses` function from the `diagnostics` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "peaceful-conviction",
      "metadata": {
        "id": "peaceful-conviction"
      },
      "outputs": [],
      "source": [
        "f = diag.plot_losses(h['train_losses'], h['val_losses'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "optical-advice",
      "metadata": {
        "id": "optical-advice"
      },
      "source": [
        "Great, it seems that our approximator has converged! Before we get too excited and throw our networks at real data, we need to make sure that they meet our expectations <em>in silico</em>, that is, given the small world of simulations the networks have seen."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "parliamentary-indiana",
      "metadata": {
        "id": "parliamentary-indiana"
      },
      "source": [
        "## Validation Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "worse-colon",
      "metadata": {
        "id": "worse-colon"
      },
      "source": [
        "### Inspecting the Latent Space\n",
        "\n",
        "A quick and useful diagnostic is to check whether the marginal latent distribution $p(\\boldsymbol{z})$ has the prescribed probabilistic structure. Since, by default, we optimize the amortizer with the Kullback-Leibler (KL) loss (also known as maximum likelihood training, which is not to be confused with maximum likelihood estimation!), we expect to observe approximately Gaussian latent space with independent axes. Moreover, since the trainer also keeps an internal `SimulationMemory` instance, we can also directly call it's `diagnose_latent2d` method (also available in the `diagnostics` module):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "catholic-depth",
      "metadata": {
        "id": "catholic-depth"
      },
      "outputs": [],
      "source": [
        "f = trainer.diagnose_latent2d()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "binary-florida",
      "metadata": {
        "id": "binary-florida"
      },
      "source": [
        "### Simulation-Based Calibration - Rank Histograms\n",
        "\n",
        "As a further <strong>small world</strong> (i.e., before real data) sanity check, we can also test the calibration of the amortizer through simulation-based calibration (SBC). See the corresponding paper by Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman for more details:\n",
        "\n",
        "https://arxiv.org/pdf/1804.06788.pdf\n",
        "\n",
        "Accordingly, we expect to observe approximately uniform rank statistic histograms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "olive-nickname",
      "metadata": {
        "id": "olive-nickname"
      },
      "outputs": [],
      "source": [
        "f = trainer.diagnose_sbc_histograms()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viral-space",
      "metadata": {
        "id": "viral-space"
      },
      "source": [
        "### Simulation-Based Calibration - Rank ECDF\n",
        "\n",
        "For models with many parameters, inspecting many histograms can become unwieldly. Moreover, the `num_bins` hyperparameter for the construction of SBC rank histograms can be hard to choose. An alternative diagnostic approach for calibration is through empirical cumulative distribution functions (ECDF) of rank statistics. You can read more about this approach in the corresponding paper by Teemu Säilynoja, Paul-Christian Bürkner, and Aki Vehtari:\n",
        "\n",
        "https://arxiv.org/abs/2103.10522\n",
        "\n",
        "In order to inspect the ECDFs of marginal distributions, we will simulate $300$ new pairs of simulated data and generating parameters $(\\boldsymbol{x}, \\boldsymbol{\\theta})$ and use the function `plot_sbc_ecdf` from the `diagnostics` module: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "southwest-housing",
      "metadata": {
        "id": "southwest-housing"
      },
      "outputs": [],
      "source": [
        "# Generate some validation data\n",
        "validation_sims = trainer.configurator(model(batch_size=300))\n",
        "\n",
        "# Generate posterior draws for all simulations\n",
        "post_samples = amortizer.sample(validation_sims, n_samples=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "original-combat",
      "metadata": {
        "id": "original-combat"
      },
      "outputs": [],
      "source": [
        "# Create ECDF plot\n",
        "f = diag.plot_sbc_ecdf(post_samples, validation_sims['parameters'], param_names=prior.param_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lucky-fighter",
      "metadata": {
        "id": "lucky-fighter"
      },
      "source": [
        "We can also produce stacked ECDFs and compute ECDF differences for a more dynamic visualization range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "further-dynamics",
      "metadata": {
        "id": "further-dynamics"
      },
      "outputs": [],
      "source": [
        "# f = diag.plot_sbc_ecdf(post_samples, validation_sims['parameters'], stacked=True, \n",
        "#                        difference=True, legend_fontsize=12, fig_size=(10, 8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "generous-visit",
      "metadata": {
        "id": "generous-visit"
      },
      "source": [
        "Fianlly, we can also compute SBC histograms on the new validation data by calling the function `plot_sbc_histograms` directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "institutional-florist",
      "metadata": {
        "id": "institutional-florist"
      },
      "outputs": [],
      "source": [
        "f = diag.plot_sbc_histograms(post_samples, validation_sims['parameters'], param_names=prior.param_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rural-safety",
      "metadata": {
        "id": "rural-safety"
      },
      "source": [
        "### Inferential Adequacy (Global)\n",
        "\n",
        "Depending on the application, it might be interesting to see how well summaries of the full posterior (e.g., means, medians) recover the assumed true parameter values. We can test this <em>in silico</em> via the `plot_recovery` function in the `diagnostics` module. For instance, we can compare how well posterior means recover the true parameter (i.e., posterior z-score, https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html). Below, we re-use the $300$ simulations we took for computing the rank ECDFs, but obtain a larger number of posterior draws per data set for more stable results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "educational-department",
      "metadata": {
        "id": "educational-department"
      },
      "outputs": [],
      "source": [
        "post_samples = amortizer.sample(validation_sims, n_samples=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "julian-accused",
      "metadata": {
        "id": "julian-accused"
      },
      "outputs": [],
      "source": [
        "f = diag.plot_recovery(post_samples, validation_sims['parameters'], param_names=prior.param_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "occupational-professor",
      "metadata": {
        "id": "occupational-professor"
      },
      "source": [
        "## Inference Phase\n",
        "\n",
        "We can now move on to using real data. As an important general remark: remember that the real and simulated data need to be in the same format (i.e., discrete indicators should be one-hot-encoded, transformations during training should also be applied during inference, etc.)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "earned-trunk",
      "metadata": {
        "id": "earned-trunk"
      },
      "source": [
        "### Bivariate Posteriors\n",
        "\n",
        "Finally, we can feed the real case data from the first two weeks and inspect the approximate posteriors or obtain model-based predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alien-litigation",
      "metadata": {
        "id": "alien-litigation"
      },
      "outputs": [],
      "source": [
        "# Format data into a 3D array of shape (1, n_time_steps, 1) and perform log transform\n",
        "obs_data = np.log1p(config['obs_data'])[np.newaxis, :, np.newaxis].astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "failing-knowing",
      "metadata": {
        "id": "failing-knowing"
      },
      "outputs": [],
      "source": [
        "# Obtain 500 posterior draws given real data\n",
        "post_samples = amortizer.sample({'summary_conditions': obs_data}, 500)\n",
        "\n",
        "# Undo standardization to get parameters on their original (unstandardized) scales\n",
        "post_samples = prior_means + post_samples * prior_stds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "russian-alexander",
      "metadata": {
        "id": "russian-alexander"
      },
      "source": [
        "#### Standalone\n",
        "\n",
        "Using the `plot_posterior_2d` function from the `diagnostics` module, we can look at the bivariate posteriors in isolation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stable-motor",
      "metadata": {
        "id": "stable-motor"
      },
      "outputs": [],
      "source": [
        "f = diag.plot_posterior_2d(post_samples, param_names=prior.param_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forbidden-change",
      "metadata": {
        "id": "forbidden-change"
      },
      "source": [
        "#### Compared to Prior\n",
        "\n",
        "In addition, we can have a more informative plot which indicates the Bayesian surprise (i.e., difference between prior and posterior) by also supplying the prior object to the function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "architectural-steering",
      "metadata": {
        "id": "architectural-steering"
      },
      "outputs": [],
      "source": [
        "f = diag.plot_posterior_2d(post_samples, prior=prior)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_samples"
      ],
      "metadata": {
        "id": "05zl4Gz7rCmz"
      },
      "id": "05zl4Gz7rCmz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "wicked-mouth",
      "metadata": {
        "id": "wicked-mouth"
      },
      "source": [
        "### Posterior Retrodictive Checks\n",
        "\n",
        "These are also called <em>posterior predictive checks</em>, but here we want to explicitly highlight the fact that we are not predicting future data but testing the <strong>generative performance</strong> or <strong>re-simulation performance</strong> of the model. In other words, we want to test how well the simulator can reproduce the actually observed data given the parameter posterior $p(\\boldsymbol{\\theta} | \\boldsymbol{x}_{1:T})$. \n",
        "\n",
        "Here, we will create a custom function which plots the observed data and then overlays draws from the posterior predictive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aggregate-share",
      "metadata": {
        "id": "aggregate-share"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_ppc(config, post_samples, logscale=True, color='Blue', \n",
        "                            dummy=True, figsize=(12, 6), font_size=18):\n",
        "    \"\"\"\n",
        "    Helper function to perform some plotting of the posterior predictive.\n",
        "    \"\"\"\n",
        "    # Plot settings\n",
        "    plt.rcParams['font.size'] = font_size\n",
        "    \n",
        "    # Remove parameters < 0\n",
        "    samples = post_samples[np.sum(post_samples < 0, axis=1) == 0]\n",
        "    \n",
        "    f, ax = plt.subplots(1, 1, figsize=(12,12))\n",
        "    j=0\n",
        "    all=[]\n",
        "    # Re-simulations\n",
        "    sims_S = []\n",
        "    # for i in range(samples.shape[0]):  \n",
        "    #     # Note - simulator returns 2D arrays of shape (T, 1), so we remove trailing dim \n",
        "    #     sim_S_cases = stationary_SIR(samples[i], config['N'], config['T'], Class=0)[:, 0]\n",
        "    #     sims_S.append(sim_S_cases)\n",
        "    # sims_S = np.array(sims_S)\n",
        "\n",
        "    sims=[]\n",
        "    for i in range(samples.shape[0]):  \n",
        "        # Note - simulator returns 2D arrays of shape (T, 1), so we remove trailing dim \n",
        "        sim_cases = stationary_SIR(samples[i], config['N'], config['T'])[:, 0]\n",
        "        sims.append(sim_cases)\n",
        "    sims = np.array(sims)\n",
        "\n",
        "    sims_R = []\n",
        "    # for i in range(samples.shape[0]):  \n",
        "    #     # Note - simulator returns 2D arrays of shape (T, 1), so we remove trailing dim \n",
        "    #     sim_R_cases = stationary_SIR(samples[i], config['N'], config['T'],Class=2)[:, 0]\n",
        "    #     sims_R.append(sim_R_cases)\n",
        "    # sims_R = np.array(sims_R)\n",
        "    \n",
        "    # Compute quantiles for each t = 1,...,T\n",
        "    # qs_50 = np.quantile(sims_S, q=[0.25, 0.75], axis=0)\n",
        "    # qs_90 = np.quantile(sims_S, q=[0.05, 0.95], axis=0)\n",
        "    # qs_95 = np.quantile(sims_S, q=[0.025, 0.975], axis=0)\n",
        "\n",
        "    qi_50 = np.quantile(sims, q=[0.25, 0.75], axis=0)\n",
        "    qi_90 = np.quantile(sims, q=[0.05, 0.95], axis=0)\n",
        "    qi_95 = np.quantile(sims, q=[0.025, 0.975], axis=0)\n",
        "\n",
        "    # qr_50 = np.quantile(sims_R, q=[0.25, 0.75], axis=0)\n",
        "    # qr_90 = np.quantile(sims_R, q=[0.05, 0.95], axis=0)\n",
        "    # qr_95 = np.quantile(sims_R, q=[0.025, 0.975], axis=0)\n",
        "    \n",
        "    # Plot median predictions and observed data\n",
        "    ax.plot(np.median(sims, axis=0), label='Median predicted Infected', color='red')\n",
        "    ax.plot(config['obs_data'], marker='o', label='Reported cases', color='black', linestyle='dashed', alpha=0.8)\n",
        "    #ax.plot(np.median(sims_S, axis=0), label='Median predicted Susceptible', color='blue')\n",
        "    #ax.plot(np.median(sims_R, axis=0), label='Median predicted Recovered', color='green')\n",
        "    \n",
        "    # Add compatibility intervals (also called credible intervals)\n",
        "    # ax.fill_between(range(config['T']), qs_50[0], qs_50[1], color='blue', alpha=0.3)\n",
        "    # ax.fill_between(range(config['T']), qs_90[0], qs_90[1], color='blue', alpha=0.2)\n",
        "    # ax.fill_between(range(config['T']), qs_95[0], qs_95[1], color='blue', alpha=0.1)\n",
        "\n",
        "    ax.fill_between(range(config['T']), qi_50[0], qi_50[1], color='red', alpha=0.3)\n",
        "    ax.fill_between(range(config['T']), qi_90[0], qi_90[1], color='red', alpha=0.2)\n",
        "    ax.fill_between(range(config['T']), qi_95[0], qi_95[1], color='red', alpha=0.1)\n",
        "\n",
        "    # ax.fill_between(range(config['T']), qr_50[0], qr_50[1], color='green', alpha=0.3)\n",
        "    # ax.fill_between(range(config['T']), qr_90[0], qr_90[1], color='green', alpha=0.2)\n",
        "    # ax.fill_between(range(config['T']), qr_95[0], qr_95[1], color='green', alpha=0.1)\n",
        "    \n",
        "    # Grid and schmuck\n",
        "    ax.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.set_xlabel('Days first case')\n",
        "    ax.set_ylabel('Number of cases')\n",
        "    ax.minorticks_off()\n",
        "    if not logscale:\n",
        "        ax.set_yscale('log')\n",
        "    ax.legend(fontsize=font_size)\n",
        "   # print(sims,sims_R,sims_S)\n",
        "    return f"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "romance-doctrine",
      "metadata": {
        "id": "romance-doctrine"
      },
      "source": [
        "We can now go on and plot the re-simulations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "native-suspension",
      "metadata": {
        "id": "native-suspension"
      },
      "outputs": [],
      "source": [
        "f = plot_ppc(config, post_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "african-rolling",
      "metadata": {
        "id": "african-rolling"
      },
      "source": [
        "That's it for this tutorial! You now know how to use the basic building blocks of `BayesFlow` to create amortized neural approximators. :)\n",
        "\n",
        "In the [next tutorial](./PriorSensitivity_Covid19_Initial.ipynb), we will go through a <strong>prior sensitivity analysis</strong> with `BayesFlow`, which is as easy to perform as it is important for ascertaining the robustness of our inferences."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "vscode": {
      "interpreter": {
        "hash": "27a86c9f63fe2f1aa7d9f3c637434a8367b5c148236c1390e91d25c0e560ef1a"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}