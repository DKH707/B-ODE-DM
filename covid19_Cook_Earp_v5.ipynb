{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DKH707/B-ODE-DM/blob/kyle-dev/covid19_Cook_Earp_v5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009472\n",
        "- https://arxiv.org/abs/2302.09125\n",
        "- https://github.com/stefanradev93/BayesFlow\n",
        "- https://github.com/bayesflow-org/JANA-Paper\n",
        "- https://github.com/stefanradev93/AIAgainstCorona/tree/main\n",
        "- https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series"
      ],
      "metadata": {
        "id": "HPH8Lw7RTGYf"
      },
      "id": "HPH8Lw7RTGYf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Packages"
      ],
      "metadata": {
        "id": "aIni5vjeSVf6"
      },
      "id": "aIni5vjeSVf6"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/stefanradev93/BayesFlow\n",
        "! pip install -U ipython-autotime wbgapi #numpy pandas tensorflow\n",
        "! pip install properscoring\n",
        "get_ipython().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "eOu7rYGRcV01"
      },
      "id": "eOu7rYGRcV01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "OWM_367MSTFL"
      },
      "id": "OWM_367MSTFL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Imports & Globals"
      ],
      "metadata": {
        "id": "HjDQ1_RiSZwp"
      },
      "id": "HjDQ1_RiSZwp"
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext autotime\n",
        "import os, warnings, datetime, pathlib, shutil, google.colab, dataclasses, pickle, wbgapi\n",
        "import numpy as np, pandas as pd, tensorflow as tf\n",
        "import properscoring as ps\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "from functools import partial\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, LSTM\n",
        "from bayesflow.networks import InvertibleNetwork, SequentialNetwork\n",
        "from bayesflow.coupling_networks import CouplingLayer\n",
        "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
        "from bayesflow.amortizers import AmortizedLikelihood, AmortizedPosterior, AmortizedPosteriorLikelihood\n",
        "from bayesflow.trainers import Trainer\n",
        "from bayesflow import default_settings\n",
        "from bayesflow.helper_functions import build_meta_dict\n",
        "import bayesflow.diagnostics as diag\n",
        "from bayesflow.computational_utilities import maximum_mean_discrepancy\n",
        "warnings.filterwarnings(\"ignore\", message=\"Could not infer format, so each element will be parsed individually\")\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "plt.rcParams.update({\"text.usetex\": False, \"font.family\": \"serif\", \"text.latex.preamble\": r\"\\usepackage{{amsmath}}\"})\n",
        "mnt = '/content/drive'\n",
        "google.colab.drive.mount(mnt)\n",
        "RNG = np.random.default_rng(42)\n",
        "EPS = 1e-6"
      ],
      "metadata": {
        "id": "tvGovgMBRsYF"
      },
      "id": "tvGovgMBRsYF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bespoke TensorFlow Code from JANA Repo that may not be necessary https://github.com/bayesflow-org/JANA-Paper/blob/main/experiments/epidemiology/covid19_joint_memory_MMD.ipynb"
      ],
      "metadata": {
        "id": "oKWG5ofaSf4K"
      },
      "id": "oKWG5ofaSf4K"
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiConvLayer(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_filters=32, strides=1):\n",
        "        super(MultiConvLayer, self).__init__()\n",
        "\n",
        "        self.convs = [\n",
        "            tf.keras.layers.Conv1D(\n",
        "                n_filters // 2,\n",
        "                kernel_size=f,\n",
        "                strides=strides,\n",
        "                padding=\"causal\",\n",
        "                activation=\"relu\",\n",
        "                kernel_initializer=\"glorot_uniform\",\n",
        "            )\n",
        "            for f in range(2, 8)\n",
        "        ]\n",
        "        self.dim_red = tf.keras.layers.Conv1D(\n",
        "            n_filters, 1, 1, activation=\"relu\", kernel_initializer=\"glorot_uniform\"\n",
        "        )\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = tf.concat([conv(x) for conv in self.convs], axis=-1)\n",
        "        out = self.dim_red(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiConvNet(tf.keras.Model):\n",
        "    \"\"\"Implements an inception-inspired conv layer using different kernel sizes\"\"\"\n",
        "\n",
        "    def __init__(self, n_layers=3, n_filters=64, strides=1):\n",
        "        super(MultiConvNet, self).__init__()\n",
        "\n",
        "        self.net = tf.keras.Sequential(\n",
        "            [MultiConvLayer(n_filters, strides) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        self.lstm = LSTM(n_filters)\n",
        "\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        out = self.net(x)\n",
        "        out = self.lstm(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SummaryNet(tf.keras.Model):\n",
        "    def __init__(self, n_summary):\n",
        "        super(SummaryNet, self).__init__()\n",
        "        self.net_I = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_R = MultiConvNet(n_filters=n_summary // 3)\n",
        "        self.net_D = MultiConvNet(n_filters=n_summary // 3)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, **args):\n",
        "        \"\"\"x is a timeseries of dimensions B timestamps, n_features\"\"\"\n",
        "\n",
        "        x = tf.split(x, 3, axis=-1)\n",
        "        x_i = self.net_I(x[0])\n",
        "        x_r = self.net_R(x[1])\n",
        "        x_d = self.net_D(x[2])\n",
        "        return tf.concat([x_i, x_r, x_d], axis=-1)\n",
        "\n",
        "class MemoryNetwork(tf.keras.Model):\n",
        "    def __init__(self, meta):\n",
        "        super(MemoryNetwork, self).__init__()\n",
        "\n",
        "        self.gru = GRU(meta[\"n_hidden\"], return_sequences=True, return_state=True)\n",
        "        self.h = meta[\"n_hidden\"]\n",
        "        self.n_params = meta[\"n_params\"]\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, target, condition):\n",
        "        \"\"\"Performs a forward pass through the network.\n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        target    : tf.Tesnor of shape (batch_size, time_stes, dim)\n",
        "            The time-dependent signal to process.\n",
        "        condition : tf.Tensor of shape (batch_size, cond_dim)\n",
        "            The conditional (static) variables, e.g., parameters.\n",
        "        \"\"\"\n",
        "        shift_target = target[:, :-1, :]\n",
        "        init = tf.zeros((target.shape[0], 1, target.shape[2]))\n",
        "        inp_teacher = tf.concat([init, shift_target], axis=1)\n",
        "        inp_teacher_c = tf.concat([inp_teacher, condition], axis=-1)\n",
        "        out, _ = self.gru(inp_teacher_c)\n",
        "        return out\n",
        "\n",
        "    def step_loop(self, target, condition, state):\n",
        "        out, new_state = self.gru(\n",
        "            tf.concat([target, condition], axis=-1), initial_state=state\n",
        "        )\n",
        "        return out, new_state\n",
        "\n",
        "class InvertibleNetworkWithMemory(tf.keras.Model):\n",
        "    \"\"\"Implements a chain of conditional invertible blocks for Bayesian parameter inference.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_params,\n",
        "        num_coupling_layers=4,\n",
        "        coupling_settings=None,\n",
        "        coupling_design=\"affine\",\n",
        "        permutation=\"fixed\",\n",
        "        use_act_norm=True,\n",
        "        act_norm_init=None,\n",
        "        use_soft_flow=False,\n",
        "        soft_flow_bounds=(1e-3, 5e-2),\n",
        "    ):\n",
        "        \"\"\"Initializes a custom invertible network with recurrent memory.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Create settings dict for coupling layer\n",
        "        settings = dict(\n",
        "            latent_dim=num_params,\n",
        "            coupling_settings=coupling_settings,\n",
        "            coupling_design=coupling_design,\n",
        "            permutation=permutation,\n",
        "            use_act_norm=use_act_norm,\n",
        "            act_norm_init=act_norm_init,\n",
        "        )\n",
        "\n",
        "        # Create sequence of coupling layers and store reference to dimensionality\n",
        "        self.coupling_layers = [\n",
        "            CouplingLayer(**settings) for _ in range(num_coupling_layers)\n",
        "        ]\n",
        "\n",
        "        # Store attributes\n",
        "        self.soft_flow = use_soft_flow\n",
        "        self.soft_low = soft_flow_bounds[0]\n",
        "        self.soft_high = soft_flow_bounds[1]\n",
        "        self.use_act_norm = use_act_norm\n",
        "        self.latent_dim = num_params\n",
        "        self.dynamic_summary_net = MemoryNetwork({\"n_hidden\": 256, \"n_params\": 3})\n",
        "        self.latent_dim = num_params\n",
        "\n",
        "    def call(self, targets, condition, inverse=False):\n",
        "        \"\"\"Performs one pass through an invertible chain (either inverse or forward).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        targets   : tf.Tensor\n",
        "            The estimation quantities of interest, shape (batch_size, ...)\n",
        "        condition : tf.Tensor\n",
        "            The conditional data x, shape (batch_size, summary_dim)\n",
        "        inverse   : bool, default: False\n",
        "            Flag indicating whether to run the chain forward or backwards\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (z, log_det_J)  :  tuple(tf.Tensor, tf.Tensor)\n",
        "            If inverse=False: The transformed input and the corresponding Jacobian of the transformation,\n",
        "            v shape: (batch_size, ...), log_det_J shape: (batch_size, ...)\n",
        "\n",
        "        target          :  tf.Tensor\n",
        "            If inverse=True: The transformed out, shape (batch_size, ...)\n",
        "\n",
        "        Important\n",
        "        ---------\n",
        "        If ``inverse=False``, the return is ``(z, log_det_J)``.\\n\n",
        "        If ``inverse=True``, the return is ``target``.\n",
        "        \"\"\"\n",
        "\n",
        "        if inverse:\n",
        "            return self.inverse(targets, condition)\n",
        "        return self.forward(targets, condition)\n",
        "\n",
        "    @tf.function\n",
        "    def forward(self, targets, condition, **kwargs):\n",
        "        \"\"\"Performs a forward pass though the chain.\"\"\"\n",
        "\n",
        "        # Add memory condition\n",
        "        memory = self.dynamic_summary_net(targets, condition)\n",
        "        condition = tf.concat([memory, condition], axis=-1)\n",
        "\n",
        "        z = targets\n",
        "        log_det_Js = []\n",
        "        for layer in self.coupling_layers:\n",
        "            z, log_det_J = layer(z, condition, **kwargs)\n",
        "            log_det_Js.append(log_det_J)\n",
        "        # Sum Jacobian determinants for all layers (coupling blocks) to obtain total Jacobian.\n",
        "        log_det_J = tf.add_n(log_det_Js)\n",
        "        return z, log_det_J\n",
        "\n",
        "    @tf.function\n",
        "    def inverse(self, z, condition, **kwargs):\n",
        "        \"\"\"Performs a reverse pass through the chain.\"\"\"\n",
        "\n",
        "        target = z\n",
        "        T = z.shape[1]\n",
        "        gru_inp = tf.zeros((z.shape[0], 1, z.shape[-1]))\n",
        "        state = tf.zeros((z.shape[0], self.dynamic_summary_net.h))\n",
        "        outs = []\n",
        "        for t in range(T):\n",
        "            # One step condition\n",
        "            memory, state = self.dynamic_summary_net.step_loop(\n",
        "                gru_inp, condition[:, t : t + 1, :], state\n",
        "            )\n",
        "            condition_t = tf.concat([memory, condition[:, t : t + 1, :]], axis=-1)\n",
        "            target_t = target[:, t : t + 1, :]\n",
        "            for layer in reversed(self.coupling_layers):\n",
        "                target_t = layer(target_t, condition_t, inverse=True, **kwargs)\n",
        "            outs.append(target_t)\n",
        "            gru_inp = target_t\n",
        "        return tf.concat(outs, axis=1)"
      ],
      "metadata": {
        "id": "wyXYKZI1TBuq"
      },
      "id": "wyXYKZI1TBuq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main COVID class"
      ],
      "metadata": {
        "id": "AAE-IfRJTFe1"
      },
      "id": "AAE-IfRJTFe1"
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass\n",
        "class COVID():\n",
        "    name: str = 'covid_000'\n",
        "    n_steps: int = 100\n",
        "    n_calibrate: int = 5000\n",
        "    refresh: bool = False\n",
        "    pop: int = 86e6\n",
        "\n",
        "\n",
        "    def check_params(self, p):\n",
        "        for key in ['E_0','sim_diff','t_1','t_2','t_3','t_4','t_5','delta_1','delta_2','delta_3','delta_4','lag_I','lag_R','lag_D']:\n",
        "            p[key] = int(round(p[key]))\n",
        "        p['E_0'] = max(p['E_0'], 1)\n",
        "        if all([\n",
        "            all(val > EPS for key, val in p.items() if key[:3] != 'phi'),\n",
        "            p['alpha'] < 1 - EPS,\n",
        "            p['delta'] < 1 - EPS,\n",
        "            p['sim_diff'] > max(p['lag_I'],p['lag_R'],p['lag_D']),\n",
        "            *[p[f't_{i}'] + p[f'delta_{i}'] <= p[f't_{i+1}'] for i in range(1,5)],\n",
        "        ]):\n",
        "            return p\n",
        "\n",
        "\n",
        "    def prior_fun(self):\n",
        "        alpha_f = (0.7**2) * ((1 - 0.7) / (0.17**2) - (1 - 0.7))\n",
        "        beta_f = alpha_f * (1 / 0.7 - 1)\n",
        "        while True:\n",
        "            p = self.check_params({\n",
        "                'N'       :self.pop,\n",
        "                'E_0'     :RNG.gamma(shape=2, scale=30),\n",
        "                'alpha'   :RNG.uniform(low=0.005, high=0.99),\n",
        "                'beta'    :RNG.lognormal(mean=np.log(0.25), sigma=0.3),\n",
        "                'gamma'   :RNG.lognormal(mean=np.log(1/6.5), sigma=0.5),\n",
        "                'delta'   :RNG.uniform(low=0.01, high=0.3),\n",
        "                'epsilon' :RNG.uniform(low=1/14, high=1/3),\n",
        "                'eta'     :RNG.lognormal(mean=np.log(1/3.2), sigma=0.5),\n",
        "                'lambda'  :RNG.lognormal(mean=np.log(1.2), sigma=0.5),\n",
        "                'mu'      :RNG.lognormal(mean=np.log(1/8), sigma=0.2),\n",
        "                'theta'   :RNG.uniform(low=1/14, high=1/3),\n",
        "                'sim_diff':16,\n",
        "                't_1'     :RNG.normal(loc=8, scale=3),\n",
        "                't_2'     :RNG.normal(loc=15, scale=3),\n",
        "                't_3'     :RNG.normal(loc=22, scale=3),\n",
        "                't_4'     :RNG.normal(loc=66, scale=3),\n",
        "                't_5'     :self.n_steps,\n",
        "                'delta_1' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_2' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_3' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'delta_4' :RNG.lognormal(mean=np.log(3), sigma=0.3),\n",
        "                'lambda_0':RNG.lognormal(mean=np.log(1.20), sigma=0.5),\n",
        "                'lambda_1':RNG.lognormal(mean=np.log(0.60), sigma=0.5),\n",
        "                'lambda_2':RNG.lognormal(mean=np.log(0.30), sigma=0.5),\n",
        "                'lambda_3':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "                # 'lambda_4':RNG.lognormal(mean=np.log(0.10), sigma=0.5),\n",
        "                'lambda_4':RNG.lognormal(mean=np.log(0.15), sigma=0.5),\n",
        "                'A_I'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'A_R'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'A_D'     :RNG.beta(a=alpha_f, b=beta_f),\n",
        "                'phi_I'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'phi_R'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'phi_D'   :RNG.vonmises(mu=0, kappa=0.01),\n",
        "                'lag_I'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'lag_R'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'lag_D'   :RNG.lognormal(mean=np.log(8), sigma=0.2),\n",
        "                'sigma_I' :RNG.gamma(shape=1, scale=5),\n",
        "                'sigma_R' :RNG.gamma(shape=1, scale=5),\n",
        "                'sigma_D' :RNG.gamma(shape=1, scale=5),\n",
        "            })\n",
        "            if p:\n",
        "                return p\n",
        "\n",
        "\n",
        "    def prep_params(self, params):\n",
        "        try:\n",
        "            p = self.const_params | params  # if params was passed as dict\n",
        "        except:\n",
        "            p = self.const_params | dict(zip(self.param_names, params))  # if prior_values was passed as list\n",
        "        return self.check_params(p)\n",
        "\n",
        "\n",
        "    # Modified from https://github.com/bayesflow-org/JANA-Paper/blob/main/experiments/epidemiology/covid19_joint_memory_MMD.ipynb\n",
        "    def calc_lambda_array(self, p):\n",
        "        \"\"\"Computes the array of time-varying contact rates/transimission probabilities\"\"\"\n",
        "\n",
        "        # Array of initial lambdas\n",
        "        lambd0_arr = np.array([p['lambda_0']] * (p['t_1'] + p['sim_diff'] - 1))\n",
        "\n",
        "        # Compute lambd1 array\n",
        "        if p['delta_1'] == 1:\n",
        "            lambd1_arr = np.array([p['lambda_1']] * (p['t_2'] - p['t_1']))\n",
        "        else:\n",
        "            lambd1_arr = np.linspace(p['lambda_0'], p['lambda_1'], p['delta_1'])\n",
        "            lambd1_arr = np.append(lambd1_arr, [p['lambda_1']] * (p['t_2'] - p['t_1'] - p['delta_1']))\n",
        "\n",
        "        # Compute lambd2 array\n",
        "        if p['delta_2'] == 1:\n",
        "            lambd2_arr = np.array([p['lambda_2']] * (p['t_3'] - p['t_2']))\n",
        "        else:\n",
        "            lambd2_arr = np.linspace(p['lambda_1'], p['lambda_2'], p['delta_2'])\n",
        "            lambd2_arr = np.append(lambd2_arr, [p['lambda_2']] * (p['t_3'] - p['t_2'] - p['delta_2']))\n",
        "\n",
        "        # Compute lambd3 array\n",
        "        if p['delta_3'] == 1:\n",
        "            lambd3_arr = np.array([p['lambda_3']] * (p['t_4'] - p['t_3']))\n",
        "        else:\n",
        "            lambd3_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_3'])\n",
        "            lambd3_arr = np.append(lambd3_arr, [p['lambda_3']] * (p['t_4'] - p['t_3'] - p['delta_3']))\n",
        "\n",
        "        # Compute lambd4 array\n",
        "        if p['delta_4'] == 1:\n",
        "            lambd4_arr = np.array([p['lambda_4']] * (p['t_5'] - p['t_4']))\n",
        "        else:\n",
        "            lambd4_arr = np.linspace(p['lambda_3'], p['lambda_4'], p['delta_4'])\n",
        "            lambd4_arr = np.append(lambd4_arr, [p['lambda_4']] * (p['t_5'] - p['t_4'] - p['delta_4']))\n",
        "\n",
        "        return np.r_[lambd0_arr, lambd1_arr, lambd2_arr, lambd3_arr, lambd4_arr]\n",
        "\n",
        "\n",
        "    # Modified from https://github.com/bayesflow-org/JANA-Paper/blob/main/experiments/epidemiology/covid19_joint_memory_MMD.ipynb\n",
        "    def sir(self, prior_draw):\n",
        "        p = self.prep_params(prior_draw)\n",
        "        assert p\n",
        "        sim_lag = p['sim_diff'] - 1\n",
        "        lambd_arr = self.calc_lambda_array(p)\n",
        "\n",
        "        # Initial conditions\n",
        "        S, E, C, I, R, D = [p['N'] - p['E_0']], [p['E_0']], [0], [0], [0], [0]\n",
        "\n",
        "        # Containers\n",
        "        #       I   D   R\n",
        "        news = [[], [], []]\n",
        "\n",
        "        # Reported new cases\n",
        "        #                I, D, R\n",
        "        data = np.zeros((3, p['t_5']))\n",
        "        # Weekly modulation functions\n",
        "        #              I, D, R\n",
        "        fs = np.zeros((3, p['t_5']))\n",
        "\n",
        "        # Reported Classes\n",
        "        classes = ['I', 'R', 'D']\n",
        "\n",
        "        # Simulate T-1 tiemsteps\n",
        "        for t in range(p['t_5'] + sim_lag):\n",
        "            # Calculate new exposed cases\n",
        "            E_new = lambd_arr[t] * ((C[t] + p['beta'] * I[t]) / p['N']) * S[t]\n",
        "\n",
        "            # Remove exposed from susceptible\n",
        "            S_t = S[t] - E_new\n",
        "\n",
        "            # Calculate current exposed by adding new exposed and\n",
        "            # subtracting the exposed becoming carriers.\n",
        "            E_t = E[t] + E_new - p['gamma'] * E[t]\n",
        "\n",
        "            # Calculate current carriers by adding the new exposed and subtracting\n",
        "            # those who will develop symptoms and become detected and those who\n",
        "            # will go through the disease asymptomatically.\n",
        "            C_t = C[t] + p['gamma'] * E[t] - (1 - p['alpha']) * p['eta'] * C[t] - p['alpha'] * p['theta'] * C[t]\n",
        "\n",
        "            # Calculate current infected by adding the symptomatic carriers and\n",
        "            # subtracting the dead and recovered. The newly infected are just the\n",
        "            # carriers who get detected.\n",
        "            I_t = I[t] + (1 - p['alpha']) * p['eta'] * C[t] - (1 - p['delta']) * p['mu'] * I[t] - p['delta'] * p['epsilon'] * I[t]\n",
        "            I_new = (1 - p['alpha']) * p['eta'] * C[t]\n",
        "\n",
        "            # Calculate current recovered by adding the symptomatic and asymptomatic\n",
        "            # recovered. The newly recovered are only the detected recovered\n",
        "            R_t = R[t] + p['alpha'] * p['theta'] * C[t] + (1 - p['delta']) * p['mu'] * I[t]\n",
        "            R_new = (1 - p['delta']) * p['mu'] * I[t]\n",
        "\n",
        "            # Calculate the current dead\n",
        "            D_t = D[t] + p['delta'] * p['epsilon'] * I[t]\n",
        "            D_new = p['delta'] * p['epsilon'] * I[t]\n",
        "\n",
        "            # Ensure some numerical onstraints\n",
        "            S_t = np.clip(S_t, 0, p['N'])\n",
        "            E_t = np.clip(E_t, 0, p['N'])\n",
        "            C_t = np.clip(C_t, 0, p['N'])\n",
        "            I_t = np.clip(I_t, 0, p['N'])\n",
        "            R_t = np.clip(R_t, 0, p['N'])\n",
        "            D_t = np.clip(D_t, 0, p['N'])\n",
        "\n",
        "            # Keep track of process over time\n",
        "            S.append(S_t)\n",
        "            E.append(E_t)\n",
        "            C.append(C_t)\n",
        "            I.append(I_t)\n",
        "            R.append(R_t)\n",
        "            D.append(D_t)\n",
        "            news[0].append(I_new)\n",
        "            news[1].append(R_new)\n",
        "            news[2].append(D_new)\n",
        "\n",
        "            # From here, start adding new cases with delay D\n",
        "            # Note, we assume the same delay\n",
        "            if t >= sim_lag:\n",
        "                # Compute lags and add to data arrays\n",
        "                for i in range(len(classes)):\n",
        "                    fs[i][t - sim_lag] = (1 - p[f'A_{classes[i]}']) * (\n",
        "                        1 - np.abs(np.sin((np.pi / 7) * (t - sim_lag) - 0.5 * p[f'phi_{classes[i]}']))\n",
        "                    )\n",
        "                    data[i][t - sim_lag] = news[i][t - p[f'lag_{classes[i]}']]\n",
        "\n",
        "        for i in range(len(classes)):\n",
        "            # Compute weekly modulation\n",
        "            data[i] = (1 - fs[i]) * data[i]\n",
        "            # Add noise\n",
        "            data[i] = data[i] + RNG.standard_t(4) * np.sqrt(data[i]) * p[f'sigma_{classes[i]}']\n",
        "\n",
        "        n = data[0].shape[0]\n",
        "        return (\n",
        "            pd.DataFrame({'S':S[-n:],'E':E[-n:],'C':C[-n:],'I':I[-n:],'R':R[-n:],'D':D[-n:],'dI_obs':data[0],'dR_obs':data[1],'dD_obs':data[2]})\n",
        "            .clip(0, p['N']).rename_axis('t'))\n",
        "\n",
        "\n",
        "    def read_or_create(self, file, fun=None, refresh=False):\n",
        "        \"\"\"Read or create pickle for simulation results\"\"\"\n",
        "        try:\n",
        "            if refresh:\n",
        "                file.unlink(missing_ok=True)\n",
        "            with open(file, \"rb\") as f:\n",
        "                sims = pickle.load(f)\n",
        "            print(f'{file} successfully read')\n",
        "        except Exception as e:\n",
        "            print(f'Running sims: {e}')\n",
        "            with open(file, \"wb\") as f:\n",
        "                sims = fun()\n",
        "                pickle.dump(sims, f)\n",
        "        return sims\n",
        "\n",
        "\n",
        "    def calibrate(self):\n",
        "        \"\"\"Computes statistics about params and data for transformations in pre & post amoritzers\"\"\"\n",
        "        c = self.generator(self.n_calibrate)\n",
        "        d = {'prior':c['prior_draws'], 'data':c['sim_data']}\n",
        "        funs = [np.mean, np.std, np.min, np.max]\n",
        "        c |= {f'{key}_{f.__name__}': f(val, axis=0) for f in funs for key, val in d.items()}\n",
        "        c |= {f'obs_{f.__name__}': c[f'data_{f.__name__}'][...,-self.n_obs:] for f in funs}\n",
        "        return c\n",
        "\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.root_path = pathlib.Path(mnt + f'/MyDrive/checkpoints/{self.name}')\n",
        "        if self.refresh:\n",
        "            # delete root_path and everything in it\n",
        "            shutil.rmtree(self.root_path, ignore_errors=True)\n",
        "        self.model_path = self.root_path / f'model/'\n",
        "        self.model_path.mkdir(exist_ok=True, parents=True)\n",
        "        self.calibration_file = self.model_path / 'calibration.pkl'\n",
        "        self.diagnostic_file  = self.model_path / 'diagnostic.pkl'\n",
        "\n",
        "        prior_samples = [self.prior_fun() for _ in range(100)]\n",
        "        # Detect constant parameters amd remove from updating process\n",
        "        self.const_params = pd.DataFrame(prior_samples).agg(['mean','std']).T.query('std < 1e-5')['mean'].to_dict()\n",
        "        self.param_names = [key for key in prior_samples[0].keys() if key not in self.const_params]\n",
        "        self.param_latex = [f'${key}$' if key in ['N','E_0','sim_diff','t_1','t_2','t_3','t_4','A_I','A_R','A_D','lag_I','lag_R','lag_D'] else f'$\\{key}$' for key in self.param_names]\n",
        "        self.n_params = len(self.param_names)\n",
        "        self.obs_classes = self.sir(prior_samples[0]).filter(like='obs').columns.tolist()\n",
        "        self.n_obs = len(self.obs_classes)\n",
        "\n",
        "        # Create prior, simulator, and generator objects\n",
        "        self.prior = Prior(prior_fun=lambda: [val for key, val in self.prior_fun().items() if key in self.param_names], param_names=self.param_names)\n",
        "        self.simulator = Simulator(simulator_fun=self.sir)\n",
        "        self.generator = GenerativeModel(self.prior, self.simulator)\n",
        "\n",
        "        self.calibration = self.read_or_create(file=self.calibration_file, fun=self.calibrate)\n",
        "\n",
        "        # Complex amortizer from https://github.com/bayesflow-org/JANA-Paper/blob/main/experiments/epidemiology/covid19_joint_memory_MMD.ipynb\n",
        "        # This uses the bespoke tensorflow code from cell above\n",
        "        coupling_settings = {\n",
        "            \"dense_args\": dict(units=128, activation=\"swish\", kernel_regularizer=None),\n",
        "            \"num_dense\": 2,\n",
        "            \"dropout\": False,\n",
        "        }\n",
        "        summary_net = SummaryNet(n_summary=192)\n",
        "        inference_net = InvertibleNetwork(\n",
        "            num_params=len(self.param_names),\n",
        "            num_coupling_layers=6,\n",
        "            coupling_settings=coupling_settings,\n",
        "        )\n",
        "\n",
        "        # simplified amortizer from https://github.com/stefanradev93/BayesFlow/blob/master/examples/Covid19_Initial_Posterior_Estimation.ipynb\n",
        "        # summary_net = SequentialNetwork()\n",
        "        # inference_net = InvertibleNetwork(num_params=len(self.param_names), num_coupling_layers=3)\n",
        "\n",
        "        self.model = Trainer(\n",
        "            generative_model = self.generator,\n",
        "            configurator = self.pre_amortizer,\n",
        "            amortizer = AmortizedPosterior(summary_net=summary_net, inference_net=inference_net),\n",
        "            checkpoint_path = self.model_path, max_to_keep = 3,\n",
        "            skip_checks = True,\n",
        "        )\n",
        "\n",
        "\n",
        "    def pre_amortizer(self, dct):\n",
        "        if 'sim_data' in dct:\n",
        "            data  = np.array(dct['sim_data'])[...,-self.n_obs:]\n",
        "            prior = np.array(dct['prior_draws'])\n",
        "        elif 'real_data' in dct:\n",
        "            data  = np.array(dct['real_data'])\n",
        "            prior = np.full([1,self.n_params], np.nan)\n",
        "        else:\n",
        "            raise Exception\n",
        "        dct['summary_conditions'] = np.float32((data - self.calibration['obs_mean']) / self.calibration['obs_std'])\n",
        "        dct['parameters'] = np.float32((prior - self.calibration['prior_mean']) / self.calibration['prior_std'])\n",
        "        return dct\n",
        "\n",
        "\n",
        "    def post_amortizer(self, dct):\n",
        "        for _ in range(3-dct['parameters_out'].ndim):\n",
        "            dct['parameters_out'] = dct['parameters_out'][np.newaxis]\n",
        "        dct['posterior_draws'] = dct['parameters_out'] * self.calibration['prior_std'] + self.calibration['prior_mean']\n",
        "        return dct\n",
        "\n",
        "\n",
        "    def draw_samples(self, n_posteriors, n_priors=None, real_data=None, ensemble=True):\n",
        "        if real_data is not None:\n",
        "            samples = {'real_data':[real_data]}\n",
        "        elif n_priors is not None:\n",
        "            samples = self.model.generative_model(n_priors)\n",
        "        else:\n",
        "            raise Exception('Must specify n_priors or real_data')\n",
        "        samples = self.model.configurator(samples)\n",
        "        samples['parameters_out'] = self.model.amortizer.sample(samples, n_posteriors)\n",
        "        samples = self.post_amortizer(samples)\n",
        "        if ensemble:\n",
        "            samples['valid_draws'] = [[p for p in d if self.prep_params(p) is not None] for d in samples['posterior_draws']]\n",
        "            samples['posterior'] = pd.concat([pd.concat([pd.DataFrame([p], columns=self.param_names).assign(prior_idx=i, posterior_idx=j).set_index(['prior_idx','posterior_idx']) for j, p in enumerate(D)]) for i, D in enumerate(samples['valid_draws'])])\n",
        "            samples['ensemble'] = pd.concat([self.sir(p).reset_index().assign(prior_idx=i, posterior_idx=j).set_index(['prior_idx','posterior_idx','t']) for (i,j),p in samples['posterior'].iterrows()])\n",
        "            if real_data is not None:\n",
        "                R = real_data.reset_index().rename_axis('t')\n",
        "                samples['ensemble'] = samples['ensemble'].join(R).set_index('date',append=True)\n",
        "        return samples\n",
        "\n",
        "\n",
        "    def plot(self, kind='loss', samples=None):\n",
        "        if kind == 'loss':\n",
        "            fig = diag.plot_losses(**self.model.loss_history.get_plottable())\n",
        "        else:\n",
        "            opts = {'param_names':self.param_latex, 'post_samples':samples['parameters_out'], 'prior_samples':samples['parameters']}\n",
        "            if kind == 'ecdf':\n",
        "                fig = diag.plot_sbc_ecdf(**opts)\n",
        "            elif kind == 'hist':\n",
        "                fig = diag.plot_sbc_histograms(**opts)\n",
        "            elif kind == 'recovery':\n",
        "                fig = diag.plot_recovery(**opts)\n",
        "            else:\n",
        "                raise Exception(f'Unrecognized kind \"{kind}\"')\n",
        "        fig.savefig(self.model_path / f'{kind}.png')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def get_diagnostics(self, n_posteriors=50, refresh=False):\n",
        "        self.diagnostic = self.read_or_create(\n",
        "            file=self.diagnostic_file, refresh=refresh,\n",
        "            fun=lambda: self.draw_samples(n_posteriors=n_posteriors, n_priors=21*n_posteriors, ensemble=False))\n",
        "        self.plot('loss')\n",
        "        self.plot('ecdf', self.diagnostic)\n",
        "        self.plot('hist', self.diagnostic)\n",
        "        self.plot('recovery', self.diagnostic)\n",
        "\n",
        "\n",
        "    def fetch_data(self, cntry, start=39):\n",
        "        iso = wbgapi.economy.coder(cntry)\n",
        "        assert iso, f'Unrecognized country {cntry}'\n",
        "        tot_pop = next(wbgapi.data.fetch('SP.POP.TOTL', economy=iso, time=2020))['value']\n",
        "\n",
        "        url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_'\n",
        "        fetch = lambda cl: pd.read_csv(url+cl+'_global.csv', sep=\",\").drop(columns=['Province/State','Lat','Long']).groupby('Country/Region').sum().loc[cntry]\n",
        "        real_data = (\n",
        "            pd.DataFrame({'dI_real':fetch('confirmed'), 'dR_real':fetch('recovered'), 'dD_real':fetch('deaths')})\n",
        "            .assign(date = lambda x: pd.to_datetime(x.index)).set_index('date')\n",
        "            .diff().dropna().clip(0, tot_pop).astype(int).iloc[start:start+self.n_steps])\n",
        "        return dict(cntry=cntry, iso=iso, tot_pop=tot_pop, real_data=real_data)\n",
        "\n",
        "\n",
        "    def get_predictive(self, cntry, n_posteriors=500, refresh=False):\n",
        "        cntry_path = self.root_path / cntry\n",
        "        cntry_path.mkdir(exist_ok=True, parents=True)\n",
        "        predictive_file = cntry_path / 'predictive.pkl'\n",
        "        dct = self.fetch_data(cntry)\n",
        "        predictive = self.read_or_create(\n",
        "            file=predictive_file, refresh=refresh,\n",
        "            fun=lambda: self.draw_samples(n_posteriors=n_posteriors, real_data=dct['real_data'], ensemble=True))\n",
        "\n",
        "        fig, ax = plt.subplots(self.n_obs, 1, sharex=True, figsize=(20,10))\n",
        "        fig.suptitle(cntry)\n",
        "        E = predictive['ensemble'].groupby('date')\n",
        "        for i, obs in enumerate(self.obs_classes):\n",
        "            real = obs.replace('obs','real')\n",
        "            ax[i].plot(E[real].median(), 'k.', label='Real')\n",
        "            for ci, clr in {90:'green', 50:'red', 10:'blue'}.items():\n",
        "                x = (100 - ci) / 200\n",
        "                lb = E[obs].quantile(x)\n",
        "                ub = E[obs].quantile(1-x)\n",
        "                ax[i].fill_between(x=lb.index, y1=lb, y2=ub, color=clr, alpha=0.3, label=f'{ci}% Prediction Interval')\n",
        "            ax[i].legend()\n",
        "            ax[i].set_title(obs[:2])\n",
        "        fig.savefig(cntry_path / f'predictive.png')\n",
        "        plt.show()\n",
        "\n",
        "        plot_params = [\n",
        "            'alpha','beta','gamma','delta','epsilon','eta','mu','theta',\n",
        "            'lambda_1','lambda_2','lambda_3','lambda_4','t_1','t_2','t_3','t_4',]\n",
        "        tx = [f'${key}$' if key in ['N','E_0','sim_diff','t_1','t_2','t_3','t_4','A_I','A_R','A_D','lag_I','lag_R','lag_D'] else f'$\\{key}$' for key in plot_params]\n",
        "        pst = predictive['posterior'].assign(kind='posterior')\n",
        "        pir = pd.DataFrame(self.prior(pst.shape[0])['prior_draws'], columns=self.param_names).assign(kind='prior')\n",
        "        Q = pd.concat([pir,pst]).set_index('kind')[plot_params]\n",
        "        Q.columns = tx\n",
        "        M = Q.melt(ignore_index=False).reset_index()\n",
        "        sns.set_palette(\"Set2\")\n",
        "        fig = sns.FacetGrid(M, hue='kind', col='variable', col_wrap=4, sharex=False, sharey=False)\n",
        "        fig.map(sns.histplot, 'value', kde=True, element='step', alpha=0.7)\n",
        "        fig.add_legend()\n",
        "        fig.fig.suptitle(cntry, y=1.05)\n",
        "        fig.savefig(cntry_path / f'distributions.png')\n",
        "        plt.show()\n",
        "        return predictive"
      ],
      "metadata": {
        "id": "qoBCndZaQ_UO"
      },
      "id": "qoBCndZaQ_UO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cov = COVID(name='earp_model_00_large_pop',\n",
        "            n_steps=100,\n",
        "            # refresh=True\n",
        "            )"
      ],
      "metadata": {
        "id": "WdrS7vfrQfz1"
      },
      "id": "WdrS7vfrQfz1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = cov.model.train_online(epochs=1000, iterations_per_epoch=32*10, batch_size=32, validation_sims=500)"
      ],
      "metadata": {
        "id": "6SMT90fyQUvX"
      },
      "id": "6SMT90fyQUvX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cov.get_diagnostics()#, refresh=True)"
      ],
      "metadata": {
        "id": "PQZ-Bs-GQKFt"
      },
      "id": "PQZ-Bs-GQKFt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict for 1 country\n",
        "cntry = 'Germany'\n",
        "pred = cov.get_predictive(cntry=cntry)#, refresh=True)\n",
        "for cl in ['I', 'R', 'D']:\n",
        "    sims = np.zeros((432, 100))\n",
        "    for i in range(432):\n",
        "        for j in range(100):\n",
        "            sims[i][j] = pred['ensemble'][f'd{cl}_obs'].values[(i+j)]\n",
        "\n",
        "    print(f'd{cl}: ' + str(ps.crps_ensemble(pred['ensemble'][f'd{cl}_real'].values[0:100], np.median(sims, axis=0)).mean()))"
      ],
      "metadata": {
        "id": "UW9FGdGvslln"
      },
      "id": "UW9FGdGvslln",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict for many countries\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv', sep=\",\")\n",
        "pred = dict()\n",
        "for cntry in ['Germany','Canada','Italy','France','US','Israel','China','United Kingdom']: # Your desired list of countries\n",
        "# for cntry in df['Country/Region'].sample(frac=1).unique(): # all countried in the Johns Hospkins data\n",
        "    try:\n",
        "        pred[cntry] = cov.get_predictive(cntry=cntry)#, refresh=True)\n",
        "        for cl in ['I', 'R', 'D']:\n",
        "            sims = np.zeros((432, 100))\n",
        "            for i in range(432):\n",
        "                for j in range(100):\n",
        "                  sims[i][j] = pred[cntry]['ensemble'][f'd{cl}_obs'].values[(i+j)]\n",
        "\n",
        "            print(cntry + f' d{cl}: ' + str(ps.crps_ensemble(pred[cntry]['ensemble'][f'd{cl}_real'].values[0:100], np.median(sims, axis=0)).mean()))\n",
        "    except Exception as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "M8NMOc29P-h_"
      },
      "id": "M8NMOc29P-h_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-11.m108",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m108"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "OWM_367MSTFL"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}