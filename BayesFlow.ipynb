{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f13198-94c5-4e09-810e-d037ec5e18d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robust robust\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|█| 16/16 [00:12<00:00,  1.25it/s, Epoch: 1, Batch: 16,Loss: 6.996,W.Decay: 0.347,Avg.Loss: 7.93\n",
      "Training epoch 2: 100%|█| 16/16 [00:02<00:00,  7.43it/s, Epoch: 2, Batch: 16,Loss: 6.438,W.Decay: 0.335,Avg.Loss: 6.72\n",
      "Training epoch 3: 100%|█| 16/16 [00:02<00:00,  7.34it/s, Epoch: 3, Batch: 16,Loss: 5.877,W.Decay: 0.323,Avg.Loss: 6.16\n",
      "Training epoch 4: 100%|█| 16/16 [00:02<00:00,  7.33it/s, Epoch: 4, Batch: 16,Loss: 5.359,W.Decay: 0.315,Avg.Loss: 5.56\n",
      "Training epoch 5: 100%|█| 16/16 [00:02<00:00,  7.32it/s, Epoch: 5, Batch: 16,Loss: 5.030,W.Decay: 0.312,Avg.Loss: 5.18\n",
      "Training epoch 6: 100%|█| 16/16 [00:02<00:00,  7.20it/s, Epoch: 6, Batch: 16,Loss: 4.893,W.Decay: 0.310,Avg.Loss: 5.02\n",
      "Training epoch 7: 100%|█| 16/16 [00:02<00:00,  7.34it/s, Epoch: 7, Batch: 16,Loss: 4.717,W.Decay: 0.309,Avg.Loss: 4.81\n",
      "Training epoch 8: 100%|█| 16/16 [00:02<00:00,  7.33it/s, Epoch: 8, Batch: 16,Loss: 4.675,W.Decay: 0.309,Avg.Loss: 4.72\n",
      "Training epoch 9: 100%|█| 16/16 [00:02<00:00,  7.27it/s, Epoch: 9, Batch: 16,Loss: 4.571,W.Decay: 0.308,Avg.Loss: 4.71\n",
      "Training epoch 10: 100%|█| 16/16 [00:02<00:00,  7.28it/s, Epoch: 10, Batch: 16,Loss: 4.688,W.Decay: 0.308,Avg.Loss: 4.\n",
      "Training epoch 11: 100%|█| 16/16 [00:02<00:00,  7.23it/s, Epoch: 11, Batch: 16,Loss: 4.527,W.Decay: 0.307,Avg.Loss: 4.\n",
      "Training epoch 12: 100%|█| 16/16 [00:02<00:00,  7.38it/s, Epoch: 12, Batch: 16,Loss: 4.492,W.Decay: 0.307,Avg.Loss: 4.\n",
      "Training epoch 13: 100%|█| 16/16 [00:02<00:00,  7.37it/s, Epoch: 13, Batch: 16,Loss: 4.389,W.Decay: 0.307,Avg.Loss: 4.\n",
      "Training epoch 14: 100%|█| 16/16 [00:02<00:00,  7.34it/s, Epoch: 14, Batch: 16,Loss: 4.292,W.Decay: 0.306,Avg.Loss: 4.\n",
      "Training epoch 15: 100%|█| 16/16 [00:02<00:00,  7.22it/s, Epoch: 15, Batch: 16,Loss: 4.193,W.Decay: 0.306,Avg.Loss: 4.\n",
      "Training epoch 16: 100%|█| 16/16 [00:02<00:00,  7.36it/s, Epoch: 16, Batch: 16,Loss: 4.225,W.Decay: 0.306,Avg.Loss: 4.\n",
      "Training epoch 17: 100%|█| 16/16 [00:02<00:00,  7.29it/s, Epoch: 17, Batch: 16,Loss: 4.130,W.Decay: 0.305,Avg.Loss: 4.\n",
      "Training epoch 18: 100%|█| 16/16 [00:02<00:00,  7.17it/s, Epoch: 18, Batch: 16,Loss: 4.112,W.Decay: 0.305,Avg.Loss: 4.\n",
      "Training epoch 19: 100%|█| 16/16 [00:02<00:00,  7.27it/s, Epoch: 19, Batch: 16,Loss: 4.039,W.Decay: 0.305,Avg.Loss: 4.\n",
      "Training epoch 20: 100%|█| 16/16 [00:02<00:00,  7.29it/s, Epoch: 20, Batch: 16,Loss: 4.167,W.Decay: 0.305,Avg.Loss: 4.\n",
      "Training epoch 21: 100%|█| 16/16 [00:02<00:00,  7.29it/s, Epoch: 21, Batch: 16,Loss: 3.928,W.Decay: 0.305,Avg.Loss: 3.\n",
      "Training epoch 22: 100%|█| 16/16 [00:02<00:00,  7.30it/s, Epoch: 22, Batch: 16,Loss: 3.891,W.Decay: 0.304,Avg.Loss: 3.\n",
      "Training epoch 23: 100%|█| 16/16 [00:02<00:00,  7.21it/s, Epoch: 23, Batch: 16,Loss: 3.900,W.Decay: 0.304,Avg.Loss: 3.\n",
      "Training epoch 24: 100%|█| 16/16 [00:02<00:00,  7.30it/s, Epoch: 24, Batch: 16,Loss: 3.721,W.Decay: 0.304,Avg.Loss: 3.\n",
      "Training epoch 25: 100%|█| 16/16 [00:02<00:00,  7.21it/s, Epoch: 25, Batch: 16,Loss: 3.737,W.Decay: 0.304,Avg.Loss: 3.\n",
      "Training epoch 26: 100%|█| 16/16 [00:02<00:00,  7.34it/s, Epoch: 26, Batch: 16,Loss: 3.738,W.Decay: 0.304,Avg.Loss: 3.\n",
      "Training epoch 27: 100%|█| 16/16 [00:02<00:00,  7.24it/s, Epoch: 27, Batch: 16,Loss: 3.667,W.Decay: 0.304,Avg.Loss: 3.\n",
      "Training epoch 28: 100%|█| 16/16 [00:02<00:00,  7.34it/s, Epoch: 28, Batch: 16,Loss: 3.499,W.Decay: 0.304,Avg.Loss: 3.\n",
      "Training epoch 29: 100%|█| 16/16 [00:02<00:00,  7.19it/s, Epoch: 29, Batch: 16,Loss: 3.475,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 30: 100%|█| 16/16 [00:02<00:00,  7.39it/s, Epoch: 30, Batch: 16,Loss: 3.444,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 31: 100%|█| 16/16 [00:02<00:00,  7.36it/s, Epoch: 31, Batch: 16,Loss: 3.422,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 32: 100%|█| 16/16 [00:02<00:00,  7.27it/s, Epoch: 32, Batch: 16,Loss: 3.323,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 33: 100%|█| 16/16 [00:02<00:00,  7.31it/s, Epoch: 33, Batch: 16,Loss: 3.335,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 34: 100%|█| 16/16 [00:02<00:00,  7.35it/s, Epoch: 34, Batch: 16,Loss: 3.225,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 35: 100%|█| 16/16 [00:02<00:00,  7.39it/s, Epoch: 35, Batch: 16,Loss: 3.109,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 36: 100%|█| 16/16 [00:02<00:00,  7.38it/s, Epoch: 36, Batch: 16,Loss: 3.046,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 37: 100%|█| 16/16 [00:02<00:00,  7.34it/s, Epoch: 37, Batch: 16,Loss: 3.033,W.Decay: 0.303,Avg.Loss: 3.\n",
      "Training epoch 38: 100%|█| 16/16 [00:02<00:00,  7.30it/s, Epoch: 38, Batch: 16,Loss: 3.716,W.Decay: 0.302,Avg.Loss: 3.\n",
      "Training epoch 39: 100%|█| 16/16 [00:02<00:00,  7.24it/s, Epoch: 39, Batch: 16,Loss: 3.013,W.Decay: 0.302,Avg.Loss: 3.\n",
      "Training epoch 40: 100%|█| 16/16 [00:02<00:00,  7.29it/s, Epoch: 40, Batch: 16,Loss: 2.952,W.Decay: 0.302,Avg.Loss: 3.\n",
      "Training epoch 41: 100%|█| 16/16 [00:02<00:00,  7.24it/s, Epoch: 41, Batch: 16,Loss: 2.949,W.Decay: 0.302,Avg.Loss: 3.\n",
      "Training epoch 42: 100%|█| 16/16 [00:02<00:00,  7.25it/s, Epoch: 42, Batch: 16,Loss: 3.109,W.Decay: 0.302,Avg.Loss: 3.\n",
      "Training epoch 43: 100%|█| 16/16 [00:02<00:00,  7.27it/s, Epoch: 43, Batch: 16,Loss: 3.027,W.Decay: 0.302,Avg.Loss: 2.\n",
      "Training epoch 44: 100%|█| 16/16 [00:02<00:00,  7.29it/s, Epoch: 44, Batch: 16,Loss: 3.108,W.Decay: 0.302,Avg.Loss: 3.\n",
      "Training epoch 45: 100%|█| 16/16 [00:02<00:00,  7.29it/s, Epoch: 45, Batch: 16,Loss: 2.890,W.Decay: 0.302,Avg.Loss: 3.\n",
      "Training epoch 46: 100%|█| 16/16 [00:02<00:00,  7.26it/s, Epoch: 46, Batch: 16,Loss: 2.859,W.Decay: 0.302,Avg.Loss: 2.\n",
      "Training epoch 47: 100%|█| 16/16 [00:02<00:00,  7.31it/s, Epoch: 47, Batch: 16,Loss: 2.771,W.Decay: 0.302,Avg.Loss: 2.\n",
      "Training epoch 48: 100%|█| 16/16 [00:02<00:00,  7.35it/s, Epoch: 48, Batch: 16,Loss: 2.810,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 49: 100%|█| 16/16 [00:02<00:00,  7.31it/s, Epoch: 49, Batch: 16,Loss: 2.825,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 50: 100%|█| 16/16 [00:02<00:00,  7.21it/s, Epoch: 50, Batch: 16,Loss: 2.910,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 51: 100%|█| 16/16 [00:02<00:00,  7.34it/s, Epoch: 51, Batch: 16,Loss: 2.737,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 52: 100%|█| 16/16 [00:02<00:00,  7.33it/s, Epoch: 52, Batch: 16,Loss: 2.726,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 53: 100%|█| 16/16 [00:02<00:00,  7.16it/s, Epoch: 53, Batch: 16,Loss: 2.759,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 54: 100%|█| 16/16 [00:02<00:00,  7.42it/s, Epoch: 54, Batch: 16,Loss: 3.300,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 55: 100%|█| 16/16 [00:02<00:00,  7.16it/s, Epoch: 55, Batch: 16,Loss: 3.294,W.Decay: 0.301,Avg.Loss: 3.\n",
      "Training epoch 56: 100%|█| 16/16 [00:02<00:00,  7.13it/s, Epoch: 56, Batch: 16,Loss: 2.816,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 57: 100%|█| 16/16 [00:02<00:00,  7.21it/s, Epoch: 57, Batch: 16,Loss: 2.741,W.Decay: 0.301,Avg.Loss: 2.\n",
      "Training epoch 58: 100%|█| 16/16 [00:02<00:00,  7.00it/s, Epoch: 58, Batch: 16,Loss: 3.003,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 59: 100%|█| 16/16 [00:02<00:00,  7.05it/s, Epoch: 59, Batch: 16,Loss: 2.646,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 60: 100%|█| 16/16 [00:02<00:00,  7.24it/s, Epoch: 60, Batch: 16,Loss: 2.655,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 61: 100%|█| 16/16 [00:02<00:00,  7.18it/s, Epoch: 61, Batch: 16,Loss: 2.659,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 62: 100%|█| 16/16 [00:02<00:00,  7.23it/s, Epoch: 62, Batch: 16,Loss: 2.725,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 63: 100%|█| 16/16 [00:02<00:00,  7.05it/s, Epoch: 63, Batch: 16,Loss: 2.562,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 64: 100%|█| 16/16 [00:02<00:00,  7.13it/s, Epoch: 64, Batch: 16,Loss: 2.600,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 65: 100%|█| 16/16 [00:02<00:00,  7.16it/s, Epoch: 65, Batch: 16,Loss: 2.623,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 66: 100%|█| 16/16 [00:02<00:00,  7.16it/s, Epoch: 66, Batch: 16,Loss: 2.541,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 67: 100%|█| 16/16 [00:02<00:00,  7.21it/s, Epoch: 67, Batch: 16,Loss: 2.554,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 68: 100%|█| 16/16 [00:02<00:00,  7.03it/s, Epoch: 68, Batch: 16,Loss: 2.850,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 69: 100%|█| 16/16 [00:02<00:00,  7.22it/s, Epoch: 69, Batch: 16,Loss: 2.816,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 70: 100%|█| 16/16 [00:02<00:00,  7.28it/s, Epoch: 70, Batch: 16,Loss: 2.536,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 71: 100%|█| 16/16 [00:02<00:00,  7.22it/s, Epoch: 71, Batch: 16,Loss: 2.695,W.Decay: 0.300,Avg.Loss: 2.\n",
      "Training epoch 72: 100%|█| 16/16 [00:02<00:00,  7.15it/s, Epoch: 72, Batch: 16,Loss: 2.661,W.Decay: 0.299,Avg.Loss: 2.\n",
      "Training epoch 73: 100%|█| 16/16 [00:02<00:00,  7.19it/s, Epoch: 73, Batch: 16,Loss: 2.765,W.Decay: 0.299,Avg.Loss: 2.\n",
      "Training epoch 74: 100%|█| 16/16 [00:02<00:00,  7.11it/s, Epoch: 74, Batch: 16,Loss: 2.539,W.Decay: 0.299,Avg.Loss: 2.\n",
      "Training epoch 75: 100%|█| 16/16 [00:02<00:00,  7.12it/s, Epoch: 75, Batch: 16,Loss: 4.743,W.Decay: 0.299,Avg.Loss: 3.\n",
      "Training epoch 76: 100%|█| 16/16 [00:02<00:00,  7.30it/s, Epoch: 76, Batch: 16,Loss: 3.034,W.Decay: 0.299,Avg.Loss: 3.\n",
      "Training epoch 77: 100%|█| 16/16 [00:02<00:00,  7.24it/s, Epoch: 77, Batch: 16,Loss: 3.217,W.Decay: 0.298,Avg.Loss: 3.\n",
      "Training epoch 78: 100%|█| 16/16 [00:02<00:00,  7.28it/s, Epoch: 78, Batch: 16,Loss: 2.575,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 79: 100%|█| 16/16 [00:02<00:00,  7.21it/s, Epoch: 79, Batch: 16,Loss: 2.962,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 80: 100%|█| 16/16 [00:02<00:00,  7.32it/s, Epoch: 80, Batch: 16,Loss: 2.574,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 81: 100%|█| 16/16 [00:02<00:00,  7.26it/s, Epoch: 81, Batch: 16,Loss: 2.964,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 82: 100%|█| 16/16 [00:02<00:00,  7.25it/s, Epoch: 82, Batch: 16,Loss: 2.957,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 83: 100%|█| 16/16 [00:02<00:00,  7.14it/s, Epoch: 83, Batch: 16,Loss: 3.044,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 84: 100%|█| 16/16 [00:02<00:00,  7.32it/s, Epoch: 84, Batch: 16,Loss: 2.834,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 85: 100%|█| 16/16 [00:02<00:00,  7.34it/s, Epoch: 85, Batch: 16,Loss: 2.467,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 86: 100%|█| 16/16 [00:02<00:00,  7.23it/s, Epoch: 86, Batch: 16,Loss: 2.370,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 87: 100%|█| 16/16 [00:02<00:00,  7.31it/s, Epoch: 87, Batch: 16,Loss: 2.444,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 88: 100%|█| 16/16 [00:02<00:00,  7.38it/s, Epoch: 88, Batch: 16,Loss: 2.321,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 89: 100%|█| 16/16 [00:02<00:00,  7.30it/s, Epoch: 89, Batch: 16,Loss: 2.337,W.Decay: 0.298,Avg.Loss: 2.\n",
      "Training epoch 90:  44%|▍| 7/16 [00:00<00:01,  7.47it/s, Epoch: 90, Batch: 7,Loss: 2.412,W.Decay: 0.298,Avg.Loss: 2.37"
     ]
    }
   ],
   "source": [
    "%load_ext autotime\n",
    "import shutil, logging, warnings, pathlib, pickle, tempfile, dataclasses, typing, contextlib, IPython\n",
    "import numpy as np, scipy as sp, pandas as pd, sympy as sy, tensorflow as tf, bayesflow as bf\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "from matplotlib.mathtext import math_to_image\n",
    "from dataclasses import dataclass, field\n",
    "from wolframclient.evaluation import WolframLanguageSession\n",
    "from wolframclient.language import wl, wlexpr\n",
    "pd.set_option('display.max_columns', None)\n",
    "logging.disable(logging.WARN)\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "warnings.simplefilter('once', UserWarning)\n",
    "EPS = 1e-8\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "IPython.display.clear_output()\n",
    "\n",
    "def safe_delete(target):\n",
    "    \"\"\"confirms before deletion to avoid accidents\"\"\"\n",
    "    if target.is_file():\n",
    "        input(f'press enter to delete and recreate {target}')\n",
    "        target.unlink()\n",
    "    elif target.is_dir():\n",
    "        input(f'press enter to delete and recreate {target}')\n",
    "        shutil.rmtree(target)\n",
    "\n",
    "def read_pickle(file, overwrite=False):\n",
    "    if overwrite:\n",
    "        safe_delete(file)\n",
    "    try:\n",
    "        with open(file, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def write_pickle(dct, file):\n",
    "    file.parent.mkdir(exist_ok=True, parents=True)\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(dct, f, -1)\n",
    "\n",
    "def latex(x):\n",
    "    \"\"\"converts symbols into nice latex for graphics\"\"\"\n",
    "    try:\n",
    "        y = f'$\\\\{x}$'\n",
    "        with tempfile.TemporaryFile() as fp:\n",
    "            math_to_image(y, fp)\n",
    "    except:\n",
    "        y = f'${x}$'\n",
    "    return y\n",
    "\n",
    "class BaseClass():\n",
    "    \"\"\"enables access to attributes & methods using dictionary syntax\"\"\"\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "    def __delitem__(self, key):\n",
    "        delattr(self, key)\n",
    "    def __setitem__(self, key, val):\n",
    "        setattr(self, key, val)\n",
    "    def __contains__(self, key):\n",
    "        return hasattr(self, key)\n",
    "\n",
    "@dataclass\n",
    "class ODE(BaseClass):\n",
    "    name: str\n",
    "    n_steps: int\n",
    "    sys: typing.Dict\n",
    "    x0: typing.Dict\n",
    "    cls_ode: typing.List\n",
    "    cls_obs: typing.List\n",
    "    cls_dif: typing.List = field(default_factory=list)\n",
    "    prior: typing.Callable = None  # not necessary (will be overwritten) if prior.pkl reads successfully\n",
    "    param_scaler: str = 'minmax'\n",
    "    data_scaler: str = 'robust'\n",
    "    n_calibrate: int = 2**20\n",
    "    num_coupling_layers: int = 6\n",
    "    coupling_settings: typing.Dict = field(default_factory=dict)\n",
    "    solver_kwargs: typing.Dict = field(default_factory=dict)\n",
    "    root_path: str = '/home/pythonserver/BayesResearch/Cook_new'\n",
    "    overwrite: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.ses = WolframLanguageSession()\n",
    "        self.param_scaler = f'{self.param_scaler}'.lower().strip()\n",
    "        self.data_scaler = f'{self.data_scaler}'.lower().strip()\n",
    "        self.root_path = pathlib.Path(self.root_path) / self.name\n",
    "        self.checkpoint_path = self.root_path / f'{self.param_scaler}_{self.data_scaler}/checkpoints'\n",
    "        self.results_path = self.root_path / f'{self.param_scaler}_{self.data_scaler}/results'\n",
    "        self.results_path.mkdir(exist_ok=True, parents=True)\n",
    "        self.use_jac = ('method' in self.solver_kwargs) and (self.solver_kwargs['jac'] in ['Radau','BDF','LSODA'])\n",
    "        \n",
    "        self.cls_all = list(self.x0.keys())\n",
    "        self.x0 = list(self.x0.values())\n",
    "        self.idx_ode = [k for k, key in enumerate(self.cls_all) if key in self.cls_ode]\n",
    "        self.idx_obs = [k for k, key in enumerate(self.cls_all) if key in self.cls_obs]\n",
    "        self.idx_dif = [k for k, key in enumerate(self.cls_all) if key in self.cls_dif]\n",
    "        self.sys_all = sy.Matrix([self.sys[key] for key in self.cls_all])\n",
    "        self.sys_ode = sy.Matrix([self.sys[key] for key in self.cls_ode])\n",
    "        self.jac_all = self.sys_all.jacobian(self.cls_all)\n",
    "        self.jac_ode = self.sys_ode.jacobian(self.cls_ode)\n",
    "        self.prior = self.init_prior()\n",
    "        self.generative_model = bf.simulation.GenerativeModel(prior=self.prior, simulator=bf.simulation.Simulator(simulator_fun=self.solve))\n",
    "        self.init_model()\n",
    "        IPython.display.clear_output()\n",
    "\n",
    "    def init_prior(self):\n",
    "        file = self.root_path / 'prior.pkl'\n",
    "        pr = read_pickle(file, self.overwrite)\n",
    "        if pr is None:\n",
    "            print('generating simulations')\n",
    "            pr = self.prior\n",
    "            self.generative_model = bf.simulation.GenerativeModel(prior=pr, simulator=bf.simulation.Simulator(simulator_fun=self.solve))\n",
    "            pr.__dict__ |= self.get_dfs(self.generative_model(self.n_calibrate))\n",
    "            print('simulations complete')\n",
    "            for key, val in {'prior_draws':'param', 'sim_data':'data'}.items():\n",
    "                arr = pr[key]\n",
    "                if arr.ndim <= 2:\n",
    "                    ax = 0\n",
    "                else:\n",
    "                    ax = (0,2)\n",
    "                pr[f'{val}_mean'] = np.mean(arr, axis=ax, keepdims=True)\n",
    "                pr[f'{val}_std'] = np.std(arr, axis=ax, keepdims=True)\n",
    "                for r in range(5):\n",
    "                    pr[f'{val}_q{r}'] = np.quantile(arr, q=r/4, axis=ax, keepdims=True)\n",
    "                pr[f'{val}_ran'] = pr[f'{val}_q4'] - pr[f'{val}_q0']\n",
    "                pr[f'{val}_iqr'] = pr[f'{val}_q3'] - pr[f'{val}_q1']\n",
    "            write_pickle(pr, file)\n",
    "        return pr\n",
    "\n",
    "    def init_model(self, overwrite=False):\n",
    "        if overwrite:\n",
    "            safe_delete(self.checkpoint_path)\n",
    "        self.amortizer = bf.amortizers.AmortizedPosterior(\n",
    "            inference_net = bf.networks.InvertibleNetwork(\n",
    "                num_params=len(self.prior.names),\n",
    "                num_coupling_layers=self.num_coupling_layers,\n",
    "                coupling_settings=self.coupling_settings,\n",
    "            ),\n",
    "            summary_net = bf.networks.SequenceNetwork())\n",
    "        self.trainer = bf.trainers.Trainer(\n",
    "            generative_model = self.generative_model,\n",
    "            configurator = self.configurator,\n",
    "            amortizer = self.amortizer,\n",
    "            checkpoint_path = self.checkpoint_path,\n",
    "            # memory = True,\n",
    "            max_to_keep = 10000,\n",
    "        )\n",
    "\n",
    "    def load_best_checkpoint(self):\n",
    "        self.init_model()\n",
    "        try:\n",
    "            best_idx = self.trainer.loss_history.total_val_loss.argmin() + 1\n",
    "            best_ckpt = self.checkpoint_path / f'ckpt-{best_idx}'\n",
    "            self.trainer.checkpoint.restore(best_ckpt)\n",
    "            print(f'loading {best_ckpt}')\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def solve(self, params, n_steps=None):\n",
    "        if not isinstance(params, dict):\n",
    "            params = dict(zip(self.prior.names, params))\n",
    "        n_steps = self.n_steps if n_steps is None else n_steps\n",
    "        fun = sy.lambdify([t, self.cls_all], self.sys_all.subs(params).flat())\n",
    "        if self.use_jac:\n",
    "            self.solver_kwargs['jac'] = sy.lambdify([t, self.cls_all], self.jac_all.subs(params))\n",
    "        sol = sp.integrate.solve_ivp(fun=fun, y0=self.x0, t_span=[0,n_steps], t_eval=np.arange(n_steps)+1, **self.solver_kwargs)\n",
    "        assert sol.status==0, f'{params}\\n{sol}'\n",
    "        sol.y[self.idx_dif] = np.diff(sol.y[self.idx_dif], axis=1, prepend=0)\n",
    "        # sol.y[self.idx_dif] = np.diff(sol.y[self.idx_dif], axis=1, prepend=self.x0[self.idx_dif])\n",
    "        sol.y[self.idx_obs] = self.prior.lognormal(np.log(sol.y[self.idx_obs]), params['sigma'])\n",
    "        return sol.y\n",
    "\n",
    "    def get_dfs(self, forward_dict):\n",
    "        \"\"\"convert arrays into dataframes\"\"\"\n",
    "        dct = {\n",
    "            'prior_draws':{'roll':-1, 'idx':'sample', 'col':self.prior.names},\n",
    "            'post_draws' :{'roll':-1, 'idx':'sample', 'col':self.prior.names},\n",
    "            'sim_data'   :{'roll': 1, 'idx':'step'  , 'col':self.cls_all},\n",
    "        }\n",
    "        for key, val in dct.items():\n",
    "            if key in forward_dict:\n",
    "                A = np.rollaxis(forward_dict[key], val['roll'])\n",
    "                sh = A.shape\n",
    "                B = pd.DataFrame(A.reshape(sh[0],-1).T, columns=val['col'][-sh[0]:]).rename_axis('draw')\n",
    "                if len(sh) > 2:\n",
    "                    B[val['idx']] = B.index % sh[-1]\n",
    "                    B.index //= sh[-1]\n",
    "                    B.set_index(val['idx'], append=True, inplace=True)\n",
    "                forward_dict[key.split('_')[0]+'_df'] = B\n",
    "        return forward_dict\n",
    "\n",
    "    def validate(self, forward_dict):\n",
    "        \"\"\"remove parameter draws outside of range, remove data with negatives\"\"\"\n",
    "        contract = lambda x: np.all(x, axis=tuple(range(1,x.ndim)))\n",
    "        param = lambda key: (self.prior.param_q0 <= forward_dict[key]) & (forward_dict[key] <= self.prior.param_q4)\n",
    "        data  = lambda key: forward_dict[key] > -EPS\n",
    "        dct = {'prior_draws':param, 'post_draws':param, 'sim_data':data}\n",
    "        masks = {key: contract(fcn(key)) for key, fcn in dct.items() if key in forward_dict}\n",
    "        idx_keep = np.array(True)\n",
    "        for mask in masks.values():\n",
    "            idx_keep = idx_keep & mask\n",
    "        for key in masks.keys():\n",
    "            forward_dict[key] = forward_dict[key][idx_keep]\n",
    "        masks['overall'] = idx_keep\n",
    "        for key, mask in masks.items():\n",
    "            n, d = np.sum(~mask), len(mask)\n",
    "            if n > 0:\n",
    "                print(f'rejecting {n} / {d} = {round(n/d*100)}% of {key}')\n",
    "        return forward_dict\n",
    "\n",
    "    def scale_param(self, x):\n",
    "        if self.param_scaler in ['minmax']:\n",
    "            x = (x - self.prior.param_q0) / self.prior.param_ran\n",
    "        elif self.param_scaler in ['robust']:\n",
    "            x = (x - self.prior.param_q2) / self.prior.param_iqr\n",
    "        elif self.param_scaler in ['standard', 'z']:\n",
    "            x = (x - self.prior.param_mean) / self.prior.param_std\n",
    "        else:\n",
    "            warnings.warn(f'unknown param_scaler \"{self.param_scaler}\"; no scaling attempted', UserWarning)\n",
    "        return x\n",
    "            \n",
    "    def unscale_param(self, x):\n",
    "        if self.param_scaler in ['minmax']:\n",
    "            x = x * self.prior.param_ran + self.prior.param_q0\n",
    "        elif self.param_scaler in ['robust']:\n",
    "            x = x * self.prior.param_iqr + self.prior.param_q2\n",
    "        elif self.param_scaler in ['standard', 'z']:\n",
    "            x = x * self.prior.param_std + self.prior.param_mean\n",
    "        else:\n",
    "            warnings.warn(f'unknown param_scaler \"{self.param_scaler}\"; no scaling attempted', UserWarning)\n",
    "        return x\n",
    "        \n",
    "    def scale_data(self, x):\n",
    "        if self.data_scaler in ['minmax']:\n",
    "            x = (x - self.prior.data_q0) / self.prior.data_ran\n",
    "        elif self.data_scaler in ['robust']:\n",
    "            x = (x - self.prior.data_q2) / self.prior.data_iqr\n",
    "        elif self.data_scaler in ['standard', 'z']:\n",
    "            x = (x - self.prior.data_mean) / self.prior.data_std\n",
    "        elif self.data_scaler in ['log', 'log1p']:\n",
    "            x = np.log1p(x)\n",
    "        else:\n",
    "            warnings.warn(f'unknown data_scaler \"{self.data_scaler}\"; no scaling attempted', UserWarning)\n",
    "        return x\n",
    "\n",
    "    def configurator(self, forward_dict, dfs=False):\n",
    "        \"\"\"prepare for input to amortizer\"\"\"\n",
    "        forward_dict = self.validate(forward_dict)\n",
    "        forward_dict['summary_conditions'] = self.scale_data(forward_dict['sim_data'])[:,self.idx_obs].astype(np.float32)\n",
    "        if 'prior_draws' in forward_dict:\n",
    "            forward_dict['parameters'] = self.scale_param(forward_dict['prior_draws']).astype(np.float32)\n",
    "        return self.get_dfs(forward_dict) if dfs else forward_dict\n",
    "\n",
    "    def defigurator(self, forward_dict, dfs=True):\n",
    "        \"\"\"post-process output from amortizer\"\"\"\n",
    "        forward_dict['post_draws'] = self.unscale_param(forward_dict['post_samples'])\n",
    "        return self.get_dfs(forward_dict) if dfs else forward_dict\n",
    "\n",
    "    def sampler(self, forward_dict, n_samples, dfs=True):\n",
    "        forward_dict = self.configurator(forward_dict)\n",
    "        forward_dict['post_samples'] = self.trainer.amortizer.sample(forward_dict, n_samples=n_samples)\n",
    "        return self.defigurator(forward_dict, dfs)\n",
    "\n",
    "    def train(self, save_period=np.inf, overwrite=False, **kwargs):\n",
    "        self.init_model(overwrite=overwrite)\n",
    "        defaults = {\n",
    "            'simulations_dict': {key:self.prior[key] for key in ['prior_draws','sim_data']},\n",
    "            'epochs': 5000,\n",
    "            'batch_size': 256,\n",
    "            'save_checkpoint': True,\n",
    "            'validation_sims': 1024,\n",
    "            'reuse_optimizer': True,\n",
    "            'early_stopping': False,\n",
    "        }\n",
    "        kwargs = defaults | kwargs  # lets kwargs overwrite defaults\n",
    "        # train for save_period, then save, fix missing, & draw new validation set\n",
    "        e = kwargs['epochs']\n",
    "        while e > 0:\n",
    "            kwargs['epochs'] = min(e, save_period)\n",
    "            h = self.trainer.train_offline(**kwargs)\n",
    "            self.fix_missing_loss(h['val_losses'].values[-1][0].round(7))\n",
    "            e -= kwargs['epochs']\n",
    "\n",
    "    def fix_missing_loss(self, y):\n",
    "        \"\"\"Fix a bug in BayesFlow 1.1.5 where last val_loss is not recorded\"\"\"\n",
    "        w = {int(x.stem.split('_')[1]): x for x in self.checkpoint_path.iterdir() if 'history' in x.stem}\n",
    "        for k, file in sorted(w.items(), reverse=True):\n",
    "            D = read_pickle(file)\n",
    "            H = D['val_history']\n",
    "            H = H[max(H.keys())]\n",
    "            if k > len(H):\n",
    "                H[f'Epoch {k}'] = [y]\n",
    "            L = D['_total_val_loss']\n",
    "            if k > len(L):\n",
    "                L.append(y)\n",
    "            if k > 1:\n",
    "                y = L[-2]\n",
    "            write_pickle(D, file)\n",
    "\n",
    "    def run_sim(self, name='sim', n_samples=50, plot=False, save=False, overwrite=False):\n",
    "        file = self.results_path / f'{name}_data.pkl'\n",
    "        self[name] = read_pickle(file, overwrite)\n",
    "        if self[name] is None:\n",
    "            self.load_best_checkpoint()\n",
    "            self[name] = self.generative_model(batch_size=n_samples*20)\n",
    "            self[name] = self.sampler(self[name], n_samples=n_samples)\n",
    "            self[name]['name'] = name\n",
    "            write_pickle(self[name], file)\n",
    "        if plot:\n",
    "            self.plot_ecdf(self[name], save)\n",
    "            self.plot_recovery(self[name], save)\n",
    "\n",
    "    def run_real(self, real_data, name, n_samples=500, n_subsample=100, n_steps=None, plot=False, save=False, overwrite=False):\n",
    "        file = self.results_path / f'{name}_data.pkl'\n",
    "        self[name] = read_pickle(file, overwrite)\n",
    "        if self[name] is None:\n",
    "            self.load_best_checkpoint()\n",
    "            # format real_data as needed by armotizer\n",
    "            R = pd.DataFrame(columns=self.cls_all).rename_axis('step')\n",
    "            for key, val in real_data.items():\n",
    "                R[key] = val\n",
    "            S = R.fillna(0).values.T[np.newaxis]\n",
    "            self[name] = {'real_data': real_data, 'sim_data': S}\n",
    "            self[name] = self.sampler(self[name], n_samples=n_samples, dfs=False)\n",
    "            self[name].pop('sim_data')\n",
    "            self[name] = self.validate(self[name])\n",
    "            self[name] |= self.generative_model.simulator(self[name]['post_draws'], n_steps=n_steps)\n",
    "            self[name] = self.get_dfs(self.validate(self[name]))\n",
    "            self[name]['all_df'] = pd.concat([self[name]['sim_df'], self[name]['real_data'].rename_axis('step').assign(draw=-1).reset_index().set_index(['draw','step'])])\n",
    "            self[name]['name'] = name\n",
    "            if n_subsample > 0:\n",
    "                A = self[name]['post_df']\n",
    "                A = A.sample(n=min(n_subsample, A.shape[0]))\n",
    "                self[name]['post_subsample'] = A\n",
    "                self[name]['sim_subsample'] = self[name]['sim_df'].loc[A.index]\n",
    "            write_pickle(self[name], file)\n",
    "        if plot:\n",
    "            self.plot_update(self[name], save)\n",
    "            self.plot_predictive(self[name], save)\n",
    "\n",
    "    def plot_losses(self, save=True):\n",
    "        self.init_model()\n",
    "        H = self.trainer.loss_history.get_plottable()\n",
    "        ub = H['train_losses'].quantile(0.98).values[0]\n",
    "        H = {k:v.clip(upper=ub) for k,v in H.items()}\n",
    "        bf.diagnostics.plot_losses(**H, moving_average=True)\n",
    "        if save:\n",
    "            plt.savefig(self.results_path / 'losses.png')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_ecdf(self, forward_dict, save=True):\n",
    "        bf.diagnostics.plot_sbc_ecdf(forward_dict['post_draws'], forward_dict['prior_draws'], param_names=self.prior.latex)\n",
    "        if save:\n",
    "            plt.savefig(self.results_path / f'{forward_dict[\"name\"]}_ecdf.png')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_recovery(self, forward_dict, save=True):\n",
    "        bf.diagnostics.plot_recovery(forward_dict['post_draws'], forward_dict['prior_draws'], param_names=self.prior.latex)\n",
    "        if save:\n",
    "            plt.savefig(self.results_path / f'{forward_dict[\"name\"]}_recovery.png')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_update(self, forward_dict, save=True):\n",
    "        pos = forward_dict['post_df'].assign(kind='posterior')\n",
    "        pri = self.prior(2**13, 'df').assign(kind='prior')\n",
    "        pri = self.prior['prior_df'].assign(kind='prior')\n",
    "        Q = pd.concat([pri,pos])\n",
    "        Q.columns = self.prior.latex+['kind']\n",
    "        fig = sns.FacetGrid(Q.melt(id_vars='kind'), hue='kind', col='variable', col_wrap=3, sharex=False, sharey=False)\n",
    "        fig.map(sns.histplot, 'value', kde=False, element='bars', alpha=0.5, stat='density')\n",
    "        fig.set_titles(template = \"{col_name}\")\n",
    "        fig.add_legend()\n",
    "        if save:\n",
    "            plt.savefig(self.results_path / f'{forward_dict[\"name\"]}_update.png')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_predictive(self, forward_dict, save=True, include=None, exclude=None):\n",
    "        S = forward_dict['sim_df'].copy()\n",
    "        R = forward_dict['real_data'].copy()\n",
    "        if include is not None:\n",
    "            inc = include if isinstance(include, list) else [include]\n",
    "            S = S[inc]\n",
    "        elif exclude is not None:\n",
    "            exc = exclude if isinstance(exclude, list) else [exclude]\n",
    "            S = S.drop(columns=exc)\n",
    "        fig, ax = plt.subplots(S.shape[1]+0, 1, sharex=False, figsize=(20,20))\n",
    "        G = S.groupby('step')\n",
    "        for i, nm in enumerate(S.columns):\n",
    "            for ci, clr in {90:'green', 50:'red', 10:'blue'}.items():\n",
    "                x = (100 - ci) / 200\n",
    "                lb = G[nm].quantile(x)\n",
    "                ub = G[nm].quantile(1-x)\n",
    "                ax[i].fill_between(x=lb.index, y1=lb, y2=ub, color=clr, alpha=0.3, label=f'{nm} {ci}%')\n",
    "            if nm in self.cls_obs:\n",
    "                ax[i].plot(R[nm], 'kx')\n",
    "            ax[i].legend(loc='right')\n",
    "            ax[i].set_title(nm)\n",
    "            ax[i].plot()\n",
    "        if save:\n",
    "            plt.savefig(self.results_path / f'{forward_dict[\"name\"]}_predictive.png')\n",
    "        plt.show()\n",
    "\n",
    "    def get_equ(self, params):\n",
    "        S = self.sys_ode.subs(params)\n",
    "        cmd = f\"{' && '.join([f'{s} == 0' for s in S] + [f'{k} >= -{EPS}' for k in self.cls_ode])}, {{{', '.join([str(k) for k in self.cls_ode])}}}\".replace('**', '^').replace('e-', '*10^-')\n",
    "        cmd = f\"Chop[N[Solve[{cmd}, Reals]]]\"\n",
    "        # cmd = f\"Chop[NSolve[{cmd}, Reals, 10]]\"\n",
    "        with self.ses, contextlib.redirect_stderr(None):\n",
    "            solutions = self.ses.evaluate(wlexpr(cmd))\n",
    "        if len(solutions) == 0:\n",
    "            print(params)\n",
    "            print(cmd)\n",
    "        else:\n",
    "            return pd.DataFrame([self.get_eig(params, sol) for sol in solutions])\n",
    "\n",
    "    def get_eig(self, params, sol):\n",
    "        equ = params | {sy.symbols(str(k).replace('Global`','')): v for k, v in sol}\n",
    "        J = sy.matrix2numpy(self.jac_ode.subs(equ), float)\n",
    "        equ['eig'] = np.max(np.real(np.linalg.eigvals(J)))\n",
    "        return equ\n",
    "\n",
    "    def get_equilibria(self, draws):\n",
    "        A = pd.concat([self.get_equ(params).assign(draw=draw) for draw, params in draws.to_dict('index').items()]).set_index('draw')\n",
    "        B = A[self.cls_ode+['eig']].rename(columns=str)\n",
    "        B['_stable']   = B['eig'] <= -EPS\n",
    "        B['_unstable'] = B['eig'] >= EPS\n",
    "        B['_unknown'] = ~(B['_stable'] | B['_unstable'])\n",
    "        B = B.drop(columns='eig') > EPS\n",
    "        A['label'] = (B.values * B.columns).to_series().str.join('').values\n",
    "\n",
    "        L = pd.get_dummies(A['label'])\n",
    "        targ = list(L.columns)\n",
    "        G = pd.concat([A, L], axis=1).groupby('draw').agg({k:'max' for k in self.prior.names} | {k:'sum' for k in targ})\n",
    "        C = G.groupby(targ).describe()\n",
    "        C.insert(0,'n_equ', C[[]].reset_index().values.sum(axis=1))\n",
    "        C.insert(1,'ct', C.iloc[:,1].astype(int))\n",
    "        C.insert(2,'pct', (C['ct'] / C['ct'].sum() * 100).round(2))\n",
    "        S = (\n",
    "            C[[x for x in C.columns if 'count' not in x]]\n",
    "            .sort_values(['ct','n_equ'], ascending=False)\n",
    "            .set_index(['n_equ','ct','pct'], append=True)\n",
    "        )\n",
    "        return {'details':A, 'summary':S}\n",
    "\n",
    "class Prior(BaseClass):\n",
    "    def __init__(self, seed=None):\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.names = list(self.sample().keys())\n",
    "        self.latex = [latex(key) for key in self.names]\n",
    "\n",
    "    def __call__(self, batch_size=None, rtn='arr'):\n",
    "        \"\"\"Format sample - don't change\"\"\"\n",
    "        s = self.sample(batch_size)\n",
    "        return pd.DataFrame(s) if rtn=='df' else np.array(list(s.values())).T if rtn=='arr' else [dict(zip(s.keys(), z)) for z in zip(*s.values())]\n",
    "    \n",
    "    def sample(self, batch_size=None):\n",
    "        \"\"\"define prior\"\"\"\n",
    "        return {\n",
    "            r1: self.rng.uniform(0, 1, size=batch_size),  # treatment rates\n",
    "            r2: self.rng.uniform(0, 1, size=batch_size),  # treatment rates\n",
    "            rho: self.rng.uniform(0, 1, size=batch_size),  # probability the treatment works\n",
    "            kappa: self.rng.uniform(0, 1, size=batch_size),  # time to become infectious\n",
    "            beta1: self.rng.uniform(0, 5, size=batch_size),  # infection rates\n",
    "            beta2: self.rng.uniform(0, 5, size=batch_size),  # infection rates\n",
    "            Lamda: self.rng.uniform(500000, 1000000, size=batch_size),  # birth counts\n",
    "            mu: self.rng.uniform(1/90, 1/50, size=batch_size),  # center at 1/70\n",
    "            # sigma: np.abs(self.rng.standard_cauchy(size=batch_size))\n",
    "        }\n",
    "\n",
    "s, e, i, t, de, beta1, beta2, lamda, mu, kappa, rho, r1, r2 = sy.symbols('s e i t de beta_1 beta_2 lambda mu kappa rho r_1 r_2')\n",
    "n = s + e + i + t\n",
    "sys = {\n",
    "    s: Lamda - beta1*s*i/n - mu*s,\n",
    "    e: (beta1*s + beta2*t)*i/n - (mu+kappa+r1)*e + rho*r2*i,\n",
    "    i: kappa*e - (r2+mu)*i,\n",
    "    t: r1*e + (1-rho)*r2*i - beta2*t*i/n - mu*t,\n",
    "    de: (beta1*s + beta2*t)*i/n,\n",
    "}\n",
    "us_data = pd.DataFrame({\n",
    "    de:[25701,26283,26673,25107,24205,22727,21210,19751,18287,17500,16309,15945,15055,14835,14499,14068,13732,13286,12905,11537,11182,10528],\n",
    "})\n",
    "\n",
    "for param_scaler in ['minmax','standard','robust']:\n",
    "    for data_scaler in ['minmax','standard','robust','log']:\n",
    "        self = ODE(\n",
    "            # overwrite=True,\n",
    "            # name='TB_noisy',\n",
    "            name='TB',\n",
    "            prior=Prior(),\n",
    "            n_steps=us_data.shape[0],\n",
    "            sys=sys,\n",
    "            x0={s:290000000, e:25000, i:25000, t:22000, de:0},\n",
    "            cls_ode=[s,e,i,t],\n",
    "            cls_obs=[de],\n",
    "            cls_dif=[de],\n",
    "            n_calibrate=2**20,\n",
    "            param_scaler=param_scaler,\n",
    "            data_scaler=data_scaler,\n",
    "            \n",
    "            # param_scaler='minmax',\n",
    "            # param_scaler='standard',\n",
    "            # param_scaler='robust',\n",
    "            \n",
    "            # data_scaler='minmax',\n",
    "            # data_scaler='standard',\n",
    "            # data_scaler='robust',\n",
    "            # data_scaler='log',\n",
    "        )\n",
    "        print(param_scaler, data_scaler)\n",
    "        try:\n",
    "            self.plot_losses()\n",
    "        except:\n",
    "            self.train(\n",
    "                overwrite=True,\n",
    "                # save_period=1000,\n",
    "                validation_sims=2**12,\n",
    "                epochs=1000,\n",
    "                batch_size=2**16,\n",
    "            )\n",
    "            self.plot_losses()\n",
    "            self.run_sim(\n",
    "                overwrite=True,\n",
    "                n_samples=50,\n",
    "                plot=True,\n",
    "                save=True,\n",
    "            )\n",
    "            self.run_real(\n",
    "                overwrite=True,\n",
    "                name='us',\n",
    "                n_samples=2**10,\n",
    "                real_data=us_data,\n",
    "                # n_steps=100,\n",
    "                # n_subsample=10,\n",
    "                plot=True,\n",
    "                save=True,\n",
    "        )\n",
    "# E = self.get_equilibria(self.us['post_subsample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e43dc0ca-54c4-49ce-91c8-42518dc5f042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 995 µs (started: 2023-11-10 12:04:37 -06:00)\n"
     ]
    }
   ],
   "source": [
    "self.trainer.loss_history.get_plottable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b142f2-1371-41f0-8432-886ca9021711",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 16\n",
    "2**k, 28.1 / 2**11 * 2**k / 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
